(this["webpackJsonpfront-end"]=this["webpackJsonpfront-end"]||[]).push([[0],[,,,,,,,,,,,,,function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){},function(e,t,s){"use strict";s.r(t);var i=s(0),n=s(1),a=s.n(n),o=s(7),r=s.n(o),c=(s(13),s(2)),l=s(3),d=s(5),h=s(4),u=(s(14),s.p+"static/media/architecture.1a7681c0.jpg"),m=s.p+"static/media/stay_gen.82c84366.wav",p=s.p+"static/media/stay_diret_style.1fe79057.wav",j=s.p+"static/media/rock.03a07210.png",b=s.p+"static/media/jazz.bc5bfea4.png",f=s.p+"static/media/blues.d181c30b.png",g=s.p+"static/media/classifier.8c3b192b.png",x=s.p+"static/media/mse.73c87a1e.png",O=s.p+"static/media/audio_generator.c141f1e9.png",v=s.p+"static/media/interpolation.b75d7b47.png",y=s.p+"static/media/direct_style.1a7279e2.png";s(15);var w=function(){return Object(i.jsxs)("div",{id:"approach",children:[Object(i.jsx)("h2",{children:"Technical Approach "}),Object(i.jsxs)("div",{id:"detail",children:[Object(i.jsx)("h3",{children:"Problem Statement and the Original Goal"}),Object(i.jsx)("p",{children:"The original goal of our music style transfer was taking two inputs, one song as the one to be modified and one song to be the desired style (i.e., a pop song transferred into Beethoven\u2019s style). Such automated music generation study involves multiple complications such as learning the style, generating the music, alternating amplitudes to comply with the targeted style while preserving the melody/lyrics of the original input, etc. Not only is this study combining polyphonic songs unprecedented, but also such an operation that writes a song into a different genre is even hard for humans to imagine and might require skilled musicians to perform some fine tuning. Therefore, taking two inputs and performing a style transfer on the go turns out to be more complex than expected. Due to limited time, we decided to train the model with the style of a certain song ahead of time and only take one input song to be transferred. If we want more styles, we will need to train the model on separate songs. We believe this is a core step towards the original goal of building the music style transfer."}),Object(i.jsx)("h3",{children:"Data"}),Object(i.jsx)("p",{children:"All the audio files used are in wav format because they can then directly be interpreted as an array of amplitudes. Since we are just training to learn the style of one song, we only use one song as input but the number of values are sufficient. On average, there are 44,100 values of amplitudes in 1 second of a song and a song is over 3 minutes. In order to train the network to learn a style/genre, we also used an additional dataset consisting of 10 genres with 100 songs of each genre. All the songs are cropped to 30 seconds. More details on how this is used in the training process will be included in the training section below."}),Object(i.jsx)("h3",{children:"Top-level architecture of the model"}),Object(i.jsx)("p",{children:"Our model contains 3 components. The major model samples 50 values of amplitudes from the input and generates the next 50 predicted values. During training, to learn about the style/genre of the input, we introduce a second model that is capable of classifying 10 different genres. This model takes 40 features of an audio file and outputs the predicted classification. In order to use this model, we need to supply it with 40 features whereas our main model outputs 50 raw values of amplitudes. Hence, we introduce a third model that takes 50 values and extracts 40 features that are associated with these values, which could then be fed into the classifier. Lastly, we backpropagate from the genre classifier back to the main model. A diagram for this architecture is shown below."}),Object(i.jsx)("div",{id:"arch-wrapper",children:Object(i.jsx)("img",{id:"architecture",src:u,alt:"arch"})}),Object(i.jsx)("h3",{children:"Detailed Architecture Design and Training"}),Object(i.jsx)("h4",{children:"1. Genre Classifier"}),Object(i.jsx)("p",{children:"This component takes 40 features of an audio file and classifies them into a genre. The dataset of 10 genres each with 100 songs is used to train this. Each song is cropped to 30 seconds. To extract the 40 features, we used the TorchAudio API."}),Object(i.jsx)("p",{children:"It has 2 hidden layers with RELU as activation function. The loss function is cross entropy loss. This component is trained for about 30 minutes."}),Object(i.jsx)("p",{children:"Following is the graph of the training loss:"}),Object(i.jsx)("img",{src:g,class:"plots",alt:"classifier train loss"}),Object(i.jsx)("h4",{children:"2. Feature Extractor"}),Object(i.jsx)("p",{children:"This component takes 50 values of amplitudes and extracts 40 features. It allows the generator component to use the genre classifier. The same dataset of 10 genres is used for this component so we could reuse the features generated by the API as labels during the training."}),Object(i.jsx)("p",{children:"It has 2 fully connected layers with RELU as activation function. The loss function is MSE. This component is trained for about 20 minutes."}),Object(i.jsx)("p",{children:"Following is the graph of the training loss:"}),Object(i.jsx)("img",{src:x,class:"plots",alt:"MSE train loss"}),Object(i.jsx)("h4",{children:"3. Generator"}),Object(i.jsx)("p",{children:"This component samples 50 amplitudes at a time from the input, and uses LSTM to output predictions of the next 50 amplitudes. Normally, the range of amplitudes is [-32728, 32728] and this would lead to big values when calculating losses and long computation time. So, before feeding the audio file into the network, the values are all normalized to be within [-1, 1]. Then, we denormalize the output by adding a fully connected layer after LSTM to restore to the common range of  amplitudes. The denormalization is necessary in order to feed the 50 values to the feature extractor. The total time taken to train this component is around 20 minutes."}),Object(i.jsx)("p",{children:"Following is the graph of the training loss:"}),Object(i.jsx)("img",{src:O,class:"plots",alt:"audio generator train loss"}),Object(i.jsx)("h3",{children:"Additional Attempts and Considerations"}),Object(i.jsx)("p",{children:"1. Instead of using LSTM to predict 3 values, an initial attempt was to predict 1 value based on 50 samples. However, this strategy was too inefficient, not only it takes a long time, but also the generated output was very noisy. An illustration of the failed output is included below:"}),Object(i.jsx)("audio",{controls:!0,children:Object(i.jsx)("source",{src:m})}),Object(i.jsx)("p",{children:"2. A more bold attempt was to just change the input directly by comparing its features with those of the style audio. We first tried to compare based on spectrograms. However, only using spectrograms presents the problem of too much variation on the patterns among the files that are not possible to deal with. An example is shown below with 3 spectrograms of 3 songs in different genres."}),Object(i.jsxs)("div",{class:"spectrogram",children:[Object(i.jsx)("img",{src:j,alt:"rock"}),Object(i.jsx)("p",{children:"Rock"})]}),Object(i.jsxs)("div",{class:"spectrogram",children:[Object(i.jsx)("img",{src:b,alt:"jazz"}),Object(i.jsx)("p",{children:"Jazz"})]}),Object(i.jsxs)("div",{class:"spectrogram",children:[Object(i.jsx)("img",{src:f,alt:"blues"}),Object(i.jsx)("p",{children:"Blues"})]}),Object(i.jsx)("p",{children:"Then, we tried to use a combination of spectrograms and MFCC of the audio files, but overall, simply comparing and altering the input song based on the differences in the features would not produce harmony but noises. Therefore, the conclusion is that the model needs a more sophisticated extraction of the features corresponding to a style. An illustration of the failed output is included below:"}),Object(i.jsx)("audio",{controls:!0,children:Object(i.jsx)("source",{src:p})}),Object(i.jsx)("p",{children:"Following is the graph of the training loss:"}),Object(i.jsx)("img",{src:y,class:"plots",alt:"direct style train loss"}),Object(i.jsx)("p",{children:"3. Another attempt was generating 100 notes per second and interpolating points in-between. However, this yields monotonous outputs that did not make sense."}),Object(i.jsx)("p",{children:"Following is the graph of the training loss:"}),Object(i.jsx)("img",{src:v,class:"plots",alt:"interpolation train loss"}),Object(i.jsx)("h3",{children:"Evaluations and Result"}),Object(i.jsx)("p",{children:"The evaluation of the output can be subjective, but our standard is to not produce output with unpleasant noises and most original elements of the input should be preserved while changes exist. In particular, the output should still be able to be identified as the input song."}),Object(i.jsx)("p",{children:"Our results are included in the previous demo section. The style song is \u201cMy Only Wish\u201d and we first experimented with \u201cStay With Me\u201d. We are aware that both songs are in pop style, but \u201cMy Only Wish\u201d is very representative of a Christmas/Holiday uplifting tone whereas \u201cStay with Me\u201d is more of a smooth and melancholic style. While the final output does not have a clear Holiday vibe added to it, we think this is an acceptable outcome as the melody, vocals, and major background music is preserved. Same applies for the other output generated for \u201cTime To Love\u201d. As mentioned before, superimposing a song with a different style might require a lot more fine tuning that could even require a nontrivial amount of work by a skilled musician. Hence, we think our model is a good starting point for a more sophisticated and powerful model in further development."}),Object(i.jsx)("h3",{children:"Related Work and Future Direction"}),Object(i.jsx)("p",{children:"As mentioned before, music style transfer from one song to another arbitrarily is not a precedent study. We think it is more feasible to adopt a narrower range of music. For example, use piano-only music and transfer input from jazz to classical. However, there are some existing works in music and audio generation that could be applied to advance our project."}),Object(i.jsx)("p",{children:"WaveNet by Oord et al. generates raw audio waveforms as synthetic utterances with input sequences recorded from human speakers using a model based on PixelCNNs [4]. Hadjeres et al. has presented DeepBach that generates chorales in the style of Johann Sebastian Bach using Keras [1]. However, unlike our approach that composes music sequentially, DeepBach uses pseudo-Gibbs sampling coupled with an adapted representation of musical data. MusicNet by Thickstun et al. presents a largely labeled dataset for classical music that consists of 34 hours of human-verified aligned recordings, and 1, 299, 329 individual labels on segments of these recordings (such as the composers, instruments used, length of each instrument playing, etc.) [3]. This could be used if we decide to focus on just transferring input into the style of classical music in a more refined manner such as adding/removing instruments during the generation."}),Object(i.jsx)("p",{children:"The work described by Hadjeres et al. for developing a style imitator with successful experiments on Bach Chorales could be relevant for the future development of our model. Their model learns about neighboring note events in a given corpus, invents new chords, and harmonizes unknown melodies. It is based on the idea that \u201cchord progressions can be generated by replicating the occurrences of pairs of neighboring notes'', and thus capturing the local \u201ctexture\u201d of the chord sequences. When it comes to style imitation, they aimed to reproduce pairwise correlations of the training set among the notes and capture higher-order interactions suitable for style imitation [2]. Hence, we could apply similar concepts to \u201csuperimpose\u201d the input after learning some stronger correlations among the specific notes."}),Object(i.jsx)("h3",{children:"Conclusion"}),Object(i.jsx)("p",{children:"Our music style transfer model consists of 3 components that learns the style of a song, takes another song as input, and outputs an altered version in correspondence to the trained style. Our experiments show that this is feasible, and our output is very close to the original version by reproducing the rhythm, melody, verses and main background sounds. We hope to extend our work to develop a more complete model that reforms a song with a new style in an artistic way while preserving its original melody."}),Object(i.jsx)("h3",{children:"References"}),Object(i.jsxs)("ul",{children:[Object(i.jsx)("li",{children:Object(i.jsx)("p",{children:"[1] Gaetan Hadjeres and Fran\xe7ois Pachet. DeepBach: a \xa8 steerable model for Bach chorales generation. arXiv preprint arXiv:1612.01010, 2016"})}),Object(i.jsx)("li",{children:Object(i.jsx)("p",{children:"[2] Hadjeres, G., Sakellariou, J., Pachet, F.: Style imitation and chord invention in polyphonic music with exponential families. CoRR abs/1609.05152 (2016)"})}),Object(i.jsx)("li",{children:Object(i.jsx)("p",{children:"[3] John Thickstun, Zaid Harchaoui, and Sham Kakade, \u201cLearning features of music from scratch,\u201d arXiv preprint arXiv:1611.09827, 2016."})}),Object(i.jsx)("li",{children:Object(i.jsx)("p",{children:"[4] Oord, A\xe4ron Van den, and Sander Dieleman. \u201cWaveNet: A Generative Model for Raw Audio.\u201d Deepmind, 8 Sept. 2016, deepmind.com/blog/article/wavenet-generative-model-raw-audio."})})]})]})]})},k=s.p+"static/media/banner.38dfe302.mov";s(16);var T=function(){return Object(i.jsxs)("div",{id:"banner",children:[Object(i.jsx)("h1",{id:"title",children:"Music Style Transfer"}),Object(i.jsx)("p",{id:"name",children:"Paul Yoo, Sherry Yang, and Hideyuki Komaki"}),Object(i.jsx)("a",{href:"#introduction",children:Object(i.jsx)("div",{className:"arrow"})}),Object(i.jsx)("video",{autoPlay:!0,loop:!0,muted:!0,playsInline:!0,src:k,id:"video"})]})};s(17);var S=function(){return Object(i.jsx)("div",{id:"code",children:Object(i.jsx)("h2",{children:"Source Code"})})},N=s.p+"static/media/my_only_wish.b360aec5.jpg",M=s.p+"static/media/time_to_love.2df2f403.jpg",C=s.p+"static/media/stay_with_me.ac784d37.jpg",A=s.p+"static/media/time_to_love_noiseless.8076c863.wav",_=s.p+"static/media/stay_with_me_noiseless.20619b81.wav",H=s.p+"static/media/now_playing.6d9f77a2.gif",F=(s(18),function(e){Object(d.a)(s,e);var t=Object(h.a)(s);function s(e){var i;return Object(c.a)(this,s),(i=t.call(this,e)).state={music:0,style:0,mframe:{0:"image-wrapper on",1:"image-wrapper",2:"image-wrapper"},mblur:{0:"",1:"off",2:"off"},audio:A,title:"Time to Love - October"},i}return Object(l.a)(s,[{key:"handleClickMusic",value:function(e){var t=[A,_],s=this.state.mframe;s[this.state.music]="image-wrapper",s[e]="image-wrapper on";var i=this.state.mblur;i[this.state.music]="off",i[e]="",this.setState({mframe:s,mblur:i,music:e,audio:t[e],title:["Time to Love - October","Stay with me - Sam Smith"][e]},(function(){this.refs.audio.pause(),this.refs.audio.load()}))}},{key:"render",value:function(){var e=this;return Object(i.jsxs)("div",{id:"demo",children:[Object(i.jsx)("h2",{children:"Music Style Transfer Demo"}),Object(i.jsx)("div",{className:"description",children:Object(i.jsx)("p",{children:"The song used for training the style is \u201cMy Only Wish\u201d by Britney Spears. The altered songs are Stay with Me\u201d by Sam Smith and \u201cTime To Love\u201d by October. It takes around 5 seconds to generate an output song that is over 2 minutes. In order to remove undesirable noises, some simple post processing was used (i.e., removing sounds that are abnormal to the range of values in the original input). A demo below shows our result."})}),Object(i.jsxs)("div",{id:"music-select",children:[Object(i.jsx)("div",{id:"select-box",children:Object(i.jsx)("div",{className:"",children:Object(i.jsx)("h3",{children:"Choose a Input Music \u266b"})})}),Object(i.jsxs)("div",{id:"music-box",children:[Object(i.jsxs)("div",{id:"music-0",className:this.state.mframe[0],onClick:function(){return e.handleClickMusic(0)},children:[Object(i.jsx)("img",{className:this.state.mblur[0],src:M,alt:"my only wish"}),Object(i.jsx)("p",{children:"Time to love"}),Object(i.jsx)("p",{children:"October"})]}),Object(i.jsxs)("div",{id:"music-1",className:this.state.mframe[1],onClick:function(){return e.handleClickMusic(1)},children:[Object(i.jsx)("img",{className:this.state.mblur[1],src:C,alt:"my only wish"}),Object(i.jsx)("p",{children:"Stay with me"}),Object(i.jsx)("p",{children:"Sam Smith"})]})]}),Object(i.jsx)("div",{className:"cb"})]}),Object(i.jsx)("div",{id:"cross-wrapper",children:Object(i.jsx)("div",{id:"cross2",children:Object(i.jsx)("span",{})})}),Object(i.jsxs)("div",{id:"style-select",children:[Object(i.jsx)("div",{id:"select-box",children:Object(i.jsx)("div",{className:"",children:Object(i.jsx)("h3",{children:"Style Music \u266c"})})}),Object(i.jsx)("div",{id:"music-box",children:Object(i.jsxs)("div",{className:"image-wrapper",children:[Object(i.jsx)("img",{src:N,alt:"my only wish"}),Object(i.jsx)("p",{children:"My Only Wish"}),Object(i.jsx)("p",{children:"Britney Spears"})]})})]}),Object(i.jsx)("div",{className:"cb"}),Object(i.jsx)("div",{className:"arrow-result bounce"}),Object(i.jsxs)("div",{className:"audio-result",children:[Object(i.jsx)("h3",{children:"Output Music"}),Object(i.jsx)("p",{children:this.state.title}),Object(i.jsx)("img",{className:"new-music",src:H,alt:"music"}),Object(i.jsx)("br",{}),Object(i.jsx)("audio",{controls:!0,ref:"audio",children:Object(i.jsx)("source",{src:this.state.audio,type:"audio/wav"})})]})]})}}]),s}(a.a.Component)),I=s.p+"static/media/Ask_permission.ac51a1e0.svg";s(19);var W=function(){return Object(i.jsxs)("div",{id:"introduction",children:[Object(i.jsx)("h2",{children:"Summary"}),Object(i.jsx)("div",{className:"description",children:Object(i.jsx)("p",{children:"We present our deep learning project Music Style Transfer that takes a song as input, modifies it with a different style that was pre-trained with our model, and outputs a modified version of the song. The model takes 50 samples (values of the amplitudes) from the original song each time and outputs 50 values. After a series of consecutive sampling and generating, it concatenates all the values and thus yields a version of the song in a different style that still preserves the original melody, vocal, major instruments used in the background, etc. as much as possible."})}),Object(i.jsxs)("div",{className:"box",children:[Object(i.jsx)("img",{id:"sample-img-1",src:C,alt:"my only wish"}),Object(i.jsx)("div",{id:"cross",children:Object(i.jsx)("span",{})}),Object(i.jsx)("img",{id:"sample-img-2",src:N,alt:"mozart"}),Object(i.jsx)("div",{id:"equal",children:Object(i.jsx)("span",{})}),Object(i.jsx)("img",{id:"question",src:I,alt:"question mark"})]})]})},z=s.p+"static/media/hide.f1e760f5.jpg",P=s.p+"static/media/paul.2a636bd1.jpg",B=s.p+"static/media/sherry.0ed285cf.jpg";s(20);var D=function(){return Object(i.jsxs)("div",{id:"people",children:[Object(i.jsx)("h2",{children:"People"}),Object(i.jsxs)("div",{id:"profiles",children:[Object(i.jsxs)("div",{className:"profile",children:[Object(i.jsx)("img",{src:P,alt:"Paul"}),Object(i.jsx)("p",{children:"Paul Yoo"})]}),Object(i.jsxs)("div",{className:"profile",children:[Object(i.jsx)("img",{src:B,alt:"Sherry"}),Object(i.jsx)("p",{children:"Sherry Yang"})]}),Object(i.jsxs)("div",{className:"profile",children:[Object(i.jsx)("img",{src:z,alt:"Hide"}),Object(i.jsx)("p",{children:"Hideyuki Komaki"})]})]})]})};s(21);var L=function(){return Object(i.jsxs)("div",{id:"movie",children:[Object(i.jsx)("h2",{children:"Video"}),Object(i.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/-7oCCaz48f4",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})]})};s(22);var q=function(){return Object(i.jsxs)("footer",{children:[Object(i.jsx)("p",{id:"contact",children:"Contact: hide27k@uw.edu"}),Object(i.jsx)("p",{id:"right",children:"CSE 490g1 Final Project | University of Washington | Autumn 2020"})]})},E=function(e){Object(d.a)(s,e);var t=Object(h.a)(s);function s(){return Object(c.a)(this,s),t.apply(this,arguments)}return Object(l.a)(s,[{key:"render",value:function(){return Object(i.jsxs)("div",{className:"content",children:[Object(i.jsx)(T,{}),Object(i.jsx)(W,{}),Object(i.jsx)(F,{}),Object(i.jsx)(L,{}),Object(i.jsx)(w,{}),Object(i.jsx)(S,{}),Object(i.jsx)(D,{}),Object(i.jsx)(q,{})]})}}]),s}(n.Component),R=function(e){e&&e instanceof Function&&s.e(3).then(s.bind(null,24)).then((function(t){var s=t.getCLS,i=t.getFID,n=t.getFCP,a=t.getLCP,o=t.getTTFB;s(e),i(e),n(e),a(e),o(e)}))};r.a.render(Object(i.jsx)(a.a.StrictMode,{children:Object(i.jsx)(E,{})}),document.getElementById("root")),R()}],[[23,1,2]]]);
//# sourceMappingURL=main.c52c9de4.chunk.js.map
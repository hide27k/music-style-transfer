{"version":3,"sources":["architecture.jpg","stay_gen.wav","stay_diret_style.wav","rock.png","jazz.png","blues.png","classifier.png","mse.png","audio_generator.png","Approach.js","banner.mov","Banner.js","Code.js","my_only_wish.jpg","time_to_love.jpg","stay_with_me.jpg","time_to_love_noiseless.wav","stay_with_me_noiseless.wav","now_playing.gif","Demo.js","Ask_permission.svg","Intro.js","hide.jpg","paul.jpg","sherry.jpg","People.js","Video.js","Footer.js","App.js","reportWebVitals.js","index.js"],"names":["Approach","id","src","Arch","alt","Classifier","class","MSE","AudioGenerator","controls","Noisy","Rock","Jazz","Blues","DiretStyle","Banner","href","className","autoPlay","loop","muted","playsInline","movie","Code","Demo","props","state","music","style","mframe","0","1","2","mblur","audio","TimeToLoveNoiseless","title","v","audioList","StayWithMeNoiseless","newFrame","this","newblur","setState","refs","pause","load","onClick","handleClickMusic","TimeToLove","StayWithMe","MyOnlyWish","NowPlaying","ref","type","React","Component","Intro","Questionmark","People","paul","sherry","hide","Video","width","height","frameBorder","allow","allowFullScreen","Footer","App","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"iYAAe,G,MAAA,IAA0B,0CCA1B,MAA0B,qCCA1B,MAA0B,6CCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,kCCA1B,MAA0B,uCCA1B,MAA0B,gCCA1B,MAA0B,4C,MC6L1BA,MAlLf,WACE,OACE,sBAAKC,GAAG,WAAR,UACE,qDACA,sBAAKA,GAAG,SAAR,UACE,yEACA,knCAgBA,sCACA,6pBAYA,qEACA,+yBAaA,qBAAKA,GAAG,eAAR,SACE,qBAAKA,GAAG,eAAeC,IAAKC,EAAMC,IAAI,WAGxC,2EACA,qDACA,oRAIA,kLAIA,6EAIA,qBAAKF,IAAKG,EAAYC,MAAM,UAE5B,sDACA,kTAIA,4KAIA,6EAIA,qBAAKJ,IAAKK,EAAKD,MAAM,UAErB,8CACA,yqBAKA,6EAIA,qBAAKJ,IAAKM,EAAgBF,MAAM,UAEhC,wEACA,8TAIA,uBAAOG,UAAQ,EAAf,SACE,wBAAQP,IAAKQ,MAGf,qaAIA,sBAAKJ,MAAM,cAAX,UACE,qBAAKJ,IAAKS,EAAMP,IAAI,SACpB,wCAEF,sBAAKE,MAAM,cAAX,UACE,qBAAKJ,IAAKU,EAAMR,IAAI,SACpB,wCAEF,sBAAKE,MAAM,cAAX,UACE,qBAAKJ,IAAKW,EAAOT,IAAI,UACrB,yCAGF,8aAIA,uBAAOK,UAAQ,EAAf,SACE,wBAAQP,IAAKY,MAGf,6LAIA,wDACA,qTAGA,g7BAIA,mEACA,0YAGA,87BAGA,m2BAIA,4CACA,8hBAIA,4CACA,+BACE,6BACE,mLAEF,6BACE,+LAEF,6BACE,mLAEF,6BACE,0OCrLG,MAA0B,mC,MCgB1BC,MAbf,WACE,OACE,sBAAKd,GAAG,SAAR,UACE,oBAAIA,GAAG,QAAP,kCACA,mBAAGA,GAAG,OAAN,wDACA,mBAAGe,KAAK,gBAAR,SACE,qBAAKC,UAAU,YAEjB,uBAAOC,UAAQ,EAACC,MAAI,EAACC,OAAK,EAACC,aAAW,EAACnB,IAAKoB,EAAOrB,GAAG,c,MCD7CsB,MARf,WACE,OACE,qBAAKtB,GAAG,OAAR,SACE,gDCLS,MAA0B,yCCA1B,MAA0B,yCCA1B,MAA0B,yCCA1B,MAA0B,mDCA1B,MAA0B,mDCA1B,MAA0B,wCCqJ1BuB,G,wDA3Ib,WAAYC,GAAQ,IAAD,8BACjB,cAAMA,IACDC,MAAQ,CACXC,MAAO,EACPC,MAAO,EACPC,OAAQ,CACNC,EAAG,mBACHC,EAAG,gBACHC,EAAG,iBAELC,MAAO,CACLH,EAAG,GACHC,EAAG,MACHC,EAAG,OAELE,MAAOC,EACPC,MAAO,0BAhBQ,E,6DAoBFC,GACf,IACIC,EAAY,CAACH,EAAqBI,GAElCC,EAAWC,KAAKf,MAAMG,OAC1BW,EAASC,KAAKf,MAAMC,OAAS,gBAC7Ba,EAASH,GAAK,mBAEd,IAAIK,EAAUD,KAAKf,MAAMO,MACzBS,EAAQD,KAAKf,MAAMC,OAAS,MAC5Be,EAAQL,GAAK,GAEbI,KAAKE,SAAS,CACZd,OAAQW,EACRP,MAAOS,EACPf,MAAOU,EACPH,MAAOI,EAAUD,GACjBD,MAhBc,CAAC,yBAA0B,4BAgBxBC,KACjB,WACAI,KAAKG,KAAKV,MAAMW,QAChBJ,KAAKG,KAAKV,MAAMY,Y,+BAIV,IAAD,OACP,OACE,sBAAK7C,GAAG,OAAR,UACE,2DACA,qBAAKgB,UAAU,cAAf,SACE,qeAIF,sBAAKhB,GAAG,eAAR,UACE,qBAAKA,GAAG,aAAR,SACE,qBAAKgB,UAAU,GAAf,SACE,iEASJ,sBAAKhB,GAAG,YAAR,UACE,sBAAKA,GAAG,UAAUgB,UAAWwB,KAAKf,MAAMG,OAAO,GAAIkB,QAAS,kBAAK,EAAKC,iBAAiB,IAAvF,UACE,qBAAK/B,UAAWwB,KAAKf,MAAMO,MAAM,GAAI/B,IAAK+C,EAAY7C,IAAI,iBAC1D,6CACA,2CAEF,sBAAKH,GAAG,UAAUgB,UAAWwB,KAAKf,MAAMG,OAAO,GAAIkB,QAAS,kBAAK,EAAKC,iBAAiB,IAAvF,UACE,qBAAK/B,UAAWwB,KAAKf,MAAMO,MAAM,GAAI/B,IAAKgD,EAAY9C,IAAI,iBAC1D,6CACA,gDAQJ,qBAAKa,UAAU,UAEjB,qBAAKhB,GAAG,gBAAR,SACE,qBAAKA,GAAG,SAAR,SAAiB,6BAEnB,sBAAKA,GAAG,eAAR,UACE,qBAAKA,GAAG,aAAR,SACE,qBAAKgB,UAAU,GAAf,SACE,wDASJ,qBAAKhB,GAAG,YAAR,SACE,sBAAKgB,UAAU,gBAAf,UACE,qBAAKf,IAAKiD,EAAY/C,IAAI,iBAC1B,6CACA,uDAgBN,qBAAKa,UAAU,OACf,qBAAKA,UAAU,wBACf,sBAAKA,UAAU,eAAf,UACE,8CAGA,4BACGwB,KAAKf,MAAMU,QAEd,qBAAKnB,UAAU,YAAYf,IAAKkD,EAAYhD,IAAI,UAChD,uBACA,uBAAOK,UAAQ,EAAC4C,IAAI,QAApB,SACE,wBAAQnD,IAAKuC,KAAKf,MAAMQ,MAAOoB,KAAK,0B,GApI7BC,IAAMC,YCTV,MAA0B,2C,MC8B1BC,MAzBf,WACE,OACE,sBAAKxD,GAAG,eAAR,UACE,yCACA,qBAAKgB,UAAU,cAAf,SACQ,gmBASR,sBAAKA,UAAU,MAAf,UACE,qBAAKhB,GAAG,eAAeC,IAAKgD,EAAY9C,IAAI,iBAC5C,qBAAKH,GAAG,QAAR,SAAgB,2BAChB,qBAAKA,GAAG,eAAeC,IAAKiD,EAAY/C,IAAI,WAC5C,qBAAKH,GAAG,QAAR,SAAgB,2BAChB,qBAAKA,GAAG,WAAWC,IAAKwD,EAActD,IAAI,yBCxBnC,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,mC,MC4B1BuD,MAtBf,WACE,OACE,sBAAK1D,GAAG,SAAR,UACE,wCACA,sBAAKA,GAAG,WAAR,UACE,sBAAKgB,UAAU,UAAf,UACC,qBAAKf,IAAK0D,EAAMxD,IAAI,SACpB,4CAED,sBAAKa,UAAU,UAAf,UACE,qBAAKf,IAAK2D,EAAQzD,IAAI,WACtB,+CAEF,sBAAKa,UAAU,UAAf,UACE,qBAAKf,IAAK4D,EAAM1D,IAAI,SACpB,0D,MCVK2D,MATf,WACE,OACE,sBAAK9D,GAAG,QAAR,UACE,uCACA,wBAAQ+D,MAAM,MAAMC,OAAO,MAAM/D,IAAI,4CAA4CgE,YAAY,IAAIC,MAAM,2FAA2FC,iBAAe,Q,MCKxMC,MATf,WACE,OACE,mCACE,mBAAGpE,GAAG,UAAN,qCACA,mBAAGA,GAAG,QAAN,kFCsBSqE,E,uKAfX,OACE,sBAAKrD,UAAU,UAAf,UACE,cAAC,EAAD,IACA,cAAC,EAAD,IACA,cAAC,EAAD,IACA,cAAC,EAAD,IACA,cAAC,EAAD,IACA,cAAC,EAAD,IACA,cAAC,EAAD,IACA,cAAC,EAAD,W,GAXUuC,aCCHe,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,M","file":"static/js/main.50c1cf90.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/architecture.1a7681c0.jpg\";","export default __webpack_public_path__ + \"static/media/stay_gen.82c84366.wav\";","export default __webpack_public_path__ + \"static/media/stay_diret_style.1fe79057.wav\";","export default __webpack_public_path__ + \"static/media/rock.03a07210.png\";","export default __webpack_public_path__ + \"static/media/jazz.bc5bfea4.png\";","export default __webpack_public_path__ + \"static/media/blues.d181c30b.png\";","export default __webpack_public_path__ + \"static/media/classifier.8c3b192b.png\";","export default __webpack_public_path__ + \"static/media/mse.73c87a1e.png\";","export default __webpack_public_path__ + \"static/media/audio_generator.c141f1e9.png\";","import Arch from './architecture.jpg';\nimport Noisy from './stay_gen.wav';\nimport DiretStyle from './stay_diret_style.wav';\nimport Rock from './rock.png';\nimport Jazz from './jazz.png';\nimport Blues from './blues.png';\nimport Classifier from './classifier.png';\nimport MSE from './mse.png';\nimport AudioGenerator from './audio_generator.png';\nimport './Approach.css'\n\nfunction Approach() {\n  return (\n    <div id=\"approach\">\n      <h2>Technical Approach </h2>\n      <div id=\"detail\">\n        <h3>Problem Statement and the Original Goal</h3>\n        <p>\n        The original goal of our music style transfer was taking two inputs, one song as\n        the one to be modified and one song to be the desired style (i.e., a pop song transferred\n        into Beethoven’s style). Such automated music generation study involves multiple\n        complications such as learning the style, generating the music, alternating amplitudes to\n        comply with the targeted style while preserving the melody/lyrics of the original input,\n        etc. Not only is this study combining polyphonic songs unprecedented, but also such an\n        operation that writes a song into a different genre is even hard for humans to imagine\n        and might require skilled musicians to perform some fine tuning. Therefore, taking two\n        inputs and performing a style transfer on the go turns out to be more complex than\n        expected. Due to limited time, we decided to train the model with the style of a certain\n        song ahead of time and only take one input song to be transferred. If we want more\n        styles, we will need to train the model on separate songs. We believe this is a core step\n        towards the original goal of building the music style transfer.\n        </p>\n\n        <h3>Data</h3>\n        <p>\n        All the audio files used are in wav format because they can then directly be\n        interpreted as an array of amplitudes. Since we are just training to learn the style of one\n        song, we only use one song as input but the number of values are sufficient. On\n        average, there are 44,100 values of amplitudes in 1 second of a song and a song is\n        over 3 minutes.\n        In order to train the network to learn a style/genre, we also used an additional\n        dataset consisting of 10 genres with 100 songs of each genre. All the songs are\n        cropped to 30 seconds. More details on how this is used in the training process will be\n        included in the training section below.\n        </p>\n\n        <h3>Top-level architecture of the model</h3>\n        <p>\n        Our model contains 3 components. The major model samples 50 values of\n        amplitudes from the input and generates the next 50 predicted values. During training,\n        to learn about the style/genre of the input, we introduce a second model that is capable\n        of classifying 10 different genres. This model takes 40 features of an audio file and\n        outputs the predicted classification. In order to use this model, we need to supply it with\n        40 features whereas our main model outputs 50 raw values of amplitudes. Hence, we\n        introduce a third model that takes 50 values and extracts 40 features that are associated \n        with these values, which could then be fed into the classifier. Lastly, we\n        backpropagate from the genre classifier back to the main model. A diagram for this\n        architecture is shown below.\n        </p>\n\n        <div id=\"arch-wrapper\">\n          <img id=\"architecture\" src={Arch} alt=\"arch\"/>\n        </div>\n\n        <h3>Detailed Architecture Design and Training</h3>\n        <h4>1. Genre Classifier</h4>\n        <p>\n        This component takes 40 features of an audio file and classifies them into a genre. The dataset of 10 genres each with 100 songs is used to train this. Each song is cropped to 30 seconds. To extract the 40 features, we used the TorchAudio API. \n        </p>\n\n        <p>\n        It has 2 hidden layers with RELU as activation function. The loss function is cross entropy loss. This component is trained for about 30 minutes.\n        </p>\n\n        <p>\n        Following is the graph of the training loss:\n        </p>\n\n        <img src={Classifier} class=\"plots\"/>\n\n        <h4>2. Feature Extractor</h4>\n        <p>\n        This component takes 50 values of amplitudes and extracts 40 features. It allows the generator component to use the genre classifier. The same dataset of 10 genres is used for this component so we could reuse the features generated by the API as labels during the training.\n        </p>\n\n        <p>\n        It has 2 fully connected layers with RELU as activation function. The loss function is MSE. This component is trained for about 20 minutes.\n        </p>\n\n        <p>\n        Following is the graph of the training loss:\n        </p>\n\n        <img src={MSE} class=\"plots\" />\n\n        <h4>3. Generator</h4>\n        <p>\n        This component samples 50 amplitudes at a time from the input, and uses LSTM to output predictions of the next 50 amplitudes. Normally, the range of amplitudes is [-32728, 32728] and this would lead to big values when calculating losses and long computation time. So, before feeding the audio file into the network, the values are all normalized to be within [-1, 1]. Then, we denormalize the output by adding a fully connected layer after LSTM to restore to the common range of  amplitudes. The denormalization is necessary in order to feed the 50 values to the feature extractor. \n\tThe total time taken to train this component is around 20 minutes.\n        </p>\n\n        <p>\n        Following is the graph of the training loss:\n        </p>\n\n        <img src={AudioGenerator} class=\"plots\" />\n\n        <h3>Additional Attempts and Considerations</h3>\n        <p>\n        1. Instead of using LSTM to predict 3 values, an initial attempt was to predict 1 value based on 50 samples. However, this strategy was too inefficient, not only it takes a long time, but also the generated output was very noisy. An illustration of the failed output is included below:\n        </p>\n\n        <audio controls>\n          <source src={Noisy} />\n        </audio>\n\n        <p>\n        2. A more bold attempt was to just change the input directly by comparing its features with those of the style audio. We first tried to compare based on spectrograms. However, only using spectrograms presents the problem of too much variation on the patterns among the files that are not possible to deal with. An example is shown below with 3 spectrograms of 3 songs in different genres.\n        </p>\n\n        <div class=\"spectrogram\">\n          <img src={Rock} alt=\"rock\"/>\n          <p>Rock</p>\n        </div>\n        <div class=\"spectrogram\" >\n          <img src={Jazz} alt=\"jazz\"/>\n          <p>Jazz</p>\n        </div>\n        <div class=\"spectrogram\">\n          <img src={Blues} alt=\"blues\"/>\n          <p>Blues</p>\n        </div>\n\n        <p>\n        Then, we tried to use a combination of spectrograms and MFCC of the audio files, but overall, simply comparing and altering the input song based on the differences in the features would not produce harmony but noises. Therefore, the conclusion is that the model needs a more sophisticated extraction of the features corresponding to a style. An illustration of the failed output is included below:\n        </p>\n\n        <audio controls>\n          <source src={DiretStyle} />\n        </audio>\n\n        <p>\n          3. Another attempt was generating 100 notes per second and interpolating points in-between. However, this yields monotonous outputs that did not make sense.\n        </p>\n\n        <h3>Evaluations and Result</h3>\n        <p>\n        The evaluation of the output can be subjective, but our standard is to not produce output with unpleasant noises and most original elements of the input should be preserved while changes exist. In particular, the output should still be able to be identified as the input song.\n        </p>\n        <p>\n        Our results are included in the previous demo section. The style song is “My Only Wish” and we first experimented with “Stay With Me”. We are aware that both songs are in pop style, but “My Only Wish” is very representative of a Christmas/Holiday uplifting tone whereas “Stay with Me” is more of a smooth and melancholic style. While the final output does not have a clear Holiday vibe added to it, we think this is an acceptable outcome as the melody, vocals, and major background music is preserved. Same applies for the other output generated for “Time To Love”. As mentioned before, superimposing a song with a different style might require a lot more fine tuning that could even require a nontrivial amount of work by a skilled musician. Hence, we think our model is a good starting point for a more sophisticated and powerful model in further development. \n        </p>\n\n        <h3>Related Work and Future Direction</h3>\n        <p>\n        As mentioned before, music style transfer from one song to another arbitrarily is not a precedent study. We think it is more feasible to adopt a narrower range of music. For example, use piano-only music and transfer input from jazz to classical. However, there are some existing works in music and audio generation that could be applied to advance our project. \n        </p>\n        <p>\n        WaveNet by Oord et al. generates raw audio waveforms as synthetic utterances with input sequences recorded from human speakers using a model based on PixelCNNs [4]. Hadjeres et al. has presented DeepBach that generates chorales in the style of Johann Sebastian Bach using Keras [1]. However, unlike our approach that composes music sequentially, DeepBach uses pseudo-Gibbs sampling coupled with an adapted representation of musical data. MusicNet by Thickstun et al. presents a largely labeled dataset for classical music that consists of 34 hours of human-verified aligned recordings, and 1, 299, 329 individual labels on segments of these recordings (such as the composers, instruments used, length of each instrument playing, etc.) [3]. This could be used if we decide to focus on just transferring input into the style of classical music in a more refined manner such as adding/removing instruments during the generation. \n        </p>\n        <p>\n        The work described by Hadjeres et al. for developing a style imitator with successful experiments on Bach Chorales could be relevant for the future development of our model. Their model learns about neighboring note events in a given corpus, invents new chords, and harmonizes unknown melodies. It is based on the idea that “chord progressions can be generated by replicating the occurrences of pairs of neighboring notes'', and thus capturing the local “texture” of the chord sequences. When it comes to style imitation, they aimed to reproduce pairwise correlations of the training set among the notes and capture higher-order interactions suitable for style imitation [2]. Hence, we could apply similar concepts to “superimpose” the input after learning some stronger correlations among the specific notes.\n        </p>\n        \n        <h3>Conclusion</h3>\n        <p>\n        Our music style transfer model consists of 3 components that learns the style of a song, takes another song as input, and outputs an altered version in correspondence to the trained style. Our experiments show that this is feasible, and our output is very close to the original version by reproducing the rhythm, melody, verses and main background sounds. We hope to extend our work to develop a more complete model that reforms a song with a new style in an artistic way while preserving its original melody. \n        </p>\n\n        <h3>References</h3>\n        <ul>\n          <li>\n            <p>[1] Gaetan Hadjeres and François Pachet. DeepBach: a ¨ steerable model for Bach chorales generation. arXiv preprint arXiv:1612.01010, 2016</p>\n          </li>\n          <li>\n            <p>[2] Hadjeres, G., Sakellariou, J., Pachet, F.: Style imitation and chord invention in polyphonic music with exponential families. CoRR abs/1609.05152 (2016)</p>\n          </li>\n          <li>\n            <p>[3] John Thickstun, Zaid Harchaoui, and Sham Kakade, “Learning features of music from scratch,” arXiv preprint arXiv:1611.09827, 2016.</p>\n          </li>\n          <li>\n            <p>[4] Oord, Aäron Van den, and Sander Dieleman. “WaveNet: A Generative Model for Raw Audio.” Deepmind, 8 Sept. 2016, deepmind.com/blog/article/wavenet-generative-model-raw-audio.</p>\n          </li>\n        </ul>\n      </div>\n    </div>\n  )\n}\n\nexport default Approach;","export default __webpack_public_path__ + \"static/media/banner.38dfe302.mov\";","import movie from './banner.mov';\nimport './Banner.css';\n\nfunction Banner() {\n  return (\n    <div id=\"banner\">\n      <h1 id=\"title\">Music Style Transfer</h1>\n      <p id=\"name\">Paul Yoo, Sherry Yang, and Hideyuki Komaki</p>\n      <a href=\"#introduction\">\n        <div className=\"arrow\"></div>\n      </a>\n      <video autoPlay loop muted playsInline src={movie} id=\"video\"></video>\n    </div>\n  )\n}\n\nexport default Banner;","import './Code.css'\n\nfunction Code() {\n  return (\n    <div id=\"code\">\n      <h2>Source Code</h2>\n    </div>\n  )\n}\n\nexport default Code;","export default __webpack_public_path__ + \"static/media/my_only_wish.b360aec5.jpg\";","export default __webpack_public_path__ + \"static/media/time_to_love.2df2f403.jpg\";","export default __webpack_public_path__ + \"static/media/stay_with_me.ac784d37.jpg\";","export default __webpack_public_path__ + \"static/media/time_to_love_noiseless.8076c863.wav\";","export default __webpack_public_path__ + \"static/media/stay_with_me_noiseless.20619b81.wav\";","export default __webpack_public_path__ + \"static/media/now_playing.6d9f77a2.gif\";","import React from 'react'\nimport MyOnlyWish from './my_only_wish.jpg';\nimport TimeToLove from './time_to_love.jpg';\nimport StayWithMe from './stay_with_me.jpg';\nimport TimeToLoveNoiseless from './time_to_love_noiseless.wav';\nimport StayWithMeNoiseless from './stay_with_me_noiseless.wav';\nimport NowPlaying from './now_playing.gif';\nimport './Demo.css';\n\nclass Demo extends React.Component {\n  constructor(props) {\n    super(props)\n    this.state = {\n      music: 0,\n      style: 0,\n      mframe: {\n        0: \"image-wrapper on\",\n        1: \"image-wrapper\",\n        2: \"image-wrapper\"\n      },\n      mblur: {\n        0: \"\",\n        1: \"off\",\n        2: \"off\"\n      },\n      audio: TimeToLoveNoiseless,\n      title: \"Time to Love - October\"\n    }\n  }\n\n  handleClickMusic(v) {\n    let titleList = [\"Time to Love - October\", \"Stay with me - Sam Smith\"];\n    let audioList = [TimeToLoveNoiseless, StayWithMeNoiseless];\n\n    let newFrame = this.state.mframe;\n    newFrame[this.state.music] = \"image-wrapper\";\n    newFrame[v] = \"image-wrapper on\";\n\n    let newblur = this.state.mblur;\n    newblur[this.state.music] = \"off\";\n    newblur[v] = \"\";\n\n    this.setState({\n      mframe: newFrame,\n      mblur: newblur,\n      music: v,\n      audio: audioList[v],\n      title: titleList[v]\n    },function(){\n      this.refs.audio.pause();\n      this.refs.audio.load();\n    })\n  }\n\n  render() {\n    return (\n      <div id=\"demo\">\n        <h2>Music Style Transfer Demo</h2>\n        <div className=\"description\">\n          <p>\n          The song used for training the style is “My Only Wish” by Britney Spears. The altered songs are Stay with Me” by Sam Smith and “Time To Love” by October. It takes around 5 seconds to generate an output song that is over 2 minutes. In order to remove undesirable noises, some simple post processing was used (i.e., removing sounds that are abnormal to the range of values in the original input). A demo below shows our result.\n          </p>\n        </div>\n        <div id=\"music-select\" >\n          <div id=\"select-box\">\n            <div className=\"\">\n              <h3>Choose a Input Music ♫</h3>\n            </div>\n            {/* <div className=\"btn\">\n              <h3>Upload your own music ♫</h3>\n            </div> */}\n            {/* <div>\n              <audio controls></audio>\n            </div> */}\n          </div>\n          <div id=\"music-box\">\n            <div id=\"music-0\" className={this.state.mframe[0]} onClick={() =>this.handleClickMusic(0)}>\n              <img className={this.state.mblur[0]} src={TimeToLove} alt=\"my only wish\"/>\n              <p>Time to love</p>\n              <p>October</p>\n            </div>\n            <div id=\"music-1\" className={this.state.mframe[1]} onClick={() =>this.handleClickMusic(1)}>\n              <img className={this.state.mblur[1]} src={StayWithMe} alt=\"my only wish\"/>\n              <p>Stay with me</p>\n              <p>Sam Smith</p>\n            </div>\n            {/* <div id=\"music-2\" className={this.state.mframe[2]} onClick={() =>this.handleClickMusic(2)}>\n              <img className={this.state.mblur[2]} src={StayWithMe} alt=\"my only wish\"/>\n              <p>Stay with me</p>\n              <p>Sam Smith</p>\n            </div> */}\n          </div>\n          <div className=\"cb\"></div>\n        </div>\n        <div id=\"cross-wrapper\">\n          <div id=\"cross2\"><span></span></div>\n        </div>\n        <div id=\"style-select\" >\n          <div id=\"select-box\">\n            <div className=\"\">\n              <h3>Style Music ♬</h3>\n            </div>\n            {/* <div className=\"btn\">\n              <h3>Upload your own style ♬</h3>\n            </div> */}\n            {/* <div>\n              <audio controls></audio>\n            </div> */}\n          </div>\n          <div id=\"music-box\">\n            <div className=\"image-wrapper\">\n              <img src={MyOnlyWish} alt=\"my only wish\"/>\n              <p>My Only Wish</p>\n              <p>Britney Spears</p>\n            </div>\n            {/* <div id=\"style-1\" className={this.state.sfråame[1]} onClick={() =>this.handleClickStyle(1)}>\n              <img className={this.state.sblur[1]} src={MyOnlyWish} alt=\"my only wish\"/>\n              <p>My Only Wish</p>\n            </div>\n            <div id=\"style-2\" className={this.state.sframe[2]} onClick={() =>this.handleClickStyle(2)}>\n              <img className={this.state.sblur[2]} src={MyOnlyWish} alt=\"my only wish\"/>\n              <p>My Only Wish</p>\n            </div>\n            <div id=\"style-3\" className={this.state.sframe[3]} onClick={() =>this.handleClickStyle(3)}>\n              <img className={this.state.sblur[3]} src={MyOnlyWish} alt=\"my only wish\"/>\n              <p>My Only Wish</p>\n            </div> */}\n          </div>\n        </div>\n        <div className=\"cb\"></div>\n        <div className=\"arrow-result bounce\"></div>\n        <div className=\"audio-result\">\n          <h3>\n            Output Music\n          </h3>\n          <p>\n            {this.state.title}\n          </p>\n          <img className=\"new-music\" src={NowPlaying} alt=\"music\" />\n          <br></br>\n          <audio controls ref=\"audio\">\n            <source src={this.state.audio} type=\"audio/wav\" />\n          </audio>\n        </div>\n      </div>\n    )\n  }\n}\n\nexport default Demo;","export default __webpack_public_path__ + \"static/media/Ask_permission.ac51a1e0.svg\";","import StayWithMe from './stay_with_me.jpg';\nimport MyOnlyWish from './my_only_wish.jpg';\nimport Questionmark from './Ask_permission.svg';\nimport './Intro.css';\n\nfunction Intro() {\n  return (\n    <div id=\"introduction\">\n      <h2>Summary</h2>\n      <div className=\"description\">\n              <p>We present our deep learning project Music Style Transfer that takes a song as\n        input, modifies it with a different style that was pre-trained with our model, and outputs a\n        modified version of the song. The model takes 50 samples (values of the amplitudes)\n        from the original song each time and outputs 50 values. After a series of consecutive\n        sampling and generating, it concatenates all the values and thus yields a version of the\n        song in a different style that still preserves the original melody, vocal, major instruments\n        used in the background, etc. as much as possible.\n        </p>\n      </div>\n      <div className=\"box\">\n        <img id=\"sample-img-1\" src={StayWithMe} alt=\"my only wish\"/>\n        <div id=\"cross\"><span></span></div>\n        <img id=\"sample-img-2\" src={MyOnlyWish} alt=\"mozart\"/>\n        <div id=\"equal\"><span></span></div>\n        <img id=\"question\" src={Questionmark} alt=\"question mark\"/>\n      </div>\n    </div>\n  )\n}\n\nexport default Intro;","export default __webpack_public_path__ + \"static/media/hide.f1e760f5.jpg\";","export default __webpack_public_path__ + \"static/media/paul.2a636bd1.jpg\";","export default __webpack_public_path__ + \"static/media/sherry.0ed285cf.jpg\";","import hide from './hide.jpg';\nimport paul from './paul.jpg';\nimport sherry from './sherry.jpg';\n\nimport './People.css'\n\nfunction People() {\n  return (\n    <div id=\"people\">\n      <h2>People</h2>\n      <div id=\"profiles\">\n        <div className=\"profile\">\n         <img src={paul} alt=\"Paul\"/>\n         <p>Paul Yoo</p>\n        </div>\n        <div className=\"profile\">\n          <img src={sherry} alt=\"Sherry\" />\n          <p>Sherry Yang</p>\n        </div>\n        <div className=\"profile\">\n          <img src={hide} alt=\"Hide\" />\n          <p>Hideyuki Komaki</p>\n        </div>\n      </div>\n    </div>\n  )\n}\n\nexport default People;","import './Video.css'\n\nfunction Video() {\n  return (\n    <div id=\"movie\">\n      <h2>Video</h2>\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-7oCCaz48f4\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowFullScreen></iframe>\n    </div>\n  )\n}\n\nexport default Video;","import './Footer.css';\n\nfunction Footer() {\n  return (\n    <footer>\n      <p id=\"contact\" >Contact: hide27k@uw.edu</p>\n      <p id=\"right\" >CSE 490g1 Final Project | University of Washington | Autumn 2020</p>\n    </footer>\n  )\n}\n\nexport default Footer;\n","import React, { Component } from 'react';\nimport './App.css';\nimport Approach from './Approach'\nimport Banner from './Banner'\nimport Code from './Code'\nimport Demo from './Demo'\nimport Intro from './Intro'\nimport People from './People'\nimport Video from './Video'\nimport Footer from './Footer'\n\nclass App extends Component {\n  render() {\n    return (\n      <div className=\"content\">\n        <Banner />\n        <Intro />\n        <Demo />\n        <Video />\n        <Approach />\n        <Code />\n        <People />\n        <Footer />\n      </div>\n    )\n  }\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}
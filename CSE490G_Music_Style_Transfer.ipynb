{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CSE490G_Music_Style_Transfer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPMSQmQLAa7Y"
      },
      "source": [
        "# Music Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdL7LlOAmkDP"
      },
      "source": [
        "# Import modules\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import librosa\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "#!pip install pydub\n",
        "import pydub\n",
        "from scipy.io.wavfile import read, write\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import torch.nn.functional as F\n",
        "#!pip install torchaudio\n",
        "import torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLDquTGdjITE"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/'\n",
        "rate, music1 = read(path + \"stay.wav\")\n",
        "lower_bound, upper_bound = -32768, 32767"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7TauCZAMLoZ"
      },
      "source": [
        "# Define dataset\n",
        "class MusicGenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, sequence_length, output_length=1):\n",
        "        super(MusicGenDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_length = output_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of sequences in the dataset\n",
        "        return (len(self.data) - 1) // self.sequence_length\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx * self.sequence_length\n",
        "        d = self.data[idx:idx + self.sequence_length]\n",
        "        if self.output_length == 1:\n",
        "          label = self.data[idx + self.sequence_length]\n",
        "        else:\n",
        "          label = self.data[idx + 1: idx + 1 + self.sequence_length]\n",
        "        # Normalize\n",
        "        return d.reshape(1,-1).copy() / 32768, label.copy() / 32768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HAT_HmA4u7"
      },
      "source": [
        "# Music Generator Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpm6tnU5wn4q"
      },
      "source": [
        "# LSTM based network that predicts the next sound amplitude given 50 previous sound amplitudes \n",
        "class MusicNet(nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super(MusicNet, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.lstm = nn.LSTM(self.feature_size, self.feature_size, num_layers=10, batch_first=True, bidirectional=False)\n",
        "        self.fc1 = nn.Linear(self.feature_size, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        x, hidden_state = self.lstm(x, hidden_state)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Loss function\n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.mse_loss(prediction, label)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmUYWOQK1MUq"
      },
      "source": [
        "# Fully connected version of the model above with special weighted loss that takes into account style\n",
        "class MusicNet2(nn.Module):\n",
        "    def __init__(self, feature_size, output_size=1):\n",
        "        super(MusicNet2, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.hidden_size = feature_size\n",
        "        self.fc1 = nn.Linear(self.hidden_size, 250)\n",
        "        self.fc2 = nn.Linear(250, 100)\n",
        "        self.fc3 = nn.Linear(100, 50)\n",
        "        self.fc4 = nn.Linear(50, 1)\n",
        "        self.genreNet = torch.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/raw_fully_conn.pt', map_location=torch.device('cpu')).eval()\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        features = x\n",
        "        x = self.fc4(x)\n",
        "        return x, hidden_state, features\n",
        "\n",
        "\n",
        "    # Loss function\n",
        "    def loss(self, prediction, label, style=None, features=None, reduction='mean'):\n",
        "        weight = 1.0 if style is None else 0.7\n",
        "        loss_val = weight * F.mse_loss(prediction, label)\n",
        "        #loss_val = F.mse_loss(prediction.view(-1), label.view(-1))\n",
        "        #loss_val = F.cross_entropy(output.view(-1, 32768 * 2), label.flatten())\n",
        "        #loss_val = F.cross_entropy(prediction.view(-1, len(notes_encoding)), label.view(-1))\n",
        "        if style is not None:\n",
        "          with torch.no_grad():\n",
        "            style_features = self.genreNet.get_feature(style)\n",
        "          #prediction_features = self.genreNet.get_feature(prediction.reshape(-1,1,32,32))\n",
        "          #loss_val += F.mse_loss(prediction_features, style_features.expand(280, -1,-1,-1))\n",
        "          loss_val += (1 - weight) * F.mse_loss(features, style_features)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AnA605g0tbr"
      },
      "source": [
        "# LSTM based model that generates the next 50 sound amplitudes given 50 previous amplitudes.\n",
        "# This was the final model we went with.\n",
        "class MusicNet2(nn.Module):\n",
        "    def __init__(self, feature_size, output_size=1):\n",
        "        super(MusicNet2, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        #self.encoder = nn.Embedding(len(notes_encoding), self.feature_size)\n",
        "        #self.encoder = nn.Embedding(32768 * 2, self.feature_size)\n",
        "        self.hidden_size = output_size\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_size, hidden_size=self.hidden_size, num_layers=5, batch_first=True, bidirectional=False)\n",
        "        #self.decoder1 = nn.Linear(self.feature_size,(self.feature_size + len(notes_encoding)) // 2)\n",
        "        #self.decoder2 = nn.Linear((self.feature_size + len(notes_encoding)) // 2, len(notes_encoding))\n",
        "        #self.decoder = nn.Linear(self.feature_size, 1)#self.feature_size)\n",
        "        #self.decoder = nn.Linear(self.feature_size, len(notes_encoding))\n",
        "        #self.decoder = nn.Linear(self.hidden_size, 32768 * 2)\n",
        "        #self.fc1 = nn.Linear(self.hidden_size, 250)\n",
        "        #self.fc4 = nn.Linear(50, output_size)\n",
        "        #self.fc4 = nn.Linear(50, 1)\n",
        "        self.fc1 = nn.Linear(SEQUENCE_LENGTH, 200)\n",
        "        self.fc2 = nn.Linear(200, SEQUENCE_LENGTH)\n",
        "        self.feature_extractor = torch.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/feature_predictor.pt', map_location=torch.device('cuda')).eval()\n",
        "        self.genreNet = torch.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/feature_fully_conn.pt', map_location=torch.device('cuda')).eval()\n",
        "        \n",
        "    def forward(self, x, hidden_state=None):\n",
        "        \n",
        "        x, hidden_state = self.lstm(x, hidden_state)\n",
        "        x = x.squeeze()\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Loss function\n",
        "    def loss(self, prediction, label, style=None, features=None, reduction='mean'):\n",
        "        weight = 1.0 if style is None else 0.5\n",
        "        loss_val = weight * F.mse_loss(prediction, label)\n",
        "        #loss_val = F.mse_loss(prediction.view(-1), label.view(-1))\n",
        "        #loss_val = F.cross_entropy(output.view(-1, 32768 * 2), label.flatten())\n",
        "        #loss_val = F.cross_entropy(prediction.view(-1, len(notes_encoding)), label.view(-1))\n",
        "        if style is not None:\n",
        "          with torch.no_grad():\n",
        "            style_features = self.genreNet.get_feature(style)\n",
        "          #prediction_features = self.genreNet.get_feature(prediction.reshape(-1,1,32,32))\n",
        "          #loss_val += F.mse_loss(prediction_features, style_features.expand(280, -1,-1,-1))\n",
        "          mfcc = self.feature_extractor(prediction * 32768)\n",
        "          features = self.genreNet.get_feature(torch.cat((mfcc, torch.zeros((mfcc.shape[0],20)).to(device)), 1))\n",
        "          loss_val += (1 - weight) * F.mse_loss(features, style_features)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp7SRbueA9Hy"
      },
      "source": [
        "# Music Generator Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne6Gt-nNyQ3N",
        "outputId": "653d3da1-a62b-4b53-e281-1fb7c5a0eec5"
      },
      "source": [
        "SEQUENCE_LENGTH = 50\n",
        "FEATURE_SIZE = 1 #1024\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 100 #280\n",
        "model1 = MusicNet2(FEATURE_SIZE, FEATURE_SIZE)\n",
        "#model2 = MusicNet2(FEATURE_SIZE, FEATURE_SIZE)\n",
        "optim = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
        "\n",
        "data1 = MusicGenDataset(music1[:,0], SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n",
        "#data2 = MusicGenDataset(music1[:,1], SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train1 = torch.utils.data.DataLoader(data1, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, **kwargs)\n",
        "#train2 = torch.utils.data.DataLoader(data2, batch_size=BATCH_SIZE,\n",
        "#                                            shuffle=True, **kwargs)\n",
        "model1 = model1.to(device)\n",
        "#model2 = model2.to(device)\n",
        "\n",
        "# Load style audio\n",
        "style, sr = torchaudio.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/santa.wav')\n",
        "#shp = int(np.sqrt(len(style)))\n",
        "#style = style[:shp * shp, 0]\n",
        "#style = torch.tensor([[style.reshape((shp, shp))]]).float()\n",
        "#style = F.interpolate(style, size=(32,32)).to(device)\n",
        "style = style[0,:] # first channel\n",
        "\n",
        "# Extract style features\n",
        "mfcc = torchaudio.transforms.MFCC()(style)\n",
        "mfcc = mfcc.squeeze().mean(dim=1).to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for idx, (data, label) in enumerate(train1):\n",
        "    data, label = data.detach().to(device), label.detach().to(device)\n",
        "    data = data.float()\n",
        "    label = label.float()\n",
        "    #data = data.permute(2,0,1).long().squeeze() + 32768\n",
        "    #label = label.permute(1,0).long() + 32768\n",
        "    optim.zero_grad()\n",
        "\n",
        "    data = data.reshape((data.shape[0], SEQUENCE_LENGTH, 1))\n",
        "    output, hidden = model1(data) #torch.cat((data, style.view(1,1,-1).expand(BATCH_SIZE,-1,-1)), 2))\n",
        "    \n",
        "    loss = model1.loss(output, label, style=mfcc.expand(output.shape[0], -1))\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if idx % 1000 == 0:\n",
        "      print('Epoch:', epoch)\n",
        "      print('Loss:', loss.item())\n",
        "      print('Prediction:', output[0].squeeze())\n",
        "      print('Truth:', label[0].squeeze())\n",
        "      torch.save(model1, '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/' + 'lstm_generator_style.pt')\n",
        "print('Finished training model 1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchaudio/functional.py:318: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Loss: 1384.4371337890625\n",
            "Prediction: tensor([ 0.0979,  0.0709, -0.0172,  0.1380,  0.0943,  0.0729,  0.1376,  0.0301,\n",
            "        -0.1203,  0.0604, -0.0444,  0.0087, -0.0766,  0.0519, -0.1003, -0.0480,\n",
            "        -0.0787, -0.0579,  0.0929, -0.1495,  0.0436, -0.0762,  0.0178, -0.0273,\n",
            "        -0.0955, -0.0251, -0.1167, -0.0939,  0.0188, -0.1802,  0.0601, -0.1456,\n",
            "        -0.1388,  0.0243,  0.1544,  0.0017,  0.0256,  0.0708,  0.1274, -0.0072,\n",
            "        -0.0762, -0.1948, -0.0014,  0.2305, -0.0674, -0.0271, -0.0906, -0.0120,\n",
            "        -0.2079, -0.0411], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.2684, -0.2747, -0.2785, -0.2842, -0.2911, -0.2935, -0.2924, -0.2939,\n",
            "        -0.2967, -0.2957, -0.2944, -0.2983, -0.3048, -0.3100, -0.3163, -0.3256,\n",
            "        -0.3354, -0.3455, -0.3581, -0.3733, -0.3904, -0.4108, -0.4335, -0.4558,\n",
            "        -0.4771, -0.4982, -0.5172, -0.5338, -0.5502, -0.5670, -0.5827, -0.5987,\n",
            "        -0.6172, -0.6358, -0.6505, -0.6623, -0.6733, -0.6830, -0.6906, -0.6969,\n",
            "        -0.7014, -0.7049, -0.7099, -0.7161, -0.7193, -0.7180, -0.7149, -0.7120,\n",
            "        -0.7077, -0.7004], device='cuda:0')\n",
            "Epoch: 0\n",
            "Loss: 0.4596755802631378\n",
            "Prediction: tensor([ 0.0026,  0.0033,  0.0013,  0.0077,  0.0038, -0.0010, -0.0002,  0.0026,\n",
            "         0.0014,  0.0004,  0.0091, -0.0015,  0.0012,  0.0007,  0.0016,  0.0009,\n",
            "         0.0117,  0.0003, -0.0015, -0.0016,  0.0013,  0.0024,  0.0043,  0.0050,\n",
            "         0.0072,  0.0057,  0.0232,  0.0038,  0.0034,  0.0031,  0.0020,  0.0024,\n",
            "         0.0027,  0.0013,  0.0014,  0.0016,  0.0071,  0.0015,  0.0022,  0.0146,\n",
            "         0.0038,  0.0020,  0.0038,  0.0025,  0.0036,  0.0159,  0.0042,  0.0165,\n",
            "         0.0034,  0.0022], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.3997, -0.3922, -0.1512, -0.0912, -0.2853, -0.3465, -0.2216, -0.1683,\n",
            "        -0.1734, -0.1274, -0.1138, -0.1493, -0.1735, -0.2295, -0.3068, -0.3469,\n",
            "        -0.3652, -0.3215, -0.2371, -0.2767, -0.3839, -0.3929, -0.4025, -0.4595,\n",
            "        -0.4488, -0.4491, -0.5216, -0.5460, -0.5503, -0.5783, -0.5368, -0.5005,\n",
            "        -0.5569, -0.5815, -0.5576, -0.5562, -0.5373, -0.5448, -0.5966, -0.5638,\n",
            "        -0.5103, -0.5498, -0.5385, -0.4562, -0.4447, -0.4321, -0.3836, -0.4262,\n",
            "        -0.5111, -0.5673], device='cuda:0')\n",
            "Epoch: 1\n",
            "Loss: 0.4893541634082794\n",
            "Prediction: tensor([ 2.3669e-03,  6.6420e-04,  2.4906e-03,  2.9739e-03, -2.1323e-04,\n",
            "         3.9743e-03,  8.1194e-04, -4.4089e-03, -5.6384e-05,  1.7037e-03,\n",
            "         9.2392e-03, -4.6739e-03,  1.1647e-03,  4.2533e-03, -4.4079e-04,\n",
            "         4.2526e-03, -3.0914e-03,  4.4021e-03, -3.2121e-03, -2.6546e-03,\n",
            "         1.7532e-03,  5.0123e-04, -2.1702e-03, -5.0962e-03, -9.9658e-03,\n",
            "        -1.2041e-02, -1.4518e-02, -4.7474e-03, -2.4241e-03, -2.2270e-03,\n",
            "        -3.2827e-04, -8.4195e-05, -1.7182e-03,  4.5843e-04, -4.1236e-03,\n",
            "        -3.2728e-03, -7.2640e-03,  1.1409e-03,  1.9909e-03, -1.0650e-02,\n",
            "        -1.9933e-03,  1.9973e-03, -7.4690e-04,  1.4149e-04, -8.8483e-05,\n",
            "        -9.6679e-03, -3.3436e-04,  1.1050e-03,  2.3665e-03,  2.2283e-03],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([ 0.1296,  0.1553,  0.1877,  0.2253,  0.2497,  0.2555,  0.2536,  0.2578,\n",
            "         0.2772,  0.3080,  0.3294,  0.3313,  0.3322,  0.3364,  0.3343,  0.3562,\n",
            "         0.4222,  0.4781,  0.4906,  0.4983,  0.5084,  0.4930,  0.4741,  0.4737,\n",
            "         0.4652,  0.4475,  0.4426,  0.4279,  0.3776,  0.3086,  0.2395,  0.1765,\n",
            "         0.1285,  0.0871,  0.0243, -0.0722, -0.1807, -0.2665, -0.3214, -0.3629,\n",
            "        -0.3939, -0.4102, -0.4244, -0.4355, -0.4299, -0.4220, -0.4235, -0.4187,\n",
            "        -0.4036, -0.3742], device='cuda:0')\n",
            "Epoch: 1\n",
            "Loss: 0.5393247604370117\n",
            "Prediction: tensor([ 0.0040,  0.0028,  0.0015,  0.0024, -0.0053,  0.0006, -0.0002, -0.0045,\n",
            "         0.0003,  0.0012,  0.0046, -0.0082, -0.0038, -0.0033, -0.0131, -0.0049,\n",
            "         0.0075, -0.0050, -0.0114, -0.0169, -0.0083, -0.0079, -0.0091, -0.0124,\n",
            "        -0.0011, -0.0110,  0.0075, -0.0102, -0.0090, -0.0087, -0.0059, -0.0072,\n",
            "        -0.0079, -0.0056, -0.0115, -0.0093, -0.0026, -0.0066, -0.0044, -0.0155,\n",
            "        -0.0121, -0.0084, -0.0062, -0.0100, -0.0093, -0.0122, -0.0134, -0.0055,\n",
            "        -0.0122, -0.0111], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.], device='cuda:0')\n",
            "Epoch: 2\n",
            "Loss: 0.5140426158905029\n",
            "Prediction: tensor([-0.0084, -0.0095, -0.0136, -0.0040, -0.0063, -0.0141, -0.0190, -0.0233,\n",
            "        -0.0224, -0.0200, -0.0103, -0.0131, -0.0215, -0.0160, -0.0168, -0.0146,\n",
            "         0.0143, -0.0096, -0.0023, -0.0107, -0.0084, -0.0076, -0.0125, -0.0179,\n",
            "        -0.0073, -0.0152, -0.0034, -0.0136, -0.0081, -0.0070, -0.0057, -0.0069,\n",
            "        -0.0114, -0.0053, -0.0206, -0.0131, -0.0241, -0.0015, -0.0029,  0.0002,\n",
            "         0.0025,  0.0006, -0.0003,  0.0119,  0.0040,  0.0206,  0.0111,  0.0174,\n",
            "         0.0106,  0.0084], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.], device='cuda:0')\n",
            "Epoch: 2\n",
            "Loss: 0.5289610624313354\n",
            "Prediction: tensor([-6.0777e-03, -1.1921e-02, -6.5714e-03, -1.2422e-02, -1.4359e-02,\n",
            "        -3.2905e-03, -6.3737e-03, -1.0861e-02, -8.5791e-03, -8.4333e-03,\n",
            "         5.6214e-02, -2.3690e-02, -1.0856e-02, -4.5646e-03, -1.7976e-02,\n",
            "        -7.1213e-03,  3.0883e-02, -9.4087e-04,  6.2684e-03, -4.5803e-03,\n",
            "         9.4085e-04,  4.8640e-04, -2.4191e-03, -6.7858e-03,  3.7588e-03,\n",
            "         9.3123e-03,  4.5149e-03, -2.0095e-03,  6.1772e-04, -1.1032e-03,\n",
            "         1.9703e-03, -5.1752e-05, -3.3403e-03,  5.9959e-03, -8.1919e-03,\n",
            "        -8.5613e-03,  4.0101e-03,  3.5845e-03,  1.0303e-03, -3.1974e-02,\n",
            "        -8.3196e-03, -5.5488e-03, -9.2123e-03, -4.2635e-03, -7.5332e-03,\n",
            "        -6.7622e-02, -1.1395e-02, -7.0695e-02, -1.2116e-02, -1.1491e-02],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([ 0.0376,  0.0426,  0.0474,  0.0518,  0.0553,  0.0579,  0.0590,  0.0584,\n",
            "         0.0563,  0.0533,  0.0499,  0.0458,  0.0415,  0.0381,  0.0359,  0.0338,\n",
            "         0.0319,  0.0307,  0.0294,  0.0274,  0.0243,  0.0198,  0.0137,  0.0067,\n",
            "        -0.0007, -0.0087, -0.0173, -0.0266, -0.0358, -0.0449, -0.0540, -0.0621,\n",
            "        -0.0687, -0.0739, -0.0772, -0.0783, -0.0779, -0.0769, -0.0759, -0.0752,\n",
            "        -0.0753, -0.0768, -0.0800, -0.0850, -0.0912, -0.0987, -0.1071, -0.1159,\n",
            "        -0.1251, -0.1341], device='cuda:0')\n",
            "Epoch: 3\n",
            "Loss: 0.5156061053276062\n",
            "Prediction: tensor([-0.0182, -0.0054, -0.0197, -0.0208, -0.0288, -0.0228, -0.0153, -0.0172,\n",
            "        -0.0213, -0.0215,  0.0103, -0.0223, -0.0193, -0.0195, -0.0107, -0.0177,\n",
            "         0.0274, -0.0134,  0.0048,  0.0025, -0.0089, -0.0080, -0.0079, -0.0052,\n",
            "         0.0385,  0.0301,  0.0374, -0.0070, -0.0073, -0.0054, -0.0055, -0.0056,\n",
            "         0.0001, -0.0062,  0.0009,  0.0036,  0.0118, -0.0001, -0.0085,  0.0394,\n",
            "         0.0090, -0.0015,  0.0048,  0.0047,  0.0002,  0.0222,  0.0001,  0.0370,\n",
            "        -0.0025, -0.0031], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.0407, -0.0323, -0.0498, -0.0536, -0.1078, -0.2300, -0.2981, -0.2877,\n",
            "        -0.3061, -0.3814, -0.4690, -0.5338, -0.5341, -0.4859, -0.4833, -0.5512,\n",
            "        -0.6146, -0.6349, -0.6469, -0.6758, -0.7157, -0.7654, -0.8062, -0.7870,\n",
            "        -0.7131, -0.6564, -0.6316, -0.6096, -0.6015, -0.5905, -0.5285, -0.4515,\n",
            "        -0.4153, -0.3878, -0.3423, -0.3161, -0.3215, -0.3231, -0.3072, -0.3066,\n",
            "        -0.3524, -0.4360, -0.5297, -0.5883, -0.5748, -0.5471, -0.5702, -0.5761,\n",
            "        -0.5404, -0.5609], device='cuda:0')\n",
            "Epoch: 3\n",
            "Loss: 0.5013810992240906\n",
            "Prediction: tensor([ 0.0051,  0.0037,  0.0026,  0.0180,  0.0104,  0.0048, -0.0036,  0.0154,\n",
            "        -0.0035, -0.0040, -0.0058,  0.0155, -0.0055, -0.0106,  0.0083, -0.0089,\n",
            "        -0.0574, -0.0100, -0.0009, -0.0007, -0.0167, -0.0206, -0.0176,  0.0065,\n",
            "         0.0005, -0.0193, -0.0140, -0.0214, -0.0246, -0.0212, -0.0119, -0.0152,\n",
            "        -0.0126, -0.0111,  0.0037,  0.0026, -0.0029, -0.0254, -0.0136,  0.0081,\n",
            "        -0.0225, -0.0181, -0.0139, -0.0273, -0.0179,  0.0069, -0.0136,  0.0561,\n",
            "        -0.0126, -0.0217], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([0.0070, 0.0136, 0.0230, 0.0310, 0.0388, 0.0499, 0.0639, 0.0767, 0.0876,\n",
            "        0.1009, 0.1152, 0.1263, 0.1377, 0.1514, 0.1628, 0.1717, 0.1806, 0.1874,\n",
            "        0.1917, 0.1969, 0.2027, 0.2061, 0.2070, 0.2070, 0.2051, 0.1992, 0.1918,\n",
            "        0.1860, 0.1786, 0.1680, 0.1590, 0.1515, 0.1423, 0.1342, 0.1283, 0.1210,\n",
            "        0.1145, 0.1121, 0.1111, 0.1085, 0.1060, 0.1063, 0.1082, 0.1057, 0.0973,\n",
            "        0.0880, 0.0797, 0.0711, 0.0618, 0.0506], device='cuda:0')\n",
            "Epoch: 4\n",
            "Loss: 0.01967715099453926\n",
            "Prediction: tensor([ 0.0098, -0.0275,  0.0324, -0.0671, -0.0501, -0.0187,  0.0007, -0.0153,\n",
            "         0.0149, -0.0226, -0.0920, -0.0548, -0.0353,  0.0092, -0.0566,  0.0236,\n",
            "        -0.0634, -0.0036, -0.0464, -0.0169, -0.0144, -0.0210, -0.0066, -0.0031,\n",
            "        -0.0388, -0.0536, -0.0713, -0.0073,  0.0481,  0.0419,  0.0159,  0.0559,\n",
            "         0.0116,  0.0092, -0.0432, -0.0322, -0.0561, -0.0003,  0.0042, -0.0104,\n",
            "        -0.0308,  0.0427, -0.0438,  0.0121,  0.0288, -0.0206,  0.0279, -0.0597,\n",
            "         0.0332,  0.0227], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-2.5940e-03, -2.4719e-03, -2.4719e-03, -2.4414e-03, -2.4109e-03,\n",
            "        -2.4109e-03, -2.1973e-03, -1.9531e-03, -1.8921e-03, -1.8616e-03,\n",
            "        -1.8311e-03, -1.8921e-03, -1.7395e-03, -1.4954e-03, -1.5869e-03,\n",
            "        -1.7395e-03, -1.5564e-03, -1.4038e-03, -1.4038e-03, -1.3428e-03,\n",
            "        -1.2817e-03, -1.2512e-03, -1.1292e-03, -1.0986e-03, -1.0681e-03,\n",
            "        -9.7656e-04, -1.0071e-03, -9.4604e-04, -7.0190e-04, -6.7139e-04,\n",
            "        -7.9346e-04, -7.0190e-04, -4.8828e-04, -3.9673e-04, -3.3569e-04,\n",
            "        -2.7466e-04, -2.7466e-04, -3.0518e-04, -2.4414e-04, -1.2207e-04,\n",
            "        -6.1035e-05, -9.1553e-05, -6.1035e-05,  3.0518e-05,  1.2207e-04,\n",
            "         3.3569e-04,  4.8828e-04,  3.0518e-04,  2.4414e-04,  5.4932e-04],\n",
            "       device='cuda:0')\n",
            "Epoch: 4\n",
            "Loss: 0.0027498980052769184\n",
            "Prediction: tensor([ 0.0030,  0.0097,  0.0006,  0.0142,  0.0104,  0.0015,  0.0024,  0.0026,\n",
            "         0.0016,  0.0045, -0.0067, -0.0090,  0.0040,  0.0020, -0.0083, -0.0057,\n",
            "        -0.0070, -0.0092, -0.0106, -0.0139, -0.0148, -0.0161, -0.0148, -0.0169,\n",
            "        -0.0121, -0.0081, -0.0096, -0.0167, -0.0179, -0.0180, -0.0201, -0.0177,\n",
            "        -0.0220, -0.0198, -0.0216, -0.0235, -0.0260, -0.0233, -0.0229, -0.0211,\n",
            "        -0.0246, -0.0261, -0.0219, -0.0207, -0.0265, -0.0205, -0.0195, -0.0101,\n",
            "        -0.0133, -0.0116], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([ 0.0181,  0.0184,  0.0063, -0.0040,  0.0309,  0.0343, -0.0109, -0.0185,\n",
            "        -0.0142, -0.0333, -0.0189,  0.0077,  0.0074,  0.0309,  0.0532,  0.0097,\n",
            "        -0.0326, -0.0195, -0.0047, -0.0045,  0.0046,  0.0091,  0.0013, -0.0078,\n",
            "        -0.0110, -0.0012,  0.0132,  0.0170,  0.0089, -0.0141, -0.0339, -0.0179,\n",
            "         0.0082,  0.0027, -0.0143, -0.0257, -0.0390, -0.0317, -0.0011,  0.0131,\n",
            "         0.0045, -0.0071, -0.0132, -0.0030,  0.0067, -0.0088, -0.0181, -0.0039,\n",
            "        -0.0054, -0.0160], device='cuda:0')\n",
            "Epoch: 5\n",
            "Loss: 0.0014983776491135359\n",
            "Prediction: tensor([0.3204, 0.3431, 0.3483, 0.3751, 0.3868, 0.4259, 0.4240, 0.4256, 0.4578,\n",
            "        0.4964, 0.5083, 0.5444, 0.5564, 0.5914, 0.6257, 0.6397, 0.6434, 0.6734,\n",
            "        0.6805, 0.6888, 0.7004, 0.7090, 0.7130, 0.7202, 0.7366, 0.7219, 0.7092,\n",
            "        0.6835, 0.6672, 0.6368, 0.6106, 0.5803, 0.5663, 0.5403, 0.5146, 0.4957,\n",
            "        0.4579, 0.4035, 0.3748, 0.3179, 0.2988, 0.2841, 0.2382, 0.2138, 0.2290,\n",
            "        0.1556, 0.1783, 0.1390, 0.1671, 0.1589], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([0.3222, 0.3469, 0.3674, 0.3813, 0.3924, 0.4084, 0.4283, 0.4462, 0.4631,\n",
            "        0.4830, 0.5042, 0.5245, 0.5474, 0.5759, 0.6077, 0.6398, 0.6737, 0.7111,\n",
            "        0.7476, 0.7729, 0.7809, 0.7811, 0.7872, 0.7925, 0.7756, 0.7363, 0.6990,\n",
            "        0.6724, 0.6381, 0.5891, 0.5475, 0.5291, 0.5179, 0.4922, 0.4545, 0.4239,\n",
            "        0.4054, 0.3827, 0.3426, 0.2982, 0.2711, 0.2559, 0.2307, 0.1990, 0.1859,\n",
            "        0.1916, 0.1892, 0.1714, 0.1623, 0.1749], device='cuda:0')\n",
            "Epoch: 5\n",
            "Loss: 0.0012268942082300782\n",
            "Prediction: tensor([ 0.0022, -0.0220, -0.0530, -0.0505, -0.0734, -0.1060, -0.1346, -0.1705,\n",
            "        -0.2068, -0.2113, -0.2702, -0.2563, -0.2994, -0.2973, -0.3141, -0.3462,\n",
            "        -0.3140, -0.3417, -0.3232, -0.3183, -0.3022, -0.2857, -0.2814, -0.2617,\n",
            "        -0.2296, -0.1949, -0.1539, -0.1135, -0.0790, -0.0371,  0.0243,  0.0321,\n",
            "         0.0782,  0.1265,  0.1293,  0.1729,  0.1891,  0.2017,  0.2056,  0.2048,\n",
            "         0.2104,  0.2071,  0.2242,  0.2151,  0.1910,  0.1666,  0.1526,  0.1512,\n",
            "         0.1434,  0.1363], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.0627, -0.0106,  0.0031, -0.0159, -0.0276, -0.0664, -0.1594, -0.2250,\n",
            "        -0.2109, -0.2042, -0.2634, -0.3114, -0.3033, -0.3132, -0.3642, -0.3606,\n",
            "        -0.2715, -0.2097, -0.2415, -0.2895, -0.3004, -0.3237, -0.3676, -0.3660,\n",
            "        -0.3076, -0.2462, -0.1987, -0.1341, -0.0367,  0.0564,  0.0917,  0.0780,\n",
            "         0.0771,  0.1100,  0.1417,  0.1424,  0.1146,  0.0921,  0.1201,  0.2031,\n",
            "         0.2917,  0.3374,  0.3328,  0.2943,  0.2315,  0.1548,  0.0853,  0.0262,\n",
            "        -0.0474, -0.1447], device='cuda:0')\n",
            "Epoch: 6\n",
            "Loss: 0.0010497445473447442\n",
            "Prediction: tensor([-0.0468, -0.0652, -0.0565, -0.0577, -0.0459, -0.0281, -0.0156, -0.0123,\n",
            "        -0.0070,  0.0281,  0.0381,  0.0490,  0.0357,  0.0501,  0.0509,  0.0492,\n",
            "         0.0648,  0.0596,  0.0715,  0.0700,  0.0825,  0.0906,  0.0876,  0.0870,\n",
            "         0.0944,  0.1095,  0.1278,  0.1575,  0.1777,  0.2010,  0.2370,  0.2399,\n",
            "         0.2649,  0.3102,  0.3134,  0.3621,  0.3854,  0.3980,  0.4145,  0.4286,\n",
            "         0.4367,  0.4370,  0.4708,  0.4635,  0.4558,  0.4394,  0.4310,  0.4128,\n",
            "         0.3942,  0.3854], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.0758, -0.0688, -0.0587, -0.0518, -0.0482, -0.0424, -0.0305, -0.0122,\n",
            "         0.0115,  0.0351,  0.0448,  0.0365,  0.0295,  0.0358,  0.0447,  0.0545,\n",
            "         0.0704,  0.0803,  0.0763,  0.0703,  0.0703,  0.0771,  0.0909,  0.1021,\n",
            "         0.1040,  0.1067,  0.1164,  0.1312,  0.1552,  0.1889,  0.2229,  0.2532,\n",
            "         0.2837,  0.3159,  0.3452,  0.3672,  0.3854,  0.4046,  0.4221,  0.4376,\n",
            "         0.4537,  0.4621,  0.4590,  0.4573,  0.4585,  0.4466,  0.4186,  0.3899,\n",
            "         0.3748,  0.3691], device='cuda:0')\n",
            "Epoch: 6\n",
            "Loss: 0.0006687120185233653\n",
            "Prediction: tensor([-0.1712, -0.1695, -0.1367, -0.1660, -0.1528, -0.1370, -0.1150, -0.0819,\n",
            "        -0.0727, -0.0712, -0.0035, -0.0023,  0.0234,  0.0084,  0.0419,  0.0964,\n",
            "         0.0634,  0.1603,  0.1668,  0.2247,  0.2815,  0.3380,  0.4090,  0.4485,\n",
            "         0.4992,  0.5434,  0.5725,  0.5818,  0.5827,  0.6097,  0.5978,  0.6393,\n",
            "         0.6236,  0.6250,  0.6232,  0.6275,  0.6108,  0.5906,  0.5955,  0.5637,\n",
            "         0.5736,  0.5463,  0.4946,  0.4962,  0.4862,  0.4686,  0.4488,  0.4201,\n",
            "         0.4061,  0.3903], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.1838, -0.1826, -0.1741, -0.1576, -0.1351, -0.1171, -0.1095, -0.0973,\n",
            "        -0.0684, -0.0345, -0.0056,  0.0193,  0.0349,  0.0366,  0.0324,  0.0327,\n",
            "         0.0487,  0.0897,  0.1513,  0.2211,  0.2902,  0.3539,  0.4155,  0.4816,\n",
            "         0.5456,  0.5940,  0.6211,  0.6244,  0.6046,  0.5776,  0.5647,  0.5717,\n",
            "         0.5897,  0.6092,  0.6225,  0.6222,  0.6097,  0.5974,  0.5907,  0.5834,\n",
            "         0.5735,  0.5592,  0.5306,  0.4876,  0.4487,  0.4283,  0.4265,  0.4344,\n",
            "         0.4333,  0.4113], device='cuda:0')\n",
            "Epoch: 7\n",
            "Loss: 0.0007361085154116154\n",
            "Prediction: tensor([-0.2854, -0.2908, -0.3056, -0.3095, -0.3170, -0.3161, -0.3109, -0.3144,\n",
            "        -0.3014, -0.2939, -0.2668, -0.2474, -0.2293, -0.2117, -0.1892, -0.1756,\n",
            "        -0.1461, -0.1336, -0.1102, -0.0940, -0.0768, -0.0605, -0.0392, -0.0199,\n",
            "         0.0011,  0.0182,  0.0250,  0.0375,  0.0419,  0.0446,  0.0455,  0.0391,\n",
            "         0.0240,  0.0080, -0.0156, -0.0412, -0.0634, -0.0904, -0.1243, -0.1432,\n",
            "        -0.1794, -0.2181, -0.2495, -0.2801, -0.3139, -0.3385, -0.3593, -0.3709,\n",
            "        -0.3762, -0.3759], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.2818, -0.2404, -0.2495, -0.3344, -0.3501, -0.3001, -0.3059, -0.3218,\n",
            "        -0.3069, -0.3168, -0.2913, -0.2100, -0.1913, -0.2012, -0.1593, -0.1608,\n",
            "        -0.2011, -0.1631, -0.1046, -0.0877, -0.0543, -0.0338, -0.0494, -0.0228,\n",
            "         0.0208,  0.0191,  0.0179,  0.0234,  0.0116,  0.0240,  0.0574,  0.0801,\n",
            "         0.0843,  0.0164, -0.0815, -0.0648, -0.0067, -0.0628, -0.1334, -0.1318,\n",
            "        -0.1649, -0.2264, -0.2291, -0.2295, -0.2773, -0.3247, -0.3498, -0.3653,\n",
            "        -0.3949, -0.4329], device='cuda:0')\n",
            "Epoch: 7\n",
            "Loss: 0.0002281294873682782\n",
            "Prediction: tensor([-0.0439, -0.0450, -0.0540, -0.0550, -0.0560, -0.0570, -0.0612, -0.0681,\n",
            "        -0.0715, -0.0722, -0.0704, -0.0766, -0.0770, -0.0781, -0.0775, -0.0862,\n",
            "        -0.0852, -0.0891, -0.0905, -0.0808, -0.0806, -0.0823, -0.0835, -0.0859,\n",
            "        -0.0836, -0.0936, -0.0886, -0.0794, -0.0786, -0.0771, -0.0793, -0.0844,\n",
            "        -0.0800, -0.0766, -0.0737, -0.0714, -0.0671, -0.0704, -0.0661, -0.0641,\n",
            "        -0.0682, -0.0645, -0.0577, -0.0508, -0.0550, -0.0616, -0.0564, -0.0499,\n",
            "        -0.0542, -0.0536], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.0467, -0.0496, -0.0538, -0.0578, -0.0605, -0.0634, -0.0669, -0.0699,\n",
            "        -0.0725, -0.0753, -0.0775, -0.0793, -0.0812, -0.0833, -0.0852, -0.0863,\n",
            "        -0.0869, -0.0878, -0.0880, -0.0874, -0.0872, -0.0872, -0.0865, -0.0855,\n",
            "        -0.0851, -0.0850, -0.0846, -0.0837, -0.0826, -0.0818, -0.0805, -0.0786,\n",
            "        -0.0770, -0.0760, -0.0752, -0.0739, -0.0720, -0.0703, -0.0687, -0.0666,\n",
            "        -0.0648, -0.0637, -0.0617, -0.0598, -0.0592, -0.0576, -0.0553, -0.0540,\n",
            "        -0.0529, -0.0513], device='cuda:0')\n",
            "Epoch: 8\n",
            "Loss: 0.00028625846607610583\n",
            "Prediction: tensor([0.6552, 0.6651, 0.6817, 0.6896, 0.6854, 0.6525, 0.6452, 0.6212, 0.5931,\n",
            "        0.5711, 0.5727, 0.5131, 0.4931, 0.4574, 0.4257, 0.4048, 0.4229, 0.3770,\n",
            "        0.3561, 0.3494, 0.3646, 0.3689, 0.3711, 0.3879, 0.3874, 0.3980, 0.3604,\n",
            "        0.3555, 0.3453, 0.3423, 0.3504, 0.3465, 0.3545, 0.3490, 0.3404, 0.3534,\n",
            "        0.3543, 0.3574, 0.3555, 0.3545, 0.3342, 0.3076, 0.2814, 0.2670, 0.2427,\n",
            "        0.2154, 0.1956, 0.1726, 0.1669, 0.1615], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([0.6716, 0.6838, 0.6850, 0.6734, 0.6556, 0.6358, 0.6172, 0.6036, 0.5927,\n",
            "        0.5756, 0.5469, 0.5116, 0.4789, 0.4516, 0.4297, 0.4146, 0.4022, 0.3863,\n",
            "        0.3723, 0.3676, 0.3670, 0.3673, 0.3747, 0.3862, 0.3929, 0.3948, 0.3911,\n",
            "        0.3743, 0.3532, 0.3475, 0.3540, 0.3544, 0.3518, 0.3579, 0.3629, 0.3571,\n",
            "        0.3548, 0.3627, 0.3641, 0.3524, 0.3369, 0.3187, 0.2925, 0.2643, 0.2412,\n",
            "        0.2219, 0.2036, 0.1870, 0.1693, 0.1448], device='cuda:0')\n",
            "Epoch: 8\n",
            "Loss: 0.00016615123604424298\n",
            "Prediction: tensor([-0.3549, -0.3566, -0.3650, -0.3654, -0.3664, -0.3737, -0.3697, -0.3646,\n",
            "        -0.3674, -0.3640, -0.3681, -0.3564, -0.3415, -0.3356, -0.3321, -0.3368,\n",
            "        -0.3392, -0.3210, -0.3196, -0.3098, -0.3019, -0.2952, -0.2922, -0.2916,\n",
            "        -0.2869, -0.2803, -0.2850, -0.2893, -0.2890, -0.2835, -0.2702, -0.2479,\n",
            "        -0.2165, -0.1760, -0.1367, -0.0854, -0.0361,  0.0192,  0.0791,  0.1382,\n",
            "         0.1883,  0.2414,  0.2899,  0.3328,  0.3678,  0.3974,  0.4131,  0.4195,\n",
            "         0.4237,  0.4129], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.3609, -0.3668, -0.3707, -0.3708, -0.3679, -0.3650, -0.3661, -0.3714,\n",
            "        -0.3756, -0.3743, -0.3674, -0.3560, -0.3424, -0.3326, -0.3322, -0.3381,\n",
            "        -0.3397, -0.3315, -0.3188, -0.3079, -0.2982, -0.2906, -0.2911, -0.2964,\n",
            "        -0.2941, -0.2837, -0.2784, -0.2816, -0.2838, -0.2807, -0.2731, -0.2561,\n",
            "        -0.2242, -0.1808, -0.1329, -0.0850, -0.0383,  0.0116,  0.0689,  0.1289,\n",
            "         0.1837,  0.2344,  0.2847,  0.3318,  0.3703,  0.3977,  0.4146,  0.4229,\n",
            "         0.4227,  0.4128], device='cuda:0')\n",
            "Epoch: 9\n",
            "Loss: 0.00018305453704670072\n",
            "Prediction: tensor([-0.0751, -0.0752, -0.0750, -0.0791, -0.0826, -0.0755, -0.0729, -0.0714,\n",
            "        -0.0699, -0.0695, -0.0694, -0.0826, -0.0951, -0.1106, -0.1229, -0.1270,\n",
            "        -0.1345, -0.1232, -0.1230, -0.1070, -0.0933, -0.0752, -0.0611, -0.0487,\n",
            "        -0.0358, -0.0322, -0.0310, -0.0267, -0.0184, -0.0106, -0.0042,  0.0053,\n",
            "         0.0162,  0.0288,  0.0360,  0.0370,  0.0391,  0.0327,  0.0188,  0.0020,\n",
            "        -0.0261, -0.0455, -0.0722, -0.0916, -0.1192, -0.1460, -0.1662, -0.1946,\n",
            "        -0.2040, -0.2086], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([-0.0816, -0.0802, -0.0772, -0.0772, -0.0809, -0.0839, -0.0822, -0.0759,\n",
            "        -0.0695, -0.0677, -0.0730, -0.0848, -0.1003, -0.1148, -0.1245, -0.1297,\n",
            "        -0.1316, -0.1289, -0.1205, -0.1090, -0.0972, -0.0837, -0.0687, -0.0556,\n",
            "        -0.0461, -0.0389, -0.0331, -0.0289, -0.0245, -0.0186, -0.0113, -0.0016,\n",
            "         0.0115,  0.0240,  0.0315,  0.0335,  0.0317,  0.0260,  0.0156, -0.0006,\n",
            "        -0.0240, -0.0513, -0.0756, -0.0960, -0.1179, -0.1429, -0.1673, -0.1889,\n",
            "        -0.2087, -0.2298], device='cuda:0')\n",
            "Epoch: 9\n",
            "Loss: 0.00010604658746160567\n",
            "Prediction: tensor([0.2014, 0.2217, 0.2426, 0.2483, 0.2528, 0.2442, 0.2464, 0.2458, 0.2268,\n",
            "        0.1958, 0.1655, 0.1409, 0.1283, 0.1382, 0.1451, 0.1602, 0.1640, 0.1592,\n",
            "        0.1327, 0.1200, 0.1295, 0.1411, 0.1544, 0.1790, 0.2235, 0.2217, 0.2022,\n",
            "        0.2356, 0.2730, 0.3171, 0.3677, 0.4127, 0.4476, 0.4411, 0.4122, 0.4011,\n",
            "        0.3935, 0.3921, 0.3675, 0.3298, 0.2905, 0.2658, 0.2192, 0.1879, 0.1681,\n",
            "        0.1550, 0.1584, 0.1441, 0.1137, 0.0928], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Truth: tensor([0.1906, 0.2119, 0.2306, 0.2459, 0.2568, 0.2577, 0.2516, 0.2422, 0.2206,\n",
            "        0.1863, 0.1616, 0.1551, 0.1463, 0.1324, 0.1397, 0.1648, 0.1657, 0.1359,\n",
            "        0.1205, 0.1349, 0.1382, 0.1185, 0.1283, 0.1890, 0.2361, 0.2210, 0.1937,\n",
            "        0.2151, 0.2628, 0.3026, 0.3633, 0.4501, 0.4818, 0.4204, 0.3621, 0.3881,\n",
            "        0.4315, 0.4013, 0.3354, 0.3143, 0.3205, 0.2875, 0.2205, 0.1747, 0.1646,\n",
            "        0.1610, 0.1466, 0.1280, 0.1105, 0.0936], device='cuda:0')\n",
            "Finished training model 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-XVVRZ8IjFU",
        "outputId": "e7eaabc0-0f35-4607-c5d4-fd03fd7b1163"
      },
      "source": [
        "# Music style transfer without any models. Modify the input audio directly. \n",
        "EPOCHS = 100\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "\n",
        "#genre_classifier = torch.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/sample_fully_conn.pt', map_location=device).eval()\n",
        "input, rate = torchaudio.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/stay.wav')\n",
        "input = input[0,:] # first channel only\n",
        "input = input[:rate * 30]\n",
        "input = input.to(device)\n",
        "input.requires_grad = True\n",
        "optim = torch.optim.Adam([input], lr=0.01)\n",
        "original = input.detach().clone()\n",
        "\n",
        "style, sr = torchaudio.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/santa.wav')\n",
        "style = style[0,:] # first channel\n",
        "style = style[:rate * 30]\n",
        "\n",
        "# Get style audio features\n",
        "style_mfcc = torchaudio.transforms.MFCC()(style).mean(dim=1).to(device)\n",
        "#style_spec = torchaudio.transforms.Spectrogram()(style)#.mean(dim=1).to(device)\n",
        "#style_mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=sr)(style)#.mean(dim=1).to(device)\n",
        "style_features = style_mfcc\n",
        "#style_features = torch.cat((style_mfcc, style_spec, style_mel_spec), dim=0)\n",
        "\n",
        "# Original input audio features\n",
        "ori = torchaudio.transforms.MFCC()(original)\n",
        "ori = ori.squeeze().mean(dim=1).to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  optim.zero_grad()\n",
        "  mfcc = torchaudio.transforms.MFCC()(input)\n",
        "  mfcc = mfcc.squeeze().mean(dim=1).to(device)\n",
        "  #spec = torchaudio.transforms.Spectrogram()(input).mean(dim=1).to(device)\n",
        "  #mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=rate)(input)#.mean(dim=1).to(device)\n",
        "  input_features = mfcc\n",
        "  #input_features = torch.cat((mfcc, spec, mel_spec), dim=0)\n",
        "  #output = genre_classifier(input_features)\n",
        "  #print(folders[output.argmax()])\n",
        "  #loss = F.cross_entropy(output.reshape((1,-1)), torch.tensor([8]).to(device))\n",
        "  #loss = 10 * F.mse_loss(input, original) + 0.3 * F.mse_loss(input, style[:len(input)])\n",
        "  loss = 30 * F.mse_loss(input_features, style_features) + 0.7 * F.mse_loss(input, original)\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  if epoch % 5 == 0:\n",
        "    print('Epoch:', epoch)\n",
        "    print('Loss:', loss.item())\n",
        "\n",
        "input = input.detach().numpy() * 32768\n",
        "write('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/output/' + 'october_diret_style.wav', rate, input.astype('int16'))\n",
        "print('Finished training model 1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchaudio/functional.py:318: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Loss: 29488.494140625\n",
            "Epoch: 5\n",
            "Loss: 5491.09765625\n",
            "Epoch: 10\n",
            "Loss: 4405.59814453125\n",
            "Epoch: 15\n",
            "Loss: 3645.991943359375\n",
            "Epoch: 20\n",
            "Loss: 3012.6220703125\n",
            "Epoch: 25\n",
            "Loss: 2519.351318359375\n",
            "Epoch: 30\n",
            "Loss: 2183.3310546875\n",
            "Epoch: 35\n",
            "Loss: 1974.0423583984375\n",
            "Epoch: 40\n",
            "Loss: 1849.1259765625\n",
            "Epoch: 45\n",
            "Loss: 1773.8585205078125\n",
            "Epoch: 50\n",
            "Loss: 1726.780029296875\n",
            "Epoch: 55\n",
            "Loss: 1709.423095703125\n",
            "Epoch: 60\n",
            "Loss: 1693.6968994140625\n",
            "Epoch: 65\n",
            "Loss: 1694.10205078125\n",
            "Epoch: 70\n",
            "Loss: 1683.2303466796875\n",
            "Epoch: 75\n",
            "Loss: 1682.75927734375\n",
            "Epoch: 80\n",
            "Loss: 1678.3603515625\n",
            "Epoch: 85\n",
            "Loss: 1680.510009765625\n",
            "Epoch: 90\n",
            "Loss: 1692.53466796875\n",
            "Epoch: 95\n",
            "Loss: 1689.5057373046875\n",
            "Finished training model 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9-bbt5U8SUK"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/'\n",
        "torch.save(model1, path + 'lstm1_longer.pt')\n",
        "torch.save(model2, path + 'lstm2_longer.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXlcEwHGpON"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/'\n",
        "model1 = torch.load(path + 'lstm_generator_style.pt')\n",
        "model2 = torch.load(path + 'lstm2_longer2nd.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLVZaBBBE_s"
      },
      "source": [
        "# Testing Music Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rig55S_Dwl_5",
        "outputId": "da2c7ae7-5d3b-4eed-aae6-fd9cb2eaf956"
      },
      "source": [
        "BATCH_SIZE = 500\n",
        "FEATURE_SIZE = 50 #8000#50\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/'\n",
        "rate, music = read(path + \"santa.wav\")\n",
        "data1 = MusicGenDataset(music[:,0], FEATURE_SIZE, FEATURE_SIZE)\n",
        "#data2 = MusicGenDataset(music[:,1], FEATURE_SIZE)\n",
        "loader1 = torch.utils.data.DataLoader(data1, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=False, **kwargs)\n",
        "#loader2 = torch.utils.data.DataLoader(data2, batch_size=BATCH_SIZE,\n",
        "                                            #shuffle=False, **kwargs)\n",
        "\n",
        "pred1 = torch.tensor([]).float().to(device)\n",
        "#pred1 = []\n",
        "#pred2 = torch.tensor([]).float().to(device)\n",
        "model1 = model1.to(device).eval()\n",
        "#model2 = model2.to(device).eval()\n",
        "with torch.no_grad():\n",
        "  \n",
        "  for idx, (data, label) in enumerate(loader1):\n",
        "    data = data.float().to(device)\n",
        "    #data = data.to(device)\n",
        "    #print(data.shape)\n",
        "    #data = data.permute(2,0,1)\n",
        "    #data = data.squeeze()\n",
        "    #output, _, _ = model1(data)\n",
        "    data = data.reshape((BATCH_SIZE, FEATURE_SIZE, 1))\n",
        "    output, _ = model1(data)\n",
        "    #pred = output.squeeze()\n",
        "    #pred = pred.permute(1,0)\n",
        "    #pred = output.max(-1)[1]\n",
        "    #pred = pred.cpu().detach().numpy().astype('int')\n",
        "    #pred = [decode(pred[i]) for i in range(len(pred))]\n",
        "    output = output * 32768\n",
        "    #first_50 = output[0]\n",
        "    #next = output[1:, -1]\n",
        "    #pred1 = torch.cat((pred1, first_50, next), 0)\n",
        "    pred1 = torch.cat((pred1, output.flatten()), 0)\n",
        "    del data\n",
        "    del output\n",
        "    #torch.cuda.empty_cache()\n",
        "    if idx == 250: #50:\n",
        "      break\n",
        "  print('Finished 1st channel')\n",
        "  final = pred1.cpu().detach().numpy().astype('int16')#pred1.detach().numpy().astype('int16')\n",
        "  print(final.shape)\n",
        "  path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/output/'\n",
        "  write(path + 'santa_gen_style.wav', rate, final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Finished 1st channel\n",
            "(6275000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBoCv0NHCxRO"
      },
      "source": [
        "# Audio Feature Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO33bk-BC6ei"
      },
      "source": [
        "# Audio feature data loading\n",
        "from scipy.io.wavfile import read, write\n",
        "import pandas as pd\n",
        "import random\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_folder, folders, transform=None):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "        # self.data_folder = data_folder\n",
        "        self.folders = folders\n",
        "        self.data = []\n",
        "        #self.labels = []\n",
        "        for folder in self.folders:\n",
        "          print(folder)\n",
        "          for file in os.listdir(os.path.join(data_folder, folder)):\n",
        "            #self.data.append(os.path.join(data_folder, folder, file))\n",
        "            path = os.path.join(data_folder, folder, file)\n",
        "            sr, audio = read(path)\n",
        "            #for i in range(len(audio) // 50):\n",
        "            for _ in range(50):\n",
        "              features = []\n",
        "              i = random.randrange(0, len(audio) - 50, 1)\n",
        "              y = audio[i:i + 50].astype('float')\n",
        "            #rmse = librosa.feature.rmse(y=y)\n",
        "            #chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            #spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "            #spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "            #rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "            #zcr = librosa.feature.zero_crossing_rate(y)\n",
        "              mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "            #features = [np.mean(chroma_stft), np.mean(rmse), np.mean(spec_cent), np.mean(spec_bw), np.mean(rolloff), np.mean(zcr)]   \n",
        "              for e in mfcc:\n",
        "                features.append(np.mean(e))\n",
        "              self.data.append((y, np.array(features)))\n",
        "            \n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        y, features = self.data[idx]\n",
        "        \n",
        "        return y, features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyAQrGT8D0hf"
      },
      "source": [
        "# Fully connected network that extracts features from raw audio amplitudes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MusicFeatureNet(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(MusicIdentifyNet, self).__init__()\n",
        "        # input 50\n",
        "        self.output_size = output_size\n",
        "        self.fc1 = nn.Linear(50, 250)\n",
        "        self.fc2 = nn.Linear(250, 100)\n",
        "        self.fc3 = nn.Linear(100, 50)\n",
        "        self.fc4 = nn.Linear(50, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label):\n",
        "        loss_val = F.mse_loss(prediction, label)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Two0CPCBD-5D"
      },
      "source": [
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        data = data.float()\n",
        "        label = label.float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            correct += torch.sum(torch.argmax(output, dim=1) == label)\n",
        "            \n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoJDLHg0D_9i",
        "outputId": "8e9ba054-3968-461f-889a-71a0be39cb52"
      },
      "source": [
        "# Training, Testing\n",
        "def main():\n",
        "    BATCH_SIZE = 256\n",
        "    FEATURE_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 256\n",
        "    EPOCHS = 70\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/'\n",
        "\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/genres/'\n",
        "\n",
        "    #t = transforms.Compose([transforms.ToPILImage(mode='F'), transforms.Resize(32), transforms.ToTensor()])\n",
        "    t = transforms.Compose([transforms.ToTensor()])\n",
        "    #data_train = MusicDataset('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/spectrograms', folders, t)\n",
        "    data_train = MusicDataset('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/genres', folders)\n",
        "    #data_test = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    #test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "    #                                          shuffle=False, **kwargs)\n",
        "\n",
        "    model = MusicIdentifyNet(20).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "\n",
        "    try:\n",
        "        for epoch in range(EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            #test_loss, test_accuracy = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            #test_losses.append((epoch, test_loss))\n",
        "            #test_accuracies.append((epoch, test_accuracy))\n",
        "            \n",
        "            #torch.save(model, DATA_PATH + 'feature_predictor.pt')\n",
        "            \n",
        "            if epoch % 5 == 0:\n",
        "              print('Epoch:', epoch)\n",
        "              print('Training loss:', train_loss)\n",
        "            \n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        #torch.save(model, DATA_PATH + 'feature_predictor.pt')\n",
        "        \n",
        "        return model, device\n",
        "final_model, device = main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rock\n",
            "reggae\n",
            "pop\n",
            "metal\n",
            "jazz\n",
            "hiphop\n",
            "disco\n",
            "country\n",
            "classical\n",
            "blues\n",
            "Using device cpu\n",
            "num workers: 2\n",
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 41299.093750\n",
            "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 9253.749023\n",
            "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 7560.312500\n",
            "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 6261.485352\n",
            "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 6711.502930\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 7549.749023\n",
            "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 6858.142090\n",
            "Train Epoch: 0 [17920/50000 (36%)]\tLoss: 6195.878418\n",
            "Train Epoch: 0 [20480/50000 (41%)]\tLoss: 7341.308594\n",
            "Train Epoch: 0 [23040/50000 (46%)]\tLoss: 6697.265625\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 6525.827148\n",
            "Train Epoch: 0 [28160/50000 (56%)]\tLoss: 6060.447266\n",
            "Train Epoch: 0 [30720/50000 (61%)]\tLoss: 6337.792480\n",
            "Train Epoch: 0 [33280/50000 (66%)]\tLoss: 6419.886230\n",
            "Train Epoch: 0 [35840/50000 (71%)]\tLoss: 6265.835938\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 6769.198242\n",
            "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 6435.232910\n",
            "Train Epoch: 0 [43520/50000 (87%)]\tLoss: 6154.337891\n",
            "Train Epoch: 0 [46080/50000 (92%)]\tLoss: 6100.285156\n",
            "Train Epoch: 0 [48640/50000 (97%)]\tLoss: 6036.043945\n",
            "Epoch: 0\n",
            "Training loss: 7202.28191266741\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 6520.323730\n",
            "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 5978.130859\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 7105.905273\n",
            "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 6422.134277\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 7156.665039\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 6505.756836\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 6343.601074\n",
            "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 6455.118164\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 6075.690918\n",
            "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 5949.125977\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 6466.463867\n",
            "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 6817.719727\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 7053.834961\n",
            "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 5278.662109\n",
            "Train Epoch: 1 [35840/50000 (71%)]\tLoss: 6507.874023\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 6196.727539\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 6194.129395\n",
            "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 6251.813965\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 6031.673340\n",
            "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 5375.025879\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 6256.395020\n",
            "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 5996.724121\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 7054.311523\n",
            "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 5643.069336\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 5805.737305\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 6290.034668\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 6323.186035\n",
            "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 6351.972656\n",
            "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 6302.904297\n",
            "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 6298.858887\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 5277.717773\n",
            "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 6670.568848\n",
            "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 6113.768555\n",
            "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 5584.245117\n",
            "Train Epoch: 2 [35840/50000 (71%)]\tLoss: 5851.929688\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 6592.218750\n",
            "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 6093.567871\n",
            "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 6532.543945\n",
            "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 5773.554199\n",
            "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 5813.913086\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 6094.739746\n",
            "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 6085.333008\n",
            "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 6636.157227\n",
            "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 6048.681152\n",
            "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 5706.532715\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 5610.415039\n",
            "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 6300.208496\n",
            "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 6034.349609\n",
            "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 6553.397949\n",
            "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 6485.910645\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 5464.741211\n",
            "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 7527.802246\n",
            "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 6629.048340\n",
            "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 5988.580566\n",
            "Train Epoch: 3 [35840/50000 (71%)]\tLoss: 6291.964355\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 6003.481445\n",
            "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 5906.393555\n",
            "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 5581.469238\n",
            "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 6254.888672\n",
            "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 6413.406250\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 6190.867676\n",
            "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 6589.676758\n",
            "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 6361.159180\n",
            "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 6160.530273\n",
            "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 5499.355469\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 5105.691895\n",
            "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 6297.750977\n",
            "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 5370.356445\n",
            "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 5823.650391\n",
            "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 5853.039062\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 6397.845215\n",
            "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 5414.557129\n",
            "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 6135.837891\n",
            "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 5814.136719\n",
            "Train Epoch: 4 [35840/50000 (71%)]\tLoss: 5930.525391\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 5954.895508\n",
            "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 5821.741211\n",
            "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 6106.618164\n",
            "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 6194.666992\n",
            "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 6154.006836\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 6331.910645\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 6451.826172\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 6501.860352\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 5437.139648\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 5797.435547\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 6924.397461\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 5600.045898\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 5759.514648\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 6126.583008\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 5731.389160\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 5411.024414\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 6643.799805\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 6167.120605\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 5963.072266\n",
            "Train Epoch: 5 [35840/50000 (71%)]\tLoss: 6034.691895\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 5754.796387\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 5930.803711\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 6736.047852\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 6091.393555\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 5872.261719\n",
            "Epoch: 5\n",
            "Training loss: 6027.2288419762435\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 5221.562988\n",
            "Train Epoch: 6 [2560/50000 (5%)]\tLoss: 6152.699219\n",
            "Train Epoch: 6 [5120/50000 (10%)]\tLoss: 5822.893555\n",
            "Train Epoch: 6 [7680/50000 (15%)]\tLoss: 5875.772461\n",
            "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 6571.341309\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 6782.364258\n",
            "Train Epoch: 6 [15360/50000 (31%)]\tLoss: 5278.679199\n",
            "Train Epoch: 6 [17920/50000 (36%)]\tLoss: 6020.956543\n",
            "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 6073.913574\n",
            "Train Epoch: 6 [23040/50000 (46%)]\tLoss: 6250.184570\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 6460.516113\n",
            "Train Epoch: 6 [28160/50000 (56%)]\tLoss: 5323.797852\n",
            "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 6506.033691\n",
            "Train Epoch: 6 [33280/50000 (66%)]\tLoss: 5757.703613\n",
            "Train Epoch: 6 [35840/50000 (71%)]\tLoss: 6100.753418\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 5870.032715\n",
            "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 5729.767578\n",
            "Train Epoch: 6 [43520/50000 (87%)]\tLoss: 5526.055664\n",
            "Train Epoch: 6 [46080/50000 (92%)]\tLoss: 6155.905762\n",
            "Train Epoch: 6 [48640/50000 (97%)]\tLoss: 5704.526855\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 6014.929688\n",
            "Train Epoch: 7 [2560/50000 (5%)]\tLoss: 5859.710938\n",
            "Train Epoch: 7 [5120/50000 (10%)]\tLoss: 6666.092773\n",
            "Train Epoch: 7 [7680/50000 (15%)]\tLoss: 5585.229004\n",
            "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 5490.766113\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 5477.347168\n",
            "Train Epoch: 7 [15360/50000 (31%)]\tLoss: 6633.666992\n",
            "Train Epoch: 7 [17920/50000 (36%)]\tLoss: 5861.317871\n",
            "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 6386.943359\n",
            "Train Epoch: 7 [23040/50000 (46%)]\tLoss: 5972.922852\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 6684.115723\n",
            "Train Epoch: 7 [28160/50000 (56%)]\tLoss: 6839.229492\n",
            "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 5663.677734\n",
            "Train Epoch: 7 [33280/50000 (66%)]\tLoss: 6063.238770\n",
            "Train Epoch: 7 [35840/50000 (71%)]\tLoss: 4962.913574\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 5733.882324\n",
            "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 5915.053711\n",
            "Train Epoch: 7 [43520/50000 (87%)]\tLoss: 5409.780762\n",
            "Train Epoch: 7 [46080/50000 (92%)]\tLoss: 5781.810059\n",
            "Train Epoch: 7 [48640/50000 (97%)]\tLoss: 6116.559082\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 5762.095703\n",
            "Train Epoch: 8 [2560/50000 (5%)]\tLoss: 6264.320312\n",
            "Train Epoch: 8 [5120/50000 (10%)]\tLoss: 5905.905273\n",
            "Train Epoch: 8 [7680/50000 (15%)]\tLoss: 5484.859375\n",
            "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 5106.233887\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 5645.688965\n",
            "Train Epoch: 8 [15360/50000 (31%)]\tLoss: 6233.041504\n",
            "Train Epoch: 8 [17920/50000 (36%)]\tLoss: 5634.681152\n",
            "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 6191.684570\n",
            "Train Epoch: 8 [23040/50000 (46%)]\tLoss: 6660.778809\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 5916.559570\n",
            "Train Epoch: 8 [28160/50000 (56%)]\tLoss: 6469.130859\n",
            "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 6201.688477\n",
            "Train Epoch: 8 [33280/50000 (66%)]\tLoss: 5372.131836\n",
            "Train Epoch: 8 [35840/50000 (71%)]\tLoss: 6436.598633\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 5334.495117\n",
            "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 5654.075195\n",
            "Train Epoch: 8 [43520/50000 (87%)]\tLoss: 5889.481445\n",
            "Train Epoch: 8 [46080/50000 (92%)]\tLoss: 5416.812500\n",
            "Train Epoch: 8 [48640/50000 (97%)]\tLoss: 5897.762695\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 5995.958496\n",
            "Train Epoch: 9 [2560/50000 (5%)]\tLoss: 5850.316895\n",
            "Train Epoch: 9 [5120/50000 (10%)]\tLoss: 5633.807617\n",
            "Train Epoch: 9 [7680/50000 (15%)]\tLoss: 6181.093750\n",
            "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 5213.274414\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 5178.093750\n",
            "Train Epoch: 9 [15360/50000 (31%)]\tLoss: 5671.651855\n",
            "Train Epoch: 9 [17920/50000 (36%)]\tLoss: 5922.273926\n",
            "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 5580.388672\n",
            "Train Epoch: 9 [23040/50000 (46%)]\tLoss: 4792.499023\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 5601.583008\n",
            "Train Epoch: 9 [28160/50000 (56%)]\tLoss: 5309.954102\n",
            "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 5499.760254\n",
            "Train Epoch: 9 [33280/50000 (66%)]\tLoss: 5366.688477\n",
            "Train Epoch: 9 [35840/50000 (71%)]\tLoss: 5463.738770\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 6067.585938\n",
            "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 6185.609863\n",
            "Train Epoch: 9 [43520/50000 (87%)]\tLoss: 5963.103027\n",
            "Train Epoch: 9 [46080/50000 (92%)]\tLoss: 6115.493164\n",
            "Train Epoch: 9 [48640/50000 (97%)]\tLoss: 5834.020508\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 5836.863281\n",
            "Train Epoch: 10 [2560/50000 (5%)]\tLoss: 5735.502441\n",
            "Train Epoch: 10 [5120/50000 (10%)]\tLoss: 5180.208008\n",
            "Train Epoch: 10 [7680/50000 (15%)]\tLoss: 5664.334961\n",
            "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 6580.342285\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 5665.817383\n",
            "Train Epoch: 10 [15360/50000 (31%)]\tLoss: 6561.109375\n",
            "Train Epoch: 10 [17920/50000 (36%)]\tLoss: 5387.376953\n",
            "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 5449.651367\n",
            "Train Epoch: 10 [23040/50000 (46%)]\tLoss: 5438.206055\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 6543.041992\n",
            "Train Epoch: 10 [28160/50000 (56%)]\tLoss: 5678.059570\n",
            "Train Epoch: 10 [30720/50000 (61%)]\tLoss: 6149.858887\n",
            "Train Epoch: 10 [33280/50000 (66%)]\tLoss: 5916.117676\n",
            "Train Epoch: 10 [35840/50000 (71%)]\tLoss: 5710.164062\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 5480.315430\n",
            "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 6292.738281\n",
            "Train Epoch: 10 [43520/50000 (87%)]\tLoss: 6377.007812\n",
            "Train Epoch: 10 [46080/50000 (92%)]\tLoss: 6321.142578\n",
            "Train Epoch: 10 [48640/50000 (97%)]\tLoss: 5956.542480\n",
            "Epoch: 10\n",
            "Training loss: 5818.551889847736\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 6060.744629\n",
            "Train Epoch: 11 [2560/50000 (5%)]\tLoss: 5738.152832\n",
            "Train Epoch: 11 [5120/50000 (10%)]\tLoss: 5784.200195\n",
            "Train Epoch: 11 [7680/50000 (15%)]\tLoss: 5851.775391\n",
            "Train Epoch: 11 [10240/50000 (20%)]\tLoss: 5215.854492\n",
            "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 5758.283203\n",
            "Train Epoch: 11 [15360/50000 (31%)]\tLoss: 5891.653809\n",
            "Train Epoch: 11 [17920/50000 (36%)]\tLoss: 5240.846191\n",
            "Train Epoch: 11 [20480/50000 (41%)]\tLoss: 5768.812012\n",
            "Train Epoch: 11 [23040/50000 (46%)]\tLoss: 4744.000000\n",
            "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 5870.204102\n",
            "Train Epoch: 11 [28160/50000 (56%)]\tLoss: 6386.400391\n",
            "Train Epoch: 11 [30720/50000 (61%)]\tLoss: 5680.794922\n",
            "Train Epoch: 11 [33280/50000 (66%)]\tLoss: 6006.957031\n",
            "Train Epoch: 11 [35840/50000 (71%)]\tLoss: 5959.031738\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 6379.353027\n",
            "Train Epoch: 11 [40960/50000 (82%)]\tLoss: 6010.422363\n",
            "Train Epoch: 11 [43520/50000 (87%)]\tLoss: 5909.780762\n",
            "Train Epoch: 11 [46080/50000 (92%)]\tLoss: 5907.039062\n",
            "Train Epoch: 11 [48640/50000 (97%)]\tLoss: 5569.988281\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 6867.456055\n",
            "Train Epoch: 12 [2560/50000 (5%)]\tLoss: 5166.477539\n",
            "Train Epoch: 12 [5120/50000 (10%)]\tLoss: 5353.487305\n",
            "Train Epoch: 12 [7680/50000 (15%)]\tLoss: 6352.342773\n",
            "Train Epoch: 12 [10240/50000 (20%)]\tLoss: 5655.732910\n",
            "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 6296.850586\n",
            "Train Epoch: 12 [15360/50000 (31%)]\tLoss: 5431.416504\n",
            "Train Epoch: 12 [17920/50000 (36%)]\tLoss: 5627.915039\n",
            "Train Epoch: 12 [20480/50000 (41%)]\tLoss: 6278.763184\n",
            "Train Epoch: 12 [23040/50000 (46%)]\tLoss: 5741.089844\n",
            "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 6021.432617\n",
            "Train Epoch: 12 [28160/50000 (56%)]\tLoss: 5567.315918\n",
            "Train Epoch: 12 [30720/50000 (61%)]\tLoss: 6526.088867\n",
            "Train Epoch: 12 [33280/50000 (66%)]\tLoss: 5338.331543\n",
            "Train Epoch: 12 [35840/50000 (71%)]\tLoss: 6050.688477\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 5373.535645\n",
            "Train Epoch: 12 [40960/50000 (82%)]\tLoss: 5917.935547\n",
            "Train Epoch: 12 [43520/50000 (87%)]\tLoss: 5575.151367\n",
            "Train Epoch: 12 [46080/50000 (92%)]\tLoss: 5925.127930\n",
            "Train Epoch: 12 [48640/50000 (97%)]\tLoss: 6395.995117\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 5003.018555\n",
            "Train Epoch: 13 [2560/50000 (5%)]\tLoss: 6049.221680\n",
            "Train Epoch: 13 [5120/50000 (10%)]\tLoss: 5307.330078\n",
            "Train Epoch: 13 [7680/50000 (15%)]\tLoss: 6113.120117\n",
            "Train Epoch: 13 [10240/50000 (20%)]\tLoss: 5657.020996\n",
            "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 5307.781250\n",
            "Train Epoch: 13 [15360/50000 (31%)]\tLoss: 6997.678711\n",
            "Train Epoch: 13 [17920/50000 (36%)]\tLoss: 6143.998535\n",
            "Train Epoch: 13 [20480/50000 (41%)]\tLoss: 5558.820801\n",
            "Train Epoch: 13 [23040/50000 (46%)]\tLoss: 5863.078125\n",
            "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 6283.871094\n",
            "Train Epoch: 13 [28160/50000 (56%)]\tLoss: 5172.753906\n",
            "Train Epoch: 13 [30720/50000 (61%)]\tLoss: 6704.585938\n",
            "Train Epoch: 13 [33280/50000 (66%)]\tLoss: 5824.669434\n",
            "Train Epoch: 13 [35840/50000 (71%)]\tLoss: 6788.177246\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 5331.800781\n",
            "Train Epoch: 13 [40960/50000 (82%)]\tLoss: 5832.504883\n",
            "Train Epoch: 13 [43520/50000 (87%)]\tLoss: 5329.759766\n",
            "Train Epoch: 13 [46080/50000 (92%)]\tLoss: 5791.721191\n",
            "Train Epoch: 13 [48640/50000 (97%)]\tLoss: 6362.155273\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 6511.522461\n",
            "Train Epoch: 14 [2560/50000 (5%)]\tLoss: 6168.628906\n",
            "Train Epoch: 14 [5120/50000 (10%)]\tLoss: 6420.355469\n",
            "Train Epoch: 14 [7680/50000 (15%)]\tLoss: 5788.160645\n",
            "Train Epoch: 14 [10240/50000 (20%)]\tLoss: 5979.302734\n",
            "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 7020.690430\n",
            "Train Epoch: 14 [15360/50000 (31%)]\tLoss: 5847.025879\n",
            "Train Epoch: 14 [17920/50000 (36%)]\tLoss: 5742.393066\n",
            "Train Epoch: 14 [20480/50000 (41%)]\tLoss: 5500.708008\n",
            "Train Epoch: 14 [23040/50000 (46%)]\tLoss: 5321.558594\n",
            "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 6117.520020\n",
            "Train Epoch: 14 [28160/50000 (56%)]\tLoss: 5481.510742\n",
            "Train Epoch: 14 [30720/50000 (61%)]\tLoss: 5640.395508\n",
            "Train Epoch: 14 [33280/50000 (66%)]\tLoss: 6380.514648\n",
            "Train Epoch: 14 [35840/50000 (71%)]\tLoss: 5548.753418\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 5384.864258\n",
            "Train Epoch: 14 [40960/50000 (82%)]\tLoss: 5134.109863\n",
            "Train Epoch: 14 [43520/50000 (87%)]\tLoss: 5386.660645\n",
            "Train Epoch: 14 [46080/50000 (92%)]\tLoss: 5453.762695\n",
            "Train Epoch: 14 [48640/50000 (97%)]\tLoss: 4836.139160\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 5514.609375\n",
            "Train Epoch: 15 [2560/50000 (5%)]\tLoss: 5466.914551\n",
            "Train Epoch: 15 [5120/50000 (10%)]\tLoss: 5742.645020\n",
            "Train Epoch: 15 [7680/50000 (15%)]\tLoss: 5829.008789\n",
            "Train Epoch: 15 [10240/50000 (20%)]\tLoss: 5767.707031\n",
            "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 5827.446777\n",
            "Train Epoch: 15 [15360/50000 (31%)]\tLoss: 5663.490723\n",
            "Train Epoch: 15 [17920/50000 (36%)]\tLoss: 5510.647461\n",
            "Train Epoch: 15 [20480/50000 (41%)]\tLoss: 6338.854980\n",
            "Train Epoch: 15 [23040/50000 (46%)]\tLoss: 5915.908691\n",
            "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 5338.647461\n",
            "Train Epoch: 15 [28160/50000 (56%)]\tLoss: 5981.003418\n",
            "Train Epoch: 15 [30720/50000 (61%)]\tLoss: 4836.680176\n",
            "Train Epoch: 15 [33280/50000 (66%)]\tLoss: 5593.626953\n",
            "Train Epoch: 15 [35840/50000 (71%)]\tLoss: 6334.440918\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 5631.205566\n",
            "Train Epoch: 15 [40960/50000 (82%)]\tLoss: 5172.360840\n",
            "Train Epoch: 15 [43520/50000 (87%)]\tLoss: 6327.168945\n",
            "Train Epoch: 15 [46080/50000 (92%)]\tLoss: 5413.079102\n",
            "Train Epoch: 15 [48640/50000 (97%)]\tLoss: 5851.443359\n",
            "Epoch: 15\n",
            "Training loss: 5605.198964644452\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 5772.279297\n",
            "Train Epoch: 16 [2560/50000 (5%)]\tLoss: 5626.346680\n",
            "Train Epoch: 16 [5120/50000 (10%)]\tLoss: 5277.399414\n",
            "Train Epoch: 16 [7680/50000 (15%)]\tLoss: 5319.473145\n",
            "Train Epoch: 16 [10240/50000 (20%)]\tLoss: 5600.973633\n",
            "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 5952.307129\n",
            "Train Epoch: 16 [15360/50000 (31%)]\tLoss: 5673.525879\n",
            "Train Epoch: 16 [17920/50000 (36%)]\tLoss: 7066.058594\n",
            "Train Epoch: 16 [20480/50000 (41%)]\tLoss: 6128.725098\n",
            "Train Epoch: 16 [23040/50000 (46%)]\tLoss: 5048.495605\n",
            "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 5889.399414\n",
            "Train Epoch: 16 [28160/50000 (56%)]\tLoss: 5644.847656\n",
            "Train Epoch: 16 [30720/50000 (61%)]\tLoss: 5732.711426\n",
            "Train Epoch: 16 [33280/50000 (66%)]\tLoss: 5654.005371\n",
            "Train Epoch: 16 [35840/50000 (71%)]\tLoss: 5144.038086\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 5492.095215\n",
            "Train Epoch: 16 [40960/50000 (82%)]\tLoss: 6051.320801\n",
            "Train Epoch: 16 [43520/50000 (87%)]\tLoss: 5805.227539\n",
            "Train Epoch: 16 [46080/50000 (92%)]\tLoss: 5352.921387\n",
            "Train Epoch: 16 [48640/50000 (97%)]\tLoss: 5531.020020\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 6182.379883\n",
            "Train Epoch: 17 [2560/50000 (5%)]\tLoss: 5397.959961\n",
            "Train Epoch: 17 [5120/50000 (10%)]\tLoss: 5445.513672\n",
            "Train Epoch: 17 [7680/50000 (15%)]\tLoss: 5275.692871\n",
            "Train Epoch: 17 [10240/50000 (20%)]\tLoss: 6353.248535\n",
            "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 5942.894043\n",
            "Train Epoch: 17 [15360/50000 (31%)]\tLoss: 4813.051270\n",
            "Train Epoch: 17 [17920/50000 (36%)]\tLoss: 5652.801758\n",
            "Train Epoch: 17 [20480/50000 (41%)]\tLoss: 5328.795898\n",
            "Train Epoch: 17 [23040/50000 (46%)]\tLoss: 7486.907715\n",
            "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 6209.313965\n",
            "Train Epoch: 17 [28160/50000 (56%)]\tLoss: 5366.391602\n",
            "Train Epoch: 17 [30720/50000 (61%)]\tLoss: 4944.847168\n",
            "Train Epoch: 17 [33280/50000 (66%)]\tLoss: 5215.040039\n",
            "Train Epoch: 17 [35840/50000 (71%)]\tLoss: 5239.229004\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 5375.120117\n",
            "Train Epoch: 17 [40960/50000 (82%)]\tLoss: 6064.861816\n",
            "Train Epoch: 17 [43520/50000 (87%)]\tLoss: 5098.330078\n",
            "Train Epoch: 17 [46080/50000 (92%)]\tLoss: 5714.366699\n",
            "Train Epoch: 17 [48640/50000 (97%)]\tLoss: 5539.741211\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 4771.103027\n",
            "Train Epoch: 18 [2560/50000 (5%)]\tLoss: 5082.571777\n",
            "Train Epoch: 18 [5120/50000 (10%)]\tLoss: 5624.408203\n",
            "Train Epoch: 18 [7680/50000 (15%)]\tLoss: 5629.065918\n",
            "Train Epoch: 18 [10240/50000 (20%)]\tLoss: 6151.343750\n",
            "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 6070.090820\n",
            "Train Epoch: 18 [15360/50000 (31%)]\tLoss: 5273.353516\n",
            "Train Epoch: 18 [17920/50000 (36%)]\tLoss: 5290.744629\n",
            "Train Epoch: 18 [20480/50000 (41%)]\tLoss: 5503.801758\n",
            "Train Epoch: 18 [23040/50000 (46%)]\tLoss: 5333.442383\n",
            "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 5335.918457\n",
            "Train Epoch: 18 [28160/50000 (56%)]\tLoss: 5500.352539\n",
            "Train Epoch: 18 [30720/50000 (61%)]\tLoss: 5858.249023\n",
            "Train Epoch: 18 [33280/50000 (66%)]\tLoss: 5443.038086\n",
            "Train Epoch: 18 [35840/50000 (71%)]\tLoss: 5637.827637\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 5028.849121\n",
            "Train Epoch: 18 [40960/50000 (82%)]\tLoss: 5572.409668\n",
            "Train Epoch: 18 [43520/50000 (87%)]\tLoss: 5866.687012\n",
            "Train Epoch: 18 [46080/50000 (92%)]\tLoss: 5439.304688\n",
            "Train Epoch: 18 [48640/50000 (97%)]\tLoss: 5436.844727\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 5346.411621\n",
            "Train Epoch: 19 [2560/50000 (5%)]\tLoss: 5146.458984\n",
            "Train Epoch: 19 [5120/50000 (10%)]\tLoss: 5155.762695\n",
            "Train Epoch: 19 [7680/50000 (15%)]\tLoss: 5425.481445\n",
            "Train Epoch: 19 [10240/50000 (20%)]\tLoss: 5553.279297\n",
            "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 5690.015137\n",
            "Train Epoch: 19 [15360/50000 (31%)]\tLoss: 5525.169434\n",
            "Train Epoch: 19 [17920/50000 (36%)]\tLoss: 4832.160645\n",
            "Train Epoch: 19 [20480/50000 (41%)]\tLoss: 5132.014160\n",
            "Train Epoch: 19 [23040/50000 (46%)]\tLoss: 5139.804688\n",
            "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 4648.807617\n",
            "Train Epoch: 19 [28160/50000 (56%)]\tLoss: 5596.120117\n",
            "Train Epoch: 19 [30720/50000 (61%)]\tLoss: 5124.164062\n",
            "Train Epoch: 19 [33280/50000 (66%)]\tLoss: 5833.496582\n",
            "Train Epoch: 19 [35840/50000 (71%)]\tLoss: 5420.469238\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 4841.274902\n",
            "Train Epoch: 19 [40960/50000 (82%)]\tLoss: 5160.931641\n",
            "Train Epoch: 19 [43520/50000 (87%)]\tLoss: 5651.845703\n",
            "Train Epoch: 19 [46080/50000 (92%)]\tLoss: 5316.586426\n",
            "Train Epoch: 19 [48640/50000 (97%)]\tLoss: 5144.379883\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 5689.001953\n",
            "Train Epoch: 20 [2560/50000 (5%)]\tLoss: 5752.528320\n",
            "Train Epoch: 20 [5120/50000 (10%)]\tLoss: 5276.948242\n",
            "Train Epoch: 20 [7680/50000 (15%)]\tLoss: 5246.486328\n",
            "Train Epoch: 20 [10240/50000 (20%)]\tLoss: 5892.784180\n",
            "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 4791.328125\n",
            "Train Epoch: 20 [15360/50000 (31%)]\tLoss: 5507.516113\n",
            "Train Epoch: 20 [17920/50000 (36%)]\tLoss: 5019.142578\n",
            "Train Epoch: 20 [20480/50000 (41%)]\tLoss: 5487.450195\n",
            "Train Epoch: 20 [23040/50000 (46%)]\tLoss: 5430.505371\n",
            "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 5163.444336\n",
            "Train Epoch: 20 [28160/50000 (56%)]\tLoss: 5244.894531\n",
            "Train Epoch: 20 [30720/50000 (61%)]\tLoss: 5515.907715\n",
            "Train Epoch: 20 [33280/50000 (66%)]\tLoss: 4800.499512\n",
            "Train Epoch: 20 [35840/50000 (71%)]\tLoss: 4807.254883\n",
            "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 5036.939941\n",
            "Train Epoch: 20 [40960/50000 (82%)]\tLoss: 4664.166992\n",
            "Train Epoch: 20 [43520/50000 (87%)]\tLoss: 4956.075195\n",
            "Train Epoch: 20 [46080/50000 (92%)]\tLoss: 5444.155273\n",
            "Train Epoch: 20 [48640/50000 (97%)]\tLoss: 5693.766113\n",
            "Epoch: 20\n",
            "Training loss: 5289.509568817762\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 5877.040039\n",
            "Train Epoch: 21 [2560/50000 (5%)]\tLoss: 5820.916504\n",
            "Train Epoch: 21 [5120/50000 (10%)]\tLoss: 5651.469727\n",
            "Train Epoch: 21 [7680/50000 (15%)]\tLoss: 5394.544434\n",
            "Train Epoch: 21 [10240/50000 (20%)]\tLoss: 5486.529785\n",
            "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 5107.724609\n",
            "Train Epoch: 21 [15360/50000 (31%)]\tLoss: 6325.239258\n",
            "Train Epoch: 21 [17920/50000 (36%)]\tLoss: 5688.465820\n",
            "Train Epoch: 21 [20480/50000 (41%)]\tLoss: 4734.000000\n",
            "Train Epoch: 21 [23040/50000 (46%)]\tLoss: 5287.626465\n",
            "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 5098.840820\n",
            "Train Epoch: 21 [28160/50000 (56%)]\tLoss: 5873.301758\n",
            "Train Epoch: 21 [30720/50000 (61%)]\tLoss: 5445.260742\n",
            "Train Epoch: 21 [33280/50000 (66%)]\tLoss: 5368.708496\n",
            "Train Epoch: 21 [35840/50000 (71%)]\tLoss: 5328.357910\n",
            "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 5099.922852\n",
            "Train Epoch: 21 [40960/50000 (82%)]\tLoss: 5093.179199\n",
            "Train Epoch: 21 [43520/50000 (87%)]\tLoss: 4812.799805\n",
            "Train Epoch: 21 [46080/50000 (92%)]\tLoss: 5509.750488\n",
            "Train Epoch: 21 [48640/50000 (97%)]\tLoss: 4829.204102\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 5698.139648\n",
            "Train Epoch: 22 [2560/50000 (5%)]\tLoss: 5286.040527\n",
            "Train Epoch: 22 [5120/50000 (10%)]\tLoss: 5268.463867\n",
            "Train Epoch: 22 [7680/50000 (15%)]\tLoss: 5219.276367\n",
            "Train Epoch: 22 [10240/50000 (20%)]\tLoss: 4987.075684\n",
            "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 5188.167480\n",
            "Train Epoch: 22 [15360/50000 (31%)]\tLoss: 5199.719727\n",
            "Train Epoch: 22 [17920/50000 (36%)]\tLoss: 5138.862793\n",
            "Train Epoch: 22 [20480/50000 (41%)]\tLoss: 5191.354492\n",
            "Train Epoch: 22 [23040/50000 (46%)]\tLoss: 5191.628418\n",
            "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 5366.750977\n",
            "Train Epoch: 22 [28160/50000 (56%)]\tLoss: 5417.683594\n",
            "Train Epoch: 22 [30720/50000 (61%)]\tLoss: 5380.069824\n",
            "Train Epoch: 22 [33280/50000 (66%)]\tLoss: 5160.568359\n",
            "Train Epoch: 22 [35840/50000 (71%)]\tLoss: 4398.296875\n",
            "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 4407.903320\n",
            "Train Epoch: 22 [40960/50000 (82%)]\tLoss: 5913.112305\n",
            "Train Epoch: 22 [43520/50000 (87%)]\tLoss: 5411.399414\n",
            "Train Epoch: 22 [46080/50000 (92%)]\tLoss: 5217.413574\n",
            "Train Epoch: 22 [48640/50000 (97%)]\tLoss: 5478.370117\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 5355.781738\n",
            "Train Epoch: 23 [2560/50000 (5%)]\tLoss: 5251.487305\n",
            "Train Epoch: 23 [5120/50000 (10%)]\tLoss: 5574.456055\n",
            "Train Epoch: 23 [7680/50000 (15%)]\tLoss: 4900.715820\n",
            "Train Epoch: 23 [10240/50000 (20%)]\tLoss: 5607.236328\n",
            "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 5106.372070\n",
            "Train Epoch: 23 [15360/50000 (31%)]\tLoss: 4962.255371\n",
            "Train Epoch: 23 [17920/50000 (36%)]\tLoss: 5236.795410\n",
            "Train Epoch: 23 [20480/50000 (41%)]\tLoss: 5200.213867\n",
            "Train Epoch: 23 [23040/50000 (46%)]\tLoss: 4820.572266\n",
            "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 5586.926758\n",
            "Train Epoch: 23 [28160/50000 (56%)]\tLoss: 5051.245117\n",
            "Train Epoch: 23 [30720/50000 (61%)]\tLoss: 5000.612793\n",
            "Train Epoch: 23 [33280/50000 (66%)]\tLoss: 5042.588867\n",
            "Train Epoch: 23 [35840/50000 (71%)]\tLoss: 5922.415039\n",
            "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 5138.760742\n",
            "Train Epoch: 23 [40960/50000 (82%)]\tLoss: 4925.810547\n",
            "Train Epoch: 23 [43520/50000 (87%)]\tLoss: 4863.882324\n",
            "Train Epoch: 23 [46080/50000 (92%)]\tLoss: 4701.974609\n",
            "Train Epoch: 23 [48640/50000 (97%)]\tLoss: 4834.199707\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 4917.849609\n",
            "Train Epoch: 24 [2560/50000 (5%)]\tLoss: 5014.580566\n",
            "Train Epoch: 24 [5120/50000 (10%)]\tLoss: 5234.333008\n",
            "Train Epoch: 24 [7680/50000 (15%)]\tLoss: 5100.037109\n",
            "Train Epoch: 24 [10240/50000 (20%)]\tLoss: 4444.804199\n",
            "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 4761.165527\n",
            "Train Epoch: 24 [15360/50000 (31%)]\tLoss: 5302.654785\n",
            "Train Epoch: 24 [17920/50000 (36%)]\tLoss: 5180.861816\n",
            "Train Epoch: 24 [20480/50000 (41%)]\tLoss: 4561.810547\n",
            "Train Epoch: 24 [23040/50000 (46%)]\tLoss: 4918.218750\n",
            "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 5120.304688\n",
            "Train Epoch: 24 [28160/50000 (56%)]\tLoss: 5067.670898\n",
            "Train Epoch: 24 [30720/50000 (61%)]\tLoss: 5085.938477\n",
            "Train Epoch: 24 [33280/50000 (66%)]\tLoss: 4881.381836\n",
            "Train Epoch: 24 [35840/50000 (71%)]\tLoss: 4681.325684\n",
            "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 5392.816406\n",
            "Train Epoch: 24 [40960/50000 (82%)]\tLoss: 5220.000977\n",
            "Train Epoch: 24 [43520/50000 (87%)]\tLoss: 4724.196777\n",
            "Train Epoch: 24 [46080/50000 (92%)]\tLoss: 4662.815918\n",
            "Train Epoch: 24 [48640/50000 (97%)]\tLoss: 4899.951660\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 4541.778320\n",
            "Train Epoch: 25 [2560/50000 (5%)]\tLoss: 4884.029785\n",
            "Train Epoch: 25 [5120/50000 (10%)]\tLoss: 4972.109375\n",
            "Train Epoch: 25 [7680/50000 (15%)]\tLoss: 4744.042969\n",
            "Train Epoch: 25 [10240/50000 (20%)]\tLoss: 5399.076172\n",
            "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 4224.807129\n",
            "Train Epoch: 25 [15360/50000 (31%)]\tLoss: 4950.231934\n",
            "Train Epoch: 25 [17920/50000 (36%)]\tLoss: 4520.622070\n",
            "Train Epoch: 25 [20480/50000 (41%)]\tLoss: 4840.748047\n",
            "Train Epoch: 25 [23040/50000 (46%)]\tLoss: 5294.993652\n",
            "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 4979.109375\n",
            "Train Epoch: 25 [28160/50000 (56%)]\tLoss: 5618.214844\n",
            "Train Epoch: 25 [30720/50000 (61%)]\tLoss: 4660.625488\n",
            "Train Epoch: 25 [33280/50000 (66%)]\tLoss: 4686.735840\n",
            "Train Epoch: 25 [35840/50000 (71%)]\tLoss: 4725.823242\n",
            "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 4621.466309\n",
            "Train Epoch: 25 [40960/50000 (82%)]\tLoss: 4344.750488\n",
            "Train Epoch: 25 [43520/50000 (87%)]\tLoss: 4609.532227\n",
            "Train Epoch: 25 [46080/50000 (92%)]\tLoss: 5072.465332\n",
            "Train Epoch: 25 [48640/50000 (97%)]\tLoss: 5379.477539\n",
            "Epoch: 25\n",
            "Training loss: 4760.781932597258\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 4782.183105\n",
            "Train Epoch: 26 [2560/50000 (5%)]\tLoss: 4931.219238\n",
            "Train Epoch: 26 [5120/50000 (10%)]\tLoss: 4878.031738\n",
            "Train Epoch: 26 [7680/50000 (15%)]\tLoss: 4462.405762\n",
            "Train Epoch: 26 [10240/50000 (20%)]\tLoss: 4305.500000\n",
            "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 4723.389648\n",
            "Train Epoch: 26 [15360/50000 (31%)]\tLoss: 5001.127930\n",
            "Train Epoch: 26 [17920/50000 (36%)]\tLoss: 4293.723633\n",
            "Train Epoch: 26 [20480/50000 (41%)]\tLoss: 4378.680664\n",
            "Train Epoch: 26 [23040/50000 (46%)]\tLoss: 4468.140625\n",
            "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 4001.563232\n",
            "Train Epoch: 26 [28160/50000 (56%)]\tLoss: 4400.222656\n",
            "Train Epoch: 26 [30720/50000 (61%)]\tLoss: 4829.190430\n",
            "Train Epoch: 26 [33280/50000 (66%)]\tLoss: 5008.504883\n",
            "Train Epoch: 26 [35840/50000 (71%)]\tLoss: 4739.530762\n",
            "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 4422.927246\n",
            "Train Epoch: 26 [40960/50000 (82%)]\tLoss: 5277.773926\n",
            "Train Epoch: 26 [43520/50000 (87%)]\tLoss: 4641.010254\n",
            "Train Epoch: 26 [46080/50000 (92%)]\tLoss: 4682.282227\n",
            "Train Epoch: 26 [48640/50000 (97%)]\tLoss: 4737.239258\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 4635.944824\n",
            "Train Epoch: 27 [2560/50000 (5%)]\tLoss: 4331.870605\n",
            "Train Epoch: 27 [5120/50000 (10%)]\tLoss: 4320.861816\n",
            "Train Epoch: 27 [7680/50000 (15%)]\tLoss: 4679.826172\n",
            "Train Epoch: 27 [10240/50000 (20%)]\tLoss: 4541.302246\n",
            "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 4579.464355\n",
            "Train Epoch: 27 [15360/50000 (31%)]\tLoss: 4240.619141\n",
            "Train Epoch: 27 [17920/50000 (36%)]\tLoss: 4212.409668\n",
            "Train Epoch: 27 [20480/50000 (41%)]\tLoss: 4804.294922\n",
            "Train Epoch: 27 [23040/50000 (46%)]\tLoss: 4287.618164\n",
            "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 4083.780518\n",
            "Train Epoch: 27 [28160/50000 (56%)]\tLoss: 4619.637695\n",
            "Train Epoch: 27 [30720/50000 (61%)]\tLoss: 4250.596191\n",
            "Train Epoch: 27 [33280/50000 (66%)]\tLoss: 4558.926758\n",
            "Train Epoch: 27 [35840/50000 (71%)]\tLoss: 3933.339844\n",
            "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 3993.825684\n",
            "Train Epoch: 27 [40960/50000 (82%)]\tLoss: 4494.385742\n",
            "Train Epoch: 27 [43520/50000 (87%)]\tLoss: 4233.536621\n",
            "Train Epoch: 27 [46080/50000 (92%)]\tLoss: 4237.798828\n",
            "Train Epoch: 27 [48640/50000 (97%)]\tLoss: 4228.412598\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 3929.928467\n",
            "Train Epoch: 28 [2560/50000 (5%)]\tLoss: 4478.715820\n",
            "Train Epoch: 28 [5120/50000 (10%)]\tLoss: 4318.685059\n",
            "Train Epoch: 28 [7680/50000 (15%)]\tLoss: 4030.076904\n",
            "Train Epoch: 28 [10240/50000 (20%)]\tLoss: 4476.136230\n",
            "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 3998.734863\n",
            "Train Epoch: 28 [15360/50000 (31%)]\tLoss: 4189.581055\n",
            "Train Epoch: 28 [17920/50000 (36%)]\tLoss: 4518.176758\n",
            "Train Epoch: 28 [20480/50000 (41%)]\tLoss: 4158.053711\n",
            "Train Epoch: 28 [23040/50000 (46%)]\tLoss: 4401.338379\n",
            "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 4204.635742\n",
            "Train Epoch: 28 [28160/50000 (56%)]\tLoss: 4515.773926\n",
            "Train Epoch: 28 [30720/50000 (61%)]\tLoss: 4004.205078\n",
            "Train Epoch: 28 [33280/50000 (66%)]\tLoss: 3632.463623\n",
            "Train Epoch: 28 [35840/50000 (71%)]\tLoss: 4407.892578\n",
            "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 3980.543457\n",
            "Train Epoch: 28 [40960/50000 (82%)]\tLoss: 4185.714844\n",
            "Train Epoch: 28 [43520/50000 (87%)]\tLoss: 3788.463623\n",
            "Train Epoch: 28 [46080/50000 (92%)]\tLoss: 4010.450684\n",
            "Train Epoch: 28 [48640/50000 (97%)]\tLoss: 3776.600830\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 3960.852295\n",
            "Train Epoch: 29 [2560/50000 (5%)]\tLoss: 4088.915283\n",
            "Train Epoch: 29 [5120/50000 (10%)]\tLoss: 4024.479004\n",
            "Train Epoch: 29 [7680/50000 (15%)]\tLoss: 4169.768066\n",
            "Train Epoch: 29 [10240/50000 (20%)]\tLoss: 3800.474121\n",
            "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 4330.920410\n",
            "Train Epoch: 29 [15360/50000 (31%)]\tLoss: 3626.762939\n",
            "Train Epoch: 29 [17920/50000 (36%)]\tLoss: 4041.363281\n",
            "Train Epoch: 29 [20480/50000 (41%)]\tLoss: 3877.923340\n",
            "Train Epoch: 29 [23040/50000 (46%)]\tLoss: 4182.968750\n",
            "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 3891.462402\n",
            "Train Epoch: 29 [28160/50000 (56%)]\tLoss: 3621.130127\n",
            "Train Epoch: 29 [30720/50000 (61%)]\tLoss: 4184.966797\n",
            "Train Epoch: 29 [33280/50000 (66%)]\tLoss: 4091.970703\n",
            "Train Epoch: 29 [35840/50000 (71%)]\tLoss: 4267.297852\n",
            "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 3853.176514\n",
            "Train Epoch: 29 [40960/50000 (82%)]\tLoss: 4384.394531\n",
            "Train Epoch: 29 [43520/50000 (87%)]\tLoss: 3558.885498\n",
            "Train Epoch: 29 [46080/50000 (92%)]\tLoss: 3855.460205\n",
            "Train Epoch: 29 [48640/50000 (97%)]\tLoss: 3900.931152\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLoss: 3598.424316\n",
            "Train Epoch: 30 [2560/50000 (5%)]\tLoss: 3488.414062\n",
            "Train Epoch: 30 [5120/50000 (10%)]\tLoss: 3723.917969\n",
            "Train Epoch: 30 [7680/50000 (15%)]\tLoss: 3698.545654\n",
            "Train Epoch: 30 [10240/50000 (20%)]\tLoss: 3435.620605\n",
            "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 3670.481934\n",
            "Train Epoch: 30 [15360/50000 (31%)]\tLoss: 4090.847168\n",
            "Train Epoch: 30 [17920/50000 (36%)]\tLoss: 4020.675781\n",
            "Train Epoch: 30 [20480/50000 (41%)]\tLoss: 3640.895020\n",
            "Train Epoch: 30 [23040/50000 (46%)]\tLoss: 3969.849121\n",
            "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 3808.520752\n",
            "Train Epoch: 30 [28160/50000 (56%)]\tLoss: 3650.445312\n",
            "Train Epoch: 30 [30720/50000 (61%)]\tLoss: 3611.203613\n",
            "Train Epoch: 30 [33280/50000 (66%)]\tLoss: 3530.474609\n",
            "Train Epoch: 30 [35840/50000 (71%)]\tLoss: 3667.436279\n",
            "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 3710.766846\n",
            "Train Epoch: 30 [40960/50000 (82%)]\tLoss: 3799.909424\n",
            "Train Epoch: 30 [43520/50000 (87%)]\tLoss: 3434.217285\n",
            "Train Epoch: 30 [46080/50000 (92%)]\tLoss: 4148.850586\n",
            "Train Epoch: 30 [48640/50000 (97%)]\tLoss: 3660.480957\n",
            "Epoch: 30\n",
            "Training loss: 3771.2791162607623\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLoss: 3943.759766\n",
            "Train Epoch: 31 [2560/50000 (5%)]\tLoss: 4112.176270\n",
            "Train Epoch: 31 [5120/50000 (10%)]\tLoss: 3364.387939\n",
            "Train Epoch: 31 [7680/50000 (15%)]\tLoss: 3446.908691\n",
            "Train Epoch: 31 [10240/50000 (20%)]\tLoss: 3628.562988\n",
            "Train Epoch: 31 [12800/50000 (26%)]\tLoss: 3352.980469\n",
            "Train Epoch: 31 [15360/50000 (31%)]\tLoss: 3519.507080\n",
            "Train Epoch: 31 [17920/50000 (36%)]\tLoss: 3552.892578\n",
            "Train Epoch: 31 [20480/50000 (41%)]\tLoss: 3631.121094\n",
            "Train Epoch: 31 [23040/50000 (46%)]\tLoss: 4038.942871\n",
            "Train Epoch: 31 [25600/50000 (51%)]\tLoss: 3533.307861\n",
            "Train Epoch: 31 [28160/50000 (56%)]\tLoss: 3541.462891\n",
            "Train Epoch: 31 [30720/50000 (61%)]\tLoss: 3558.762939\n",
            "Train Epoch: 31 [33280/50000 (66%)]\tLoss: 3288.070801\n",
            "Train Epoch: 31 [35840/50000 (71%)]\tLoss: 3909.285156\n",
            "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 3715.740234\n",
            "Train Epoch: 31 [40960/50000 (82%)]\tLoss: 3492.913574\n",
            "Train Epoch: 31 [43520/50000 (87%)]\tLoss: 3773.503418\n",
            "Train Epoch: 31 [46080/50000 (92%)]\tLoss: 3409.762939\n",
            "Train Epoch: 31 [48640/50000 (97%)]\tLoss: 3417.104004\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLoss: 3437.614502\n",
            "Train Epoch: 32 [2560/50000 (5%)]\tLoss: 3139.389160\n",
            "Train Epoch: 32 [5120/50000 (10%)]\tLoss: 3982.331543\n",
            "Train Epoch: 32 [7680/50000 (15%)]\tLoss: 3101.751221\n",
            "Train Epoch: 32 [10240/50000 (20%)]\tLoss: 3111.126709\n",
            "Train Epoch: 32 [12800/50000 (26%)]\tLoss: 3308.701660\n",
            "Train Epoch: 32 [15360/50000 (31%)]\tLoss: 3887.909424\n",
            "Train Epoch: 32 [17920/50000 (36%)]\tLoss: 3204.253174\n",
            "Train Epoch: 32 [20480/50000 (41%)]\tLoss: 3512.111328\n",
            "Train Epoch: 32 [23040/50000 (46%)]\tLoss: 3263.091309\n",
            "Train Epoch: 32 [25600/50000 (51%)]\tLoss: 3065.912109\n",
            "Train Epoch: 32 [28160/50000 (56%)]\tLoss: 2928.376953\n",
            "Train Epoch: 32 [30720/50000 (61%)]\tLoss: 2953.100098\n",
            "Train Epoch: 32 [33280/50000 (66%)]\tLoss: 3424.680420\n",
            "Train Epoch: 32 [35840/50000 (71%)]\tLoss: 3082.457764\n",
            "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 3330.365234\n",
            "Train Epoch: 32 [40960/50000 (82%)]\tLoss: 2918.371582\n",
            "Train Epoch: 32 [43520/50000 (87%)]\tLoss: 3035.263428\n",
            "Train Epoch: 32 [46080/50000 (92%)]\tLoss: 3215.989746\n",
            "Train Epoch: 32 [48640/50000 (97%)]\tLoss: 3385.144531\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLoss: 2808.590576\n",
            "Train Epoch: 33 [2560/50000 (5%)]\tLoss: 3208.757324\n",
            "Train Epoch: 33 [5120/50000 (10%)]\tLoss: 3077.591064\n",
            "Train Epoch: 33 [7680/50000 (15%)]\tLoss: 3105.429688\n",
            "Train Epoch: 33 [10240/50000 (20%)]\tLoss: 2845.823730\n",
            "Train Epoch: 33 [12800/50000 (26%)]\tLoss: 3113.248779\n",
            "Train Epoch: 33 [15360/50000 (31%)]\tLoss: 3362.520264\n",
            "Train Epoch: 33 [17920/50000 (36%)]\tLoss: 2793.892090\n",
            "Train Epoch: 33 [20480/50000 (41%)]\tLoss: 2847.852051\n",
            "Train Epoch: 33 [23040/50000 (46%)]\tLoss: 3396.832031\n",
            "Train Epoch: 33 [25600/50000 (51%)]\tLoss: 2925.578613\n",
            "Train Epoch: 33 [28160/50000 (56%)]\tLoss: 3288.570801\n",
            "Train Epoch: 33 [30720/50000 (61%)]\tLoss: 2870.917480\n",
            "Train Epoch: 33 [33280/50000 (66%)]\tLoss: 2970.593994\n",
            "Train Epoch: 33 [35840/50000 (71%)]\tLoss: 3326.117920\n",
            "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 3430.489502\n",
            "Train Epoch: 33 [40960/50000 (82%)]\tLoss: 3093.053223\n",
            "Train Epoch: 33 [43520/50000 (87%)]\tLoss: 3066.837646\n",
            "Train Epoch: 33 [46080/50000 (92%)]\tLoss: 3013.442871\n",
            "Train Epoch: 33 [48640/50000 (97%)]\tLoss: 2762.668213\n",
            "Train Epoch: 34 [0/50000 (0%)]\tLoss: 2886.559570\n",
            "Train Epoch: 34 [2560/50000 (5%)]\tLoss: 2550.151855\n",
            "Train Epoch: 34 [5120/50000 (10%)]\tLoss: 2792.658936\n",
            "Train Epoch: 34 [7680/50000 (15%)]\tLoss: 2865.932861\n",
            "Train Epoch: 34 [10240/50000 (20%)]\tLoss: 2759.184570\n",
            "Train Epoch: 34 [12800/50000 (26%)]\tLoss: 2606.577637\n",
            "Train Epoch: 34 [15360/50000 (31%)]\tLoss: 2670.537598\n",
            "Train Epoch: 34 [17920/50000 (36%)]\tLoss: 2784.990723\n",
            "Train Epoch: 34 [20480/50000 (41%)]\tLoss: 2847.723877\n",
            "Train Epoch: 34 [23040/50000 (46%)]\tLoss: 2741.393066\n",
            "Train Epoch: 34 [25600/50000 (51%)]\tLoss: 2778.616211\n",
            "Train Epoch: 34 [28160/50000 (56%)]\tLoss: 2690.765625\n",
            "Train Epoch: 34 [30720/50000 (61%)]\tLoss: 2705.590332\n",
            "Train Epoch: 34 [33280/50000 (66%)]\tLoss: 2777.210693\n",
            "Train Epoch: 34 [35840/50000 (71%)]\tLoss: 2533.103027\n",
            "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 2554.329590\n",
            "Train Epoch: 34 [40960/50000 (82%)]\tLoss: 2882.436768\n",
            "Train Epoch: 34 [43520/50000 (87%)]\tLoss: 2633.114258\n",
            "Train Epoch: 34 [46080/50000 (92%)]\tLoss: 2575.434326\n",
            "Train Epoch: 34 [48640/50000 (97%)]\tLoss: 2782.821289\n",
            "Train Epoch: 35 [0/50000 (0%)]\tLoss: 2699.995361\n",
            "Train Epoch: 35 [2560/50000 (5%)]\tLoss: 2705.650146\n",
            "Train Epoch: 35 [5120/50000 (10%)]\tLoss: 2653.230957\n",
            "Train Epoch: 35 [7680/50000 (15%)]\tLoss: 2297.892334\n",
            "Train Epoch: 35 [10240/50000 (20%)]\tLoss: 2485.230225\n",
            "Train Epoch: 35 [12800/50000 (26%)]\tLoss: 2818.873047\n",
            "Train Epoch: 35 [15360/50000 (31%)]\tLoss: 2208.382080\n",
            "Train Epoch: 35 [17920/50000 (36%)]\tLoss: 2790.035156\n",
            "Train Epoch: 35 [20480/50000 (41%)]\tLoss: 2699.357666\n",
            "Train Epoch: 35 [23040/50000 (46%)]\tLoss: 2406.469238\n",
            "Train Epoch: 35 [25600/50000 (51%)]\tLoss: 2530.994873\n",
            "Train Epoch: 35 [28160/50000 (56%)]\tLoss: 2336.613525\n",
            "Train Epoch: 35 [30720/50000 (61%)]\tLoss: 2593.985840\n",
            "Train Epoch: 35 [33280/50000 (66%)]\tLoss: 2299.033691\n",
            "Train Epoch: 35 [35840/50000 (71%)]\tLoss: 2442.293701\n",
            "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 2168.322998\n",
            "Train Epoch: 35 [40960/50000 (82%)]\tLoss: 2788.219971\n",
            "Train Epoch: 35 [43520/50000 (87%)]\tLoss: 2347.869873\n",
            "Train Epoch: 35 [46080/50000 (92%)]\tLoss: 2280.940674\n",
            "Train Epoch: 35 [48640/50000 (97%)]\tLoss: 2420.452393\n",
            "Epoch: 35\n",
            "Training loss: 2525.832138372927\n",
            "Train Epoch: 36 [0/50000 (0%)]\tLoss: 2297.529541\n",
            "Train Epoch: 36 [2560/50000 (5%)]\tLoss: 2441.704102\n",
            "Train Epoch: 36 [5120/50000 (10%)]\tLoss: 2153.393066\n",
            "Train Epoch: 36 [7680/50000 (15%)]\tLoss: 2699.673340\n",
            "Train Epoch: 36 [10240/50000 (20%)]\tLoss: 2140.752441\n",
            "Train Epoch: 36 [12800/50000 (26%)]\tLoss: 2075.039307\n",
            "Train Epoch: 36 [15360/50000 (31%)]\tLoss: 2498.111328\n",
            "Train Epoch: 36 [17920/50000 (36%)]\tLoss: 2461.888916\n",
            "Train Epoch: 36 [20480/50000 (41%)]\tLoss: 2302.042969\n",
            "Train Epoch: 36 [23040/50000 (46%)]\tLoss: 2244.670654\n",
            "Train Epoch: 36 [25600/50000 (51%)]\tLoss: 2262.212891\n",
            "Train Epoch: 36 [28160/50000 (56%)]\tLoss: 2282.786133\n",
            "Train Epoch: 36 [30720/50000 (61%)]\tLoss: 2133.613770\n",
            "Train Epoch: 36 [33280/50000 (66%)]\tLoss: 2259.934082\n",
            "Train Epoch: 36 [35840/50000 (71%)]\tLoss: 2177.505371\n",
            "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 1951.744873\n",
            "Train Epoch: 36 [40960/50000 (82%)]\tLoss: 2397.932373\n",
            "Train Epoch: 36 [43520/50000 (87%)]\tLoss: 2139.414062\n",
            "Train Epoch: 36 [46080/50000 (92%)]\tLoss: 2226.718750\n",
            "Train Epoch: 36 [48640/50000 (97%)]\tLoss: 2188.766113\n",
            "Train Epoch: 37 [0/50000 (0%)]\tLoss: 2183.733887\n",
            "Train Epoch: 37 [2560/50000 (5%)]\tLoss: 2267.787598\n",
            "Train Epoch: 37 [5120/50000 (10%)]\tLoss: 1898.327515\n",
            "Train Epoch: 37 [7680/50000 (15%)]\tLoss: 1966.572876\n",
            "Train Epoch: 37 [10240/50000 (20%)]\tLoss: 2197.703857\n",
            "Train Epoch: 37 [12800/50000 (26%)]\tLoss: 1815.942017\n",
            "Train Epoch: 37 [15360/50000 (31%)]\tLoss: 1984.079468\n",
            "Train Epoch: 37 [17920/50000 (36%)]\tLoss: 1961.855469\n",
            "Train Epoch: 37 [20480/50000 (41%)]\tLoss: 1713.796875\n",
            "Train Epoch: 37 [23040/50000 (46%)]\tLoss: 2198.641113\n",
            "Train Epoch: 37 [25600/50000 (51%)]\tLoss: 1766.181274\n",
            "Train Epoch: 37 [28160/50000 (56%)]\tLoss: 1942.682007\n",
            "Train Epoch: 37 [30720/50000 (61%)]\tLoss: 1913.221313\n",
            "Train Epoch: 37 [33280/50000 (66%)]\tLoss: 2046.777100\n",
            "Train Epoch: 37 [35840/50000 (71%)]\tLoss: 2131.132324\n",
            "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 1664.052002\n",
            "Train Epoch: 37 [40960/50000 (82%)]\tLoss: 1933.488892\n",
            "Train Epoch: 37 [43520/50000 (87%)]\tLoss: 1822.306030\n",
            "Train Epoch: 37 [46080/50000 (92%)]\tLoss: 1836.378906\n",
            "Train Epoch: 37 [48640/50000 (97%)]\tLoss: 2288.355957\n",
            "Train Epoch: 38 [0/50000 (0%)]\tLoss: 2672.994141\n",
            "Train Epoch: 38 [2560/50000 (5%)]\tLoss: 2107.002686\n",
            "Train Epoch: 38 [5120/50000 (10%)]\tLoss: 1930.111084\n",
            "Train Epoch: 38 [7680/50000 (15%)]\tLoss: 1867.218750\n",
            "Train Epoch: 38 [10240/50000 (20%)]\tLoss: 1961.187500\n",
            "Train Epoch: 38 [12800/50000 (26%)]\tLoss: 1949.185791\n",
            "Train Epoch: 38 [15360/50000 (31%)]\tLoss: 1777.665771\n",
            "Train Epoch: 38 [17920/50000 (36%)]\tLoss: 1915.959351\n",
            "Train Epoch: 38 [20480/50000 (41%)]\tLoss: 1930.553345\n",
            "Train Epoch: 38 [23040/50000 (46%)]\tLoss: 1677.683838\n",
            "Train Epoch: 38 [25600/50000 (51%)]\tLoss: 1822.098267\n",
            "Train Epoch: 38 [28160/50000 (56%)]\tLoss: 1721.096069\n",
            "Train Epoch: 38 [30720/50000 (61%)]\tLoss: 1793.200928\n",
            "Train Epoch: 38 [33280/50000 (66%)]\tLoss: 1802.608398\n",
            "Train Epoch: 38 [35840/50000 (71%)]\tLoss: 1840.944702\n",
            "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 1656.718140\n",
            "Train Epoch: 38 [40960/50000 (82%)]\tLoss: 1633.379272\n",
            "Train Epoch: 38 [43520/50000 (87%)]\tLoss: 1664.245728\n",
            "Train Epoch: 38 [46080/50000 (92%)]\tLoss: 1633.884033\n",
            "Train Epoch: 38 [48640/50000 (97%)]\tLoss: 1568.141357\n",
            "Train Epoch: 39 [0/50000 (0%)]\tLoss: 1834.784180\n",
            "Train Epoch: 39 [2560/50000 (5%)]\tLoss: 1675.242432\n",
            "Train Epoch: 39 [5120/50000 (10%)]\tLoss: 1710.605469\n",
            "Train Epoch: 39 [7680/50000 (15%)]\tLoss: 1757.970093\n",
            "Train Epoch: 39 [10240/50000 (20%)]\tLoss: 1519.386230\n",
            "Train Epoch: 39 [12800/50000 (26%)]\tLoss: 1649.145752\n",
            "Train Epoch: 39 [15360/50000 (31%)]\tLoss: 1508.346191\n",
            "Train Epoch: 39 [17920/50000 (36%)]\tLoss: 1545.843750\n",
            "Train Epoch: 39 [20480/50000 (41%)]\tLoss: 1625.280029\n",
            "Train Epoch: 39 [23040/50000 (46%)]\tLoss: 1535.313354\n",
            "Train Epoch: 39 [25600/50000 (51%)]\tLoss: 1569.512573\n",
            "Train Epoch: 39 [28160/50000 (56%)]\tLoss: 1577.036133\n",
            "Train Epoch: 39 [30720/50000 (61%)]\tLoss: 1596.585815\n",
            "Train Epoch: 39 [33280/50000 (66%)]\tLoss: 1611.318481\n",
            "Train Epoch: 39 [35840/50000 (71%)]\tLoss: 1537.042114\n",
            "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 1674.086304\n",
            "Train Epoch: 39 [40960/50000 (82%)]\tLoss: 1972.627930\n",
            "Train Epoch: 39 [43520/50000 (87%)]\tLoss: 1344.424072\n",
            "Train Epoch: 39 [46080/50000 (92%)]\tLoss: 1499.936279\n",
            "Train Epoch: 39 [48640/50000 (97%)]\tLoss: 1384.930664\n",
            "Train Epoch: 40 [0/50000 (0%)]\tLoss: 1984.884033\n",
            "Train Epoch: 40 [2560/50000 (5%)]\tLoss: 1332.376343\n",
            "Train Epoch: 40 [5120/50000 (10%)]\tLoss: 1477.677490\n",
            "Train Epoch: 40 [7680/50000 (15%)]\tLoss: 1445.452148\n",
            "Train Epoch: 40 [10240/50000 (20%)]\tLoss: 1330.146851\n",
            "Train Epoch: 40 [12800/50000 (26%)]\tLoss: 1381.060669\n",
            "Train Epoch: 40 [15360/50000 (31%)]\tLoss: 1443.657104\n",
            "Train Epoch: 40 [17920/50000 (36%)]\tLoss: 1372.783447\n",
            "Train Epoch: 40 [20480/50000 (41%)]\tLoss: 1378.963623\n",
            "Train Epoch: 40 [23040/50000 (46%)]\tLoss: 1351.863525\n",
            "Train Epoch: 40 [25600/50000 (51%)]\tLoss: 1312.522827\n",
            "Train Epoch: 40 [28160/50000 (56%)]\tLoss: 1313.138062\n",
            "Train Epoch: 40 [30720/50000 (61%)]\tLoss: 1318.940918\n",
            "Train Epoch: 40 [33280/50000 (66%)]\tLoss: 1408.735107\n",
            "Train Epoch: 40 [35840/50000 (71%)]\tLoss: 1469.258423\n",
            "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 1413.575928\n",
            "Train Epoch: 40 [40960/50000 (82%)]\tLoss: 1283.022583\n",
            "Train Epoch: 40 [43520/50000 (87%)]\tLoss: 1289.269287\n",
            "Train Epoch: 40 [46080/50000 (92%)]\tLoss: 1343.653687\n",
            "Train Epoch: 40 [48640/50000 (97%)]\tLoss: 1268.727905\n",
            "Epoch: 40\n",
            "Training loss: 1388.924052958586\n",
            "Train Epoch: 41 [0/50000 (0%)]\tLoss: 1340.463623\n",
            "Train Epoch: 41 [2560/50000 (5%)]\tLoss: 1181.863159\n",
            "Train Epoch: 41 [5120/50000 (10%)]\tLoss: 1282.266602\n",
            "Train Epoch: 41 [7680/50000 (15%)]\tLoss: 1335.077393\n",
            "Train Epoch: 41 [10240/50000 (20%)]\tLoss: 1104.296875\n",
            "Train Epoch: 41 [12800/50000 (26%)]\tLoss: 1224.686035\n",
            "Train Epoch: 41 [15360/50000 (31%)]\tLoss: 1166.779053\n",
            "Train Epoch: 41 [17920/50000 (36%)]\tLoss: 1754.201538\n",
            "Train Epoch: 41 [20480/50000 (41%)]\tLoss: 1351.516602\n",
            "Train Epoch: 41 [23040/50000 (46%)]\tLoss: 1277.784790\n",
            "Train Epoch: 41 [25600/50000 (51%)]\tLoss: 1139.564575\n",
            "Train Epoch: 41 [28160/50000 (56%)]\tLoss: 1202.871216\n",
            "Train Epoch: 41 [30720/50000 (61%)]\tLoss: 1065.939453\n",
            "Train Epoch: 41 [33280/50000 (66%)]\tLoss: 1241.891724\n",
            "Train Epoch: 41 [35840/50000 (71%)]\tLoss: 1270.610474\n",
            "Train Epoch: 41 [38400/50000 (77%)]\tLoss: 1318.633423\n",
            "Train Epoch: 41 [40960/50000 (82%)]\tLoss: 1047.092407\n",
            "Train Epoch: 41 [43520/50000 (87%)]\tLoss: 1268.336670\n",
            "Train Epoch: 41 [46080/50000 (92%)]\tLoss: 1274.256226\n",
            "Train Epoch: 41 [48640/50000 (97%)]\tLoss: 1125.165405\n",
            "Train Epoch: 42 [0/50000 (0%)]\tLoss: 1118.485107\n",
            "Train Epoch: 42 [2560/50000 (5%)]\tLoss: 1043.361938\n",
            "Train Epoch: 42 [5120/50000 (10%)]\tLoss: 1008.211304\n",
            "Train Epoch: 42 [7680/50000 (15%)]\tLoss: 1138.883911\n",
            "Train Epoch: 42 [10240/50000 (20%)]\tLoss: 1033.085205\n",
            "Train Epoch: 42 [12800/50000 (26%)]\tLoss: 1904.876587\n",
            "Train Epoch: 42 [15360/50000 (31%)]\tLoss: 1014.292969\n",
            "Train Epoch: 42 [17920/50000 (36%)]\tLoss: 1027.124756\n",
            "Train Epoch: 42 [20480/50000 (41%)]\tLoss: 1074.280273\n",
            "Train Epoch: 42 [23040/50000 (46%)]\tLoss: 1096.682861\n",
            "Train Epoch: 42 [25600/50000 (51%)]\tLoss: 1141.267822\n",
            "Train Epoch: 42 [28160/50000 (56%)]\tLoss: 1271.055176\n",
            "Train Epoch: 42 [30720/50000 (61%)]\tLoss: 1009.203430\n",
            "Train Epoch: 42 [33280/50000 (66%)]\tLoss: 1063.486084\n",
            "Train Epoch: 42 [35840/50000 (71%)]\tLoss: 1108.545532\n",
            "Train Epoch: 42 [38400/50000 (77%)]\tLoss: 988.513855\n",
            "Train Epoch: 42 [40960/50000 (82%)]\tLoss: 950.586121\n",
            "Train Epoch: 42 [43520/50000 (87%)]\tLoss: 1054.551514\n",
            "Train Epoch: 42 [46080/50000 (92%)]\tLoss: 1055.826172\n",
            "Train Epoch: 42 [48640/50000 (97%)]\tLoss: 1007.127930\n",
            "Train Epoch: 43 [0/50000 (0%)]\tLoss: 991.552856\n",
            "Train Epoch: 43 [2560/50000 (5%)]\tLoss: 961.108582\n",
            "Train Epoch: 43 [5120/50000 (10%)]\tLoss: 948.822754\n",
            "Train Epoch: 43 [7680/50000 (15%)]\tLoss: 938.277710\n",
            "Train Epoch: 43 [10240/50000 (20%)]\tLoss: 875.359070\n",
            "Train Epoch: 43 [12800/50000 (26%)]\tLoss: 908.084839\n",
            "Train Epoch: 43 [15360/50000 (31%)]\tLoss: 831.795593\n",
            "Train Epoch: 43 [17920/50000 (36%)]\tLoss: 960.342407\n",
            "Train Epoch: 43 [20480/50000 (41%)]\tLoss: 1041.062012\n",
            "Train Epoch: 43 [23040/50000 (46%)]\tLoss: 933.400574\n",
            "Train Epoch: 43 [25600/50000 (51%)]\tLoss: 851.047241\n",
            "Train Epoch: 43 [28160/50000 (56%)]\tLoss: 858.852356\n",
            "Train Epoch: 43 [30720/50000 (61%)]\tLoss: 899.127563\n",
            "Train Epoch: 43 [33280/50000 (66%)]\tLoss: 854.485474\n",
            "Train Epoch: 43 [35840/50000 (71%)]\tLoss: 877.426086\n",
            "Train Epoch: 43 [38400/50000 (77%)]\tLoss: 1045.146606\n",
            "Train Epoch: 43 [40960/50000 (82%)]\tLoss: 946.817688\n",
            "Train Epoch: 43 [43520/50000 (87%)]\tLoss: 871.389832\n",
            "Train Epoch: 43 [46080/50000 (92%)]\tLoss: 785.966797\n",
            "Train Epoch: 43 [48640/50000 (97%)]\tLoss: 859.904175\n",
            "Train Epoch: 44 [0/50000 (0%)]\tLoss: 881.731567\n",
            "Train Epoch: 44 [2560/50000 (5%)]\tLoss: 851.286011\n",
            "Train Epoch: 44 [5120/50000 (10%)]\tLoss: 866.969910\n",
            "Train Epoch: 44 [7680/50000 (15%)]\tLoss: 782.210388\n",
            "Train Epoch: 44 [10240/50000 (20%)]\tLoss: 862.431519\n",
            "Train Epoch: 44 [12800/50000 (26%)]\tLoss: 1247.872925\n",
            "Train Epoch: 44 [15360/50000 (31%)]\tLoss: 816.252991\n",
            "Train Epoch: 44 [17920/50000 (36%)]\tLoss: 883.082397\n",
            "Train Epoch: 44 [20480/50000 (41%)]\tLoss: 780.255493\n",
            "Train Epoch: 44 [23040/50000 (46%)]\tLoss: 739.082642\n",
            "Train Epoch: 44 [25600/50000 (51%)]\tLoss: 803.681641\n",
            "Train Epoch: 44 [28160/50000 (56%)]\tLoss: 855.334656\n",
            "Train Epoch: 44 [30720/50000 (61%)]\tLoss: 824.829224\n",
            "Train Epoch: 44 [33280/50000 (66%)]\tLoss: 765.399048\n",
            "Train Epoch: 44 [35840/50000 (71%)]\tLoss: 854.093567\n",
            "Train Epoch: 44 [38400/50000 (77%)]\tLoss: 761.905579\n",
            "Train Epoch: 44 [40960/50000 (82%)]\tLoss: 717.405823\n",
            "Train Epoch: 44 [43520/50000 (87%)]\tLoss: 787.255249\n",
            "Train Epoch: 44 [46080/50000 (92%)]\tLoss: 787.981201\n",
            "Train Epoch: 44 [48640/50000 (97%)]\tLoss: 785.239075\n",
            "Train Epoch: 45 [0/50000 (0%)]\tLoss: 863.143738\n",
            "Train Epoch: 45 [2560/50000 (5%)]\tLoss: 741.780640\n",
            "Train Epoch: 45 [5120/50000 (10%)]\tLoss: 833.812317\n",
            "Train Epoch: 45 [7680/50000 (15%)]\tLoss: 783.092590\n",
            "Train Epoch: 45 [10240/50000 (20%)]\tLoss: 737.839966\n",
            "Train Epoch: 45 [12800/50000 (26%)]\tLoss: 730.743652\n",
            "Train Epoch: 45 [15360/50000 (31%)]\tLoss: 706.995728\n",
            "Train Epoch: 45 [17920/50000 (36%)]\tLoss: 698.995911\n",
            "Train Epoch: 45 [20480/50000 (41%)]\tLoss: 661.180542\n",
            "Train Epoch: 45 [23040/50000 (46%)]\tLoss: 731.647766\n",
            "Train Epoch: 45 [25600/50000 (51%)]\tLoss: 691.734375\n",
            "Train Epoch: 45 [28160/50000 (56%)]\tLoss: 724.135132\n",
            "Train Epoch: 45 [30720/50000 (61%)]\tLoss: 747.715942\n",
            "Train Epoch: 45 [33280/50000 (66%)]\tLoss: 813.201355\n",
            "Train Epoch: 45 [35840/50000 (71%)]\tLoss: 714.011353\n",
            "Train Epoch: 45 [38400/50000 (77%)]\tLoss: 686.727417\n",
            "Train Epoch: 45 [40960/50000 (82%)]\tLoss: 690.303162\n",
            "Train Epoch: 45 [43520/50000 (87%)]\tLoss: 738.699646\n",
            "Train Epoch: 45 [46080/50000 (92%)]\tLoss: 630.786926\n",
            "Train Epoch: 45 [48640/50000 (97%)]\tLoss: 598.852417\n",
            "Epoch: 45\n",
            "Training loss: 744.5486459537428\n",
            "Train Epoch: 46 [0/50000 (0%)]\tLoss: 648.803650\n",
            "Train Epoch: 46 [2560/50000 (5%)]\tLoss: 662.561279\n",
            "Train Epoch: 46 [5120/50000 (10%)]\tLoss: 681.406067\n",
            "Train Epoch: 46 [7680/50000 (15%)]\tLoss: 710.243408\n",
            "Train Epoch: 46 [10240/50000 (20%)]\tLoss: 602.391235\n",
            "Train Epoch: 46 [12800/50000 (26%)]\tLoss: 648.183105\n",
            "Train Epoch: 46 [15360/50000 (31%)]\tLoss: 676.989746\n",
            "Train Epoch: 46 [17920/50000 (36%)]\tLoss: 649.518433\n",
            "Train Epoch: 46 [20480/50000 (41%)]\tLoss: 693.027405\n",
            "Train Epoch: 46 [23040/50000 (46%)]\tLoss: 665.204468\n",
            "Train Epoch: 46 [25600/50000 (51%)]\tLoss: 1096.722656\n",
            "Train Epoch: 46 [28160/50000 (56%)]\tLoss: 647.220337\n",
            "Train Epoch: 46 [30720/50000 (61%)]\tLoss: 652.579895\n",
            "Train Epoch: 46 [33280/50000 (66%)]\tLoss: 633.993042\n",
            "Train Epoch: 46 [35840/50000 (71%)]\tLoss: 593.672302\n",
            "Train Epoch: 46 [38400/50000 (77%)]\tLoss: 615.411011\n",
            "Train Epoch: 46 [40960/50000 (82%)]\tLoss: 596.059875\n",
            "Train Epoch: 46 [43520/50000 (87%)]\tLoss: 583.809448\n",
            "Train Epoch: 46 [46080/50000 (92%)]\tLoss: 602.532288\n",
            "Train Epoch: 46 [48640/50000 (97%)]\tLoss: 564.411377\n",
            "Train Epoch: 47 [0/50000 (0%)]\tLoss: 605.946106\n",
            "Train Epoch: 47 [2560/50000 (5%)]\tLoss: 548.038818\n",
            "Train Epoch: 47 [5120/50000 (10%)]\tLoss: 567.754761\n",
            "Train Epoch: 47 [7680/50000 (15%)]\tLoss: 621.559326\n",
            "Train Epoch: 47 [10240/50000 (20%)]\tLoss: 582.831726\n",
            "Train Epoch: 47 [12800/50000 (26%)]\tLoss: 566.370300\n",
            "Train Epoch: 47 [15360/50000 (31%)]\tLoss: 570.689880\n",
            "Train Epoch: 47 [17920/50000 (36%)]\tLoss: 587.708862\n",
            "Train Epoch: 47 [20480/50000 (41%)]\tLoss: 567.613892\n",
            "Train Epoch: 47 [23040/50000 (46%)]\tLoss: 595.712769\n",
            "Train Epoch: 47 [25600/50000 (51%)]\tLoss: 598.410767\n",
            "Train Epoch: 47 [28160/50000 (56%)]\tLoss: 534.581055\n",
            "Train Epoch: 47 [30720/50000 (61%)]\tLoss: 651.383240\n",
            "Train Epoch: 47 [33280/50000 (66%)]\tLoss: 584.972290\n",
            "Train Epoch: 47 [35840/50000 (71%)]\tLoss: 544.859253\n",
            "Train Epoch: 47 [38400/50000 (77%)]\tLoss: 1069.005371\n",
            "Train Epoch: 47 [40960/50000 (82%)]\tLoss: 539.010010\n",
            "Train Epoch: 47 [43520/50000 (87%)]\tLoss: 583.434448\n",
            "Train Epoch: 47 [46080/50000 (92%)]\tLoss: 575.521301\n",
            "Train Epoch: 47 [48640/50000 (97%)]\tLoss: 577.752380\n",
            "Train Epoch: 48 [0/50000 (0%)]\tLoss: 569.618286\n",
            "Train Epoch: 48 [2560/50000 (5%)]\tLoss: 547.982117\n",
            "Train Epoch: 48 [5120/50000 (10%)]\tLoss: 567.988403\n",
            "Train Epoch: 48 [7680/50000 (15%)]\tLoss: 557.844421\n",
            "Train Epoch: 48 [10240/50000 (20%)]\tLoss: 576.566650\n",
            "Train Epoch: 48 [12800/50000 (26%)]\tLoss: 523.799805\n",
            "Train Epoch: 48 [15360/50000 (31%)]\tLoss: 559.548218\n",
            "Train Epoch: 48 [17920/50000 (36%)]\tLoss: 543.622314\n",
            "Train Epoch: 48 [20480/50000 (41%)]\tLoss: 557.074585\n",
            "Train Epoch: 48 [23040/50000 (46%)]\tLoss: 562.969360\n",
            "Train Epoch: 48 [25600/50000 (51%)]\tLoss: 554.239807\n",
            "Train Epoch: 48 [28160/50000 (56%)]\tLoss: 547.241272\n",
            "Train Epoch: 48 [30720/50000 (61%)]\tLoss: 534.972656\n",
            "Train Epoch: 48 [33280/50000 (66%)]\tLoss: 563.949219\n",
            "Train Epoch: 48 [35840/50000 (71%)]\tLoss: 524.498901\n",
            "Train Epoch: 48 [38400/50000 (77%)]\tLoss: 568.474060\n",
            "Train Epoch: 48 [40960/50000 (82%)]\tLoss: 526.435059\n",
            "Train Epoch: 48 [43520/50000 (87%)]\tLoss: 541.408630\n",
            "Train Epoch: 48 [46080/50000 (92%)]\tLoss: 559.807617\n",
            "Train Epoch: 48 [48640/50000 (97%)]\tLoss: 534.149963\n",
            "Train Epoch: 49 [0/50000 (0%)]\tLoss: 571.880005\n",
            "Train Epoch: 49 [2560/50000 (5%)]\tLoss: 508.410461\n",
            "Train Epoch: 49 [5120/50000 (10%)]\tLoss: 539.671021\n",
            "Train Epoch: 49 [7680/50000 (15%)]\tLoss: 557.347046\n",
            "Train Epoch: 49 [10240/50000 (20%)]\tLoss: 1009.350403\n",
            "Train Epoch: 49 [12800/50000 (26%)]\tLoss: 556.839050\n",
            "Train Epoch: 49 [15360/50000 (31%)]\tLoss: 519.766968\n",
            "Train Epoch: 49 [17920/50000 (36%)]\tLoss: 532.631470\n",
            "Train Epoch: 49 [20480/50000 (41%)]\tLoss: 514.623169\n",
            "Train Epoch: 49 [23040/50000 (46%)]\tLoss: 542.295593\n",
            "Train Epoch: 49 [25600/50000 (51%)]\tLoss: 518.408203\n",
            "Train Epoch: 49 [28160/50000 (56%)]\tLoss: 1039.189209\n",
            "Train Epoch: 49 [30720/50000 (61%)]\tLoss: 535.453430\n",
            "Train Epoch: 49 [33280/50000 (66%)]\tLoss: 519.377441\n",
            "Train Epoch: 49 [35840/50000 (71%)]\tLoss: 512.774353\n",
            "Train Epoch: 49 [38400/50000 (77%)]\tLoss: 513.639526\n",
            "Train Epoch: 49 [40960/50000 (82%)]\tLoss: 535.009033\n",
            "Train Epoch: 49 [43520/50000 (87%)]\tLoss: 469.037506\n",
            "Train Epoch: 49 [46080/50000 (92%)]\tLoss: 503.227783\n",
            "Train Epoch: 49 [48640/50000 (97%)]\tLoss: 544.682434\n",
            "Train Epoch: 50 [0/50000 (0%)]\tLoss: 526.612000\n",
            "Train Epoch: 50 [2560/50000 (5%)]\tLoss: 495.525970\n",
            "Train Epoch: 50 [5120/50000 (10%)]\tLoss: 492.062103\n",
            "Train Epoch: 50 [7680/50000 (15%)]\tLoss: 492.395660\n",
            "Train Epoch: 50 [10240/50000 (20%)]\tLoss: 467.175446\n",
            "Train Epoch: 50 [12800/50000 (26%)]\tLoss: 521.491882\n",
            "Train Epoch: 50 [15360/50000 (31%)]\tLoss: 530.895996\n",
            "Train Epoch: 50 [17920/50000 (36%)]\tLoss: 492.669495\n",
            "Train Epoch: 50 [20480/50000 (41%)]\tLoss: 502.693268\n",
            "Train Epoch: 50 [23040/50000 (46%)]\tLoss: 515.908081\n",
            "Train Epoch: 50 [25600/50000 (51%)]\tLoss: 513.833862\n",
            "Train Epoch: 50 [28160/50000 (56%)]\tLoss: 534.485291\n",
            "Train Epoch: 50 [30720/50000 (61%)]\tLoss: 511.843903\n",
            "Train Epoch: 50 [33280/50000 (66%)]\tLoss: 482.017151\n",
            "Train Epoch: 50 [35840/50000 (71%)]\tLoss: 540.319458\n",
            "Train Epoch: 50 [38400/50000 (77%)]\tLoss: 504.100037\n",
            "Train Epoch: 50 [40960/50000 (82%)]\tLoss: 478.561279\n",
            "Train Epoch: 50 [43520/50000 (87%)]\tLoss: 492.398041\n",
            "Train Epoch: 50 [46080/50000 (92%)]\tLoss: 520.311829\n",
            "Train Epoch: 50 [48640/50000 (97%)]\tLoss: 498.277924\n",
            "Epoch: 50\n",
            "Training loss: 527.3678286805444\n",
            "Train Epoch: 51 [0/50000 (0%)]\tLoss: 481.369049\n",
            "Train Epoch: 51 [2560/50000 (5%)]\tLoss: 469.295105\n",
            "Train Epoch: 51 [5120/50000 (10%)]\tLoss: 498.113037\n",
            "Train Epoch: 51 [7680/50000 (15%)]\tLoss: 503.941742\n",
            "Train Epoch: 51 [10240/50000 (20%)]\tLoss: 507.272400\n",
            "Train Epoch: 51 [12800/50000 (26%)]\tLoss: 485.591248\n",
            "Train Epoch: 51 [15360/50000 (31%)]\tLoss: 474.339844\n",
            "Train Epoch: 51 [17920/50000 (36%)]\tLoss: 521.758240\n",
            "Train Epoch: 51 [20480/50000 (41%)]\tLoss: 490.865051\n",
            "Train Epoch: 51 [23040/50000 (46%)]\tLoss: 502.706848\n",
            "Train Epoch: 51 [25600/50000 (51%)]\tLoss: 537.115479\n",
            "Train Epoch: 51 [28160/50000 (56%)]\tLoss: 498.462891\n",
            "Train Epoch: 51 [30720/50000 (61%)]\tLoss: 522.613525\n",
            "Train Epoch: 51 [33280/50000 (66%)]\tLoss: 460.646057\n",
            "Train Epoch: 51 [35840/50000 (71%)]\tLoss: 468.764008\n",
            "Train Epoch: 51 [38400/50000 (77%)]\tLoss: 478.086334\n",
            "Train Epoch: 51 [40960/50000 (82%)]\tLoss: 532.429443\n",
            "Train Epoch: 51 [43520/50000 (87%)]\tLoss: 513.468933\n",
            "Train Epoch: 51 [46080/50000 (92%)]\tLoss: 509.486145\n",
            "Train Epoch: 51 [48640/50000 (97%)]\tLoss: 476.859680\n",
            "Train Epoch: 52 [0/50000 (0%)]\tLoss: 465.671875\n",
            "Train Epoch: 52 [2560/50000 (5%)]\tLoss: 527.481079\n",
            "Train Epoch: 52 [5120/50000 (10%)]\tLoss: 495.007080\n",
            "Train Epoch: 52 [7680/50000 (15%)]\tLoss: 487.766907\n",
            "Train Epoch: 52 [10240/50000 (20%)]\tLoss: 494.686127\n",
            "Train Epoch: 52 [12800/50000 (26%)]\tLoss: 490.840393\n",
            "Train Epoch: 52 [15360/50000 (31%)]\tLoss: 500.731537\n",
            "Train Epoch: 52 [17920/50000 (36%)]\tLoss: 481.976898\n",
            "Train Epoch: 52 [20480/50000 (41%)]\tLoss: 462.707825\n",
            "Train Epoch: 52 [23040/50000 (46%)]\tLoss: 478.772461\n",
            "Train Epoch: 52 [25600/50000 (51%)]\tLoss: 502.649109\n",
            "Train Epoch: 52 [28160/50000 (56%)]\tLoss: 453.029694\n",
            "Train Epoch: 52 [30720/50000 (61%)]\tLoss: 467.496887\n",
            "Train Epoch: 52 [33280/50000 (66%)]\tLoss: 460.086426\n",
            "Train Epoch: 52 [35840/50000 (71%)]\tLoss: 459.740173\n",
            "Train Epoch: 52 [38400/50000 (77%)]\tLoss: 467.336731\n",
            "Train Epoch: 52 [40960/50000 (82%)]\tLoss: 489.454895\n",
            "Train Epoch: 52 [43520/50000 (87%)]\tLoss: 429.195221\n",
            "Train Epoch: 52 [46080/50000 (92%)]\tLoss: 974.492676\n",
            "Train Epoch: 52 [48640/50000 (97%)]\tLoss: 506.289062\n",
            "Train Epoch: 53 [0/50000 (0%)]\tLoss: 459.380768\n",
            "Train Epoch: 53 [2560/50000 (5%)]\tLoss: 446.856750\n",
            "Train Epoch: 53 [5120/50000 (10%)]\tLoss: 482.048248\n",
            "Train Epoch: 53 [7680/50000 (15%)]\tLoss: 477.979156\n",
            "Train Epoch: 53 [10240/50000 (20%)]\tLoss: 462.232574\n",
            "Train Epoch: 53 [12800/50000 (26%)]\tLoss: 470.953125\n",
            "Train Epoch: 53 [15360/50000 (31%)]\tLoss: 473.067078\n",
            "Train Epoch: 53 [17920/50000 (36%)]\tLoss: 444.521332\n",
            "Train Epoch: 53 [20480/50000 (41%)]\tLoss: 433.301575\n",
            "Train Epoch: 53 [23040/50000 (46%)]\tLoss: 452.785461\n",
            "Train Epoch: 53 [25600/50000 (51%)]\tLoss: 485.460388\n",
            "Train Epoch: 53 [28160/50000 (56%)]\tLoss: 500.309509\n",
            "Train Epoch: 53 [30720/50000 (61%)]\tLoss: 451.865662\n",
            "Train Epoch: 53 [33280/50000 (66%)]\tLoss: 496.230133\n",
            "Train Epoch: 53 [35840/50000 (71%)]\tLoss: 455.715088\n",
            "Train Epoch: 53 [38400/50000 (77%)]\tLoss: 507.167633\n",
            "Train Epoch: 53 [40960/50000 (82%)]\tLoss: 986.495483\n",
            "Train Epoch: 53 [43520/50000 (87%)]\tLoss: 457.011169\n",
            "Train Epoch: 53 [46080/50000 (92%)]\tLoss: 492.875641\n",
            "Train Epoch: 53 [48640/50000 (97%)]\tLoss: 493.363953\n",
            "Train Epoch: 54 [0/50000 (0%)]\tLoss: 520.766479\n",
            "Train Epoch: 54 [2560/50000 (5%)]\tLoss: 486.701813\n",
            "Train Epoch: 54 [5120/50000 (10%)]\tLoss: 480.776306\n",
            "Train Epoch: 54 [7680/50000 (15%)]\tLoss: 483.736511\n",
            "Train Epoch: 54 [10240/50000 (20%)]\tLoss: 456.225677\n",
            "Train Epoch: 54 [12800/50000 (26%)]\tLoss: 474.909241\n",
            "Train Epoch: 54 [15360/50000 (31%)]\tLoss: 537.420532\n",
            "Train Epoch: 54 [17920/50000 (36%)]\tLoss: 477.100830\n",
            "Train Epoch: 54 [20480/50000 (41%)]\tLoss: 474.684570\n",
            "Train Epoch: 54 [23040/50000 (46%)]\tLoss: 476.853119\n",
            "Train Epoch: 54 [25600/50000 (51%)]\tLoss: 465.955750\n",
            "Train Epoch: 54 [28160/50000 (56%)]\tLoss: 493.321686\n",
            "Train Epoch: 54 [30720/50000 (61%)]\tLoss: 484.017242\n",
            "Train Epoch: 54 [33280/50000 (66%)]\tLoss: 510.420258\n",
            "Train Epoch: 54 [35840/50000 (71%)]\tLoss: 424.850525\n",
            "Train Epoch: 54 [38400/50000 (77%)]\tLoss: 476.076569\n",
            "Train Epoch: 54 [40960/50000 (82%)]\tLoss: 442.580475\n",
            "Train Epoch: 54 [43520/50000 (87%)]\tLoss: 990.707031\n",
            "Train Epoch: 54 [46080/50000 (92%)]\tLoss: 456.554993\n",
            "Train Epoch: 54 [48640/50000 (97%)]\tLoss: 429.542664\n",
            "Train Epoch: 55 [0/50000 (0%)]\tLoss: 488.555176\n",
            "Train Epoch: 55 [2560/50000 (5%)]\tLoss: 509.658508\n",
            "Train Epoch: 55 [5120/50000 (10%)]\tLoss: 447.447266\n",
            "Train Epoch: 55 [7680/50000 (15%)]\tLoss: 453.099762\n",
            "Train Epoch: 55 [10240/50000 (20%)]\tLoss: 506.254791\n",
            "Train Epoch: 55 [12800/50000 (26%)]\tLoss: 1500.751465\n",
            "Train Epoch: 55 [15360/50000 (31%)]\tLoss: 442.273529\n",
            "Train Epoch: 55 [17920/50000 (36%)]\tLoss: 472.017883\n",
            "Train Epoch: 55 [20480/50000 (41%)]\tLoss: 428.925964\n",
            "Train Epoch: 55 [23040/50000 (46%)]\tLoss: 452.133698\n",
            "Train Epoch: 55 [25600/50000 (51%)]\tLoss: 431.925934\n",
            "Train Epoch: 55 [28160/50000 (56%)]\tLoss: 461.652344\n",
            "Train Epoch: 55 [30720/50000 (61%)]\tLoss: 487.863770\n",
            "Train Epoch: 55 [33280/50000 (66%)]\tLoss: 532.789062\n",
            "Train Epoch: 55 [35840/50000 (71%)]\tLoss: 463.490051\n",
            "Train Epoch: 55 [38400/50000 (77%)]\tLoss: 477.755524\n",
            "Train Epoch: 55 [40960/50000 (82%)]\tLoss: 437.202545\n",
            "Train Epoch: 55 [43520/50000 (87%)]\tLoss: 451.776764\n",
            "Train Epoch: 55 [46080/50000 (92%)]\tLoss: 466.005859\n",
            "Train Epoch: 55 [48640/50000 (97%)]\tLoss: 464.962311\n",
            "Epoch: 55\n",
            "Training loss: 485.5636454990932\n",
            "Train Epoch: 56 [0/50000 (0%)]\tLoss: 436.624695\n",
            "Train Epoch: 56 [2560/50000 (5%)]\tLoss: 513.171997\n",
            "Train Epoch: 56 [5120/50000 (10%)]\tLoss: 459.373474\n",
            "Train Epoch: 56 [7680/50000 (15%)]\tLoss: 405.887756\n",
            "Train Epoch: 56 [10240/50000 (20%)]\tLoss: 516.691284\n",
            "Train Epoch: 56 [12800/50000 (26%)]\tLoss: 480.613464\n",
            "Train Epoch: 56 [15360/50000 (31%)]\tLoss: 513.358704\n",
            "Train Epoch: 56 [17920/50000 (36%)]\tLoss: 480.418884\n",
            "Train Epoch: 56 [20480/50000 (41%)]\tLoss: 434.556702\n",
            "Train Epoch: 56 [23040/50000 (46%)]\tLoss: 456.072510\n",
            "Train Epoch: 56 [25600/50000 (51%)]\tLoss: 428.566345\n",
            "Train Epoch: 56 [28160/50000 (56%)]\tLoss: 546.733704\n",
            "Train Epoch: 56 [30720/50000 (61%)]\tLoss: 954.271118\n",
            "Train Epoch: 56 [33280/50000 (66%)]\tLoss: 457.207947\n",
            "Train Epoch: 56 [35840/50000 (71%)]\tLoss: 466.647156\n",
            "Train Epoch: 56 [38400/50000 (77%)]\tLoss: 448.434418\n",
            "Train Epoch: 56 [40960/50000 (82%)]\tLoss: 430.823151\n",
            "Train Epoch: 56 [43520/50000 (87%)]\tLoss: 451.257812\n",
            "Train Epoch: 56 [46080/50000 (92%)]\tLoss: 409.206970\n",
            "Train Epoch: 56 [48640/50000 (97%)]\tLoss: 990.883301\n",
            "Train Epoch: 57 [0/50000 (0%)]\tLoss: 444.032410\n",
            "Train Epoch: 57 [2560/50000 (5%)]\tLoss: 439.151855\n",
            "Train Epoch: 57 [5120/50000 (10%)]\tLoss: 494.683441\n",
            "Train Epoch: 57 [7680/50000 (15%)]\tLoss: 492.330322\n",
            "Train Epoch: 57 [10240/50000 (20%)]\tLoss: 435.516052\n",
            "Train Epoch: 57 [12800/50000 (26%)]\tLoss: 450.380066\n",
            "Train Epoch: 57 [15360/50000 (31%)]\tLoss: 464.351227\n",
            "Train Epoch: 57 [17920/50000 (36%)]\tLoss: 420.112061\n",
            "Train Epoch: 57 [20480/50000 (41%)]\tLoss: 416.013123\n",
            "Train Epoch: 57 [23040/50000 (46%)]\tLoss: 455.532562\n",
            "Train Epoch: 57 [25600/50000 (51%)]\tLoss: 506.263855\n",
            "Train Epoch: 57 [28160/50000 (56%)]\tLoss: 429.505524\n",
            "Train Epoch: 57 [30720/50000 (61%)]\tLoss: 943.987183\n",
            "Train Epoch: 57 [33280/50000 (66%)]\tLoss: 427.869690\n",
            "Train Epoch: 57 [35840/50000 (71%)]\tLoss: 438.261902\n",
            "Train Epoch: 57 [38400/50000 (77%)]\tLoss: 453.396484\n",
            "Train Epoch: 57 [40960/50000 (82%)]\tLoss: 445.995789\n",
            "Train Epoch: 57 [43520/50000 (87%)]\tLoss: 514.291138\n",
            "Train Epoch: 57 [46080/50000 (92%)]\tLoss: 457.760071\n",
            "Train Epoch: 57 [48640/50000 (97%)]\tLoss: 431.836365\n",
            "Train Epoch: 58 [0/50000 (0%)]\tLoss: 463.748718\n",
            "Train Epoch: 58 [2560/50000 (5%)]\tLoss: 457.257385\n",
            "Train Epoch: 58 [5120/50000 (10%)]\tLoss: 451.815002\n",
            "Train Epoch: 58 [7680/50000 (15%)]\tLoss: 451.725922\n",
            "Train Epoch: 58 [10240/50000 (20%)]\tLoss: 416.665100\n",
            "Train Epoch: 58 [12800/50000 (26%)]\tLoss: 441.738434\n",
            "Train Epoch: 58 [15360/50000 (31%)]\tLoss: 422.109711\n",
            "Train Epoch: 58 [17920/50000 (36%)]\tLoss: 443.737976\n",
            "Train Epoch: 58 [20480/50000 (41%)]\tLoss: 470.883484\n",
            "Train Epoch: 58 [23040/50000 (46%)]\tLoss: 490.817566\n",
            "Train Epoch: 58 [25600/50000 (51%)]\tLoss: 497.438965\n",
            "Train Epoch: 58 [28160/50000 (56%)]\tLoss: 458.939941\n",
            "Train Epoch: 58 [30720/50000 (61%)]\tLoss: 467.702576\n",
            "Train Epoch: 58 [33280/50000 (66%)]\tLoss: 449.875671\n",
            "Train Epoch: 58 [35840/50000 (71%)]\tLoss: 473.600189\n",
            "Train Epoch: 58 [38400/50000 (77%)]\tLoss: 409.371277\n",
            "Train Epoch: 58 [40960/50000 (82%)]\tLoss: 490.521576\n",
            "Train Epoch: 58 [43520/50000 (87%)]\tLoss: 416.555817\n",
            "Train Epoch: 58 [46080/50000 (92%)]\tLoss: 470.767334\n",
            "Train Epoch: 58 [48640/50000 (97%)]\tLoss: 476.912598\n",
            "Train Epoch: 59 [0/50000 (0%)]\tLoss: 436.056213\n",
            "Train Epoch: 59 [2560/50000 (5%)]\tLoss: 456.198303\n",
            "Train Epoch: 59 [5120/50000 (10%)]\tLoss: 939.942688\n",
            "Train Epoch: 59 [7680/50000 (15%)]\tLoss: 481.679108\n",
            "Train Epoch: 59 [10240/50000 (20%)]\tLoss: 434.888092\n",
            "Train Epoch: 59 [12800/50000 (26%)]\tLoss: 457.291656\n",
            "Train Epoch: 59 [15360/50000 (31%)]\tLoss: 432.379944\n",
            "Train Epoch: 59 [17920/50000 (36%)]\tLoss: 416.461426\n",
            "Train Epoch: 59 [20480/50000 (41%)]\tLoss: 452.294739\n",
            "Train Epoch: 59 [23040/50000 (46%)]\tLoss: 436.765045\n",
            "Train Epoch: 59 [25600/50000 (51%)]\tLoss: 438.115326\n",
            "Train Epoch: 59 [28160/50000 (56%)]\tLoss: 479.350647\n",
            "Train Epoch: 59 [30720/50000 (61%)]\tLoss: 1038.053833\n",
            "Train Epoch: 59 [33280/50000 (66%)]\tLoss: 490.680969\n",
            "Train Epoch: 59 [35840/50000 (71%)]\tLoss: 457.890686\n",
            "Train Epoch: 59 [38400/50000 (77%)]\tLoss: 433.881195\n",
            "Train Epoch: 59 [40960/50000 (82%)]\tLoss: 438.099915\n",
            "Train Epoch: 59 [43520/50000 (87%)]\tLoss: 447.229584\n",
            "Train Epoch: 59 [46080/50000 (92%)]\tLoss: 521.256714\n",
            "Train Epoch: 59 [48640/50000 (97%)]\tLoss: 412.616791\n",
            "Train Epoch: 60 [0/50000 (0%)]\tLoss: 475.991119\n",
            "Train Epoch: 60 [2560/50000 (5%)]\tLoss: 493.117859\n",
            "Train Epoch: 60 [5120/50000 (10%)]\tLoss: 430.230225\n",
            "Train Epoch: 60 [7680/50000 (15%)]\tLoss: 442.263428\n",
            "Train Epoch: 60 [10240/50000 (20%)]\tLoss: 420.431976\n",
            "Train Epoch: 60 [12800/50000 (26%)]\tLoss: 436.651276\n",
            "Train Epoch: 60 [15360/50000 (31%)]\tLoss: 452.604797\n",
            "Train Epoch: 60 [17920/50000 (36%)]\tLoss: 432.103607\n",
            "Train Epoch: 60 [20480/50000 (41%)]\tLoss: 441.914246\n",
            "Train Epoch: 60 [23040/50000 (46%)]\tLoss: 475.531982\n",
            "Train Epoch: 60 [25600/50000 (51%)]\tLoss: 488.387878\n",
            "Train Epoch: 60 [28160/50000 (56%)]\tLoss: 482.057129\n",
            "Train Epoch: 60 [30720/50000 (61%)]\tLoss: 445.862854\n",
            "Train Epoch: 60 [33280/50000 (66%)]\tLoss: 466.187195\n",
            "Train Epoch: 60 [35840/50000 (71%)]\tLoss: 473.379395\n",
            "Train Epoch: 60 [38400/50000 (77%)]\tLoss: 433.986145\n",
            "Train Epoch: 60 [40960/50000 (82%)]\tLoss: 482.429993\n",
            "Train Epoch: 60 [43520/50000 (87%)]\tLoss: 488.381653\n",
            "Train Epoch: 60 [46080/50000 (92%)]\tLoss: 477.075104\n",
            "Train Epoch: 60 [48640/50000 (97%)]\tLoss: 453.401611\n",
            "Epoch: 60\n",
            "Training loss: 476.31205235695353\n",
            "Train Epoch: 61 [0/50000 (0%)]\tLoss: 429.534912\n",
            "Train Epoch: 61 [2560/50000 (5%)]\tLoss: 455.857513\n",
            "Train Epoch: 61 [5120/50000 (10%)]\tLoss: 462.828217\n",
            "Train Epoch: 61 [7680/50000 (15%)]\tLoss: 460.134827\n",
            "Train Epoch: 61 [10240/50000 (20%)]\tLoss: 446.307220\n",
            "Train Epoch: 61 [12800/50000 (26%)]\tLoss: 470.077881\n",
            "Train Epoch: 61 [15360/50000 (31%)]\tLoss: 448.336914\n",
            "Train Epoch: 61 [17920/50000 (36%)]\tLoss: 486.510010\n",
            "Train Epoch: 61 [20480/50000 (41%)]\tLoss: 968.358765\n",
            "Train Epoch: 61 [23040/50000 (46%)]\tLoss: 461.150299\n",
            "Train Epoch: 61 [25600/50000 (51%)]\tLoss: 498.986481\n",
            "Train Epoch: 61 [28160/50000 (56%)]\tLoss: 470.535553\n",
            "Train Epoch: 61 [30720/50000 (61%)]\tLoss: 455.694275\n",
            "Train Epoch: 61 [33280/50000 (66%)]\tLoss: 441.486633\n",
            "Train Epoch: 61 [35840/50000 (71%)]\tLoss: 435.269775\n",
            "Train Epoch: 61 [38400/50000 (77%)]\tLoss: 538.117798\n",
            "Train Epoch: 61 [40960/50000 (82%)]\tLoss: 447.810852\n",
            "Train Epoch: 61 [43520/50000 (87%)]\tLoss: 439.015289\n",
            "Train Epoch: 61 [46080/50000 (92%)]\tLoss: 455.769775\n",
            "Train Epoch: 61 [48640/50000 (97%)]\tLoss: 421.697083\n",
            "Train Epoch: 62 [0/50000 (0%)]\tLoss: 449.613617\n",
            "Train Epoch: 62 [2560/50000 (5%)]\tLoss: 496.295349\n",
            "Train Epoch: 62 [5120/50000 (10%)]\tLoss: 445.423248\n",
            "Train Epoch: 62 [7680/50000 (15%)]\tLoss: 467.678802\n",
            "Train Epoch: 62 [10240/50000 (20%)]\tLoss: 466.925781\n",
            "Train Epoch: 62 [12800/50000 (26%)]\tLoss: 450.122742\n",
            "Train Epoch: 62 [15360/50000 (31%)]\tLoss: 461.284180\n",
            "Train Epoch: 62 [17920/50000 (36%)]\tLoss: 480.017181\n",
            "Train Epoch: 62 [20480/50000 (41%)]\tLoss: 465.082336\n",
            "Train Epoch: 62 [23040/50000 (46%)]\tLoss: 437.533936\n",
            "Train Epoch: 62 [25600/50000 (51%)]\tLoss: 454.895935\n",
            "Train Epoch: 62 [28160/50000 (56%)]\tLoss: 456.864075\n",
            "Train Epoch: 62 [30720/50000 (61%)]\tLoss: 470.507172\n",
            "Train Epoch: 62 [33280/50000 (66%)]\tLoss: 422.315979\n",
            "Train Epoch: 62 [35840/50000 (71%)]\tLoss: 436.359283\n",
            "Train Epoch: 62 [38400/50000 (77%)]\tLoss: 488.280670\n",
            "Train Epoch: 62 [40960/50000 (82%)]\tLoss: 461.108215\n",
            "Train Epoch: 62 [43520/50000 (87%)]\tLoss: 481.877502\n",
            "Train Epoch: 62 [46080/50000 (92%)]\tLoss: 451.668701\n",
            "Train Epoch: 62 [48640/50000 (97%)]\tLoss: 432.909119\n",
            "Train Epoch: 63 [0/50000 (0%)]\tLoss: 463.632721\n",
            "Train Epoch: 63 [2560/50000 (5%)]\tLoss: 436.891754\n",
            "Train Epoch: 63 [5120/50000 (10%)]\tLoss: 454.837341\n",
            "Train Epoch: 63 [7680/50000 (15%)]\tLoss: 446.031647\n",
            "Train Epoch: 63 [10240/50000 (20%)]\tLoss: 478.465515\n",
            "Train Epoch: 63 [12800/50000 (26%)]\tLoss: 462.048737\n",
            "Train Epoch: 63 [15360/50000 (31%)]\tLoss: 448.912933\n",
            "Train Epoch: 63 [17920/50000 (36%)]\tLoss: 430.807129\n",
            "Train Epoch: 63 [20480/50000 (41%)]\tLoss: 426.034729\n",
            "Train Epoch: 63 [23040/50000 (46%)]\tLoss: 432.618103\n",
            "Train Epoch: 63 [25600/50000 (51%)]\tLoss: 446.646332\n",
            "Train Epoch: 63 [28160/50000 (56%)]\tLoss: 405.660339\n",
            "Train Epoch: 63 [30720/50000 (61%)]\tLoss: 451.586243\n",
            "Train Epoch: 63 [33280/50000 (66%)]\tLoss: 399.669678\n",
            "Train Epoch: 63 [35840/50000 (71%)]\tLoss: 438.789398\n",
            "Train Epoch: 63 [38400/50000 (77%)]\tLoss: 455.381012\n",
            "Train Epoch: 63 [40960/50000 (82%)]\tLoss: 437.623871\n",
            "Train Epoch: 63 [43520/50000 (87%)]\tLoss: 462.329742\n",
            "Train Epoch: 63 [46080/50000 (92%)]\tLoss: 450.123444\n",
            "Train Epoch: 63 [48640/50000 (97%)]\tLoss: 425.614502\n",
            "Train Epoch: 64 [0/50000 (0%)]\tLoss: 457.615875\n",
            "Train Epoch: 64 [2560/50000 (5%)]\tLoss: 437.666351\n",
            "Train Epoch: 64 [5120/50000 (10%)]\tLoss: 428.363190\n",
            "Train Epoch: 64 [7680/50000 (15%)]\tLoss: 472.429138\n",
            "Train Epoch: 64 [10240/50000 (20%)]\tLoss: 448.411041\n",
            "Train Epoch: 64 [12800/50000 (26%)]\tLoss: 477.138611\n",
            "Train Epoch: 64 [15360/50000 (31%)]\tLoss: 503.948669\n",
            "Train Epoch: 64 [17920/50000 (36%)]\tLoss: 470.884094\n",
            "Train Epoch: 64 [20480/50000 (41%)]\tLoss: 502.836823\n",
            "Train Epoch: 64 [23040/50000 (46%)]\tLoss: 453.218658\n",
            "Train Epoch: 64 [25600/50000 (51%)]\tLoss: 489.380310\n",
            "Train Epoch: 64 [28160/50000 (56%)]\tLoss: 463.644470\n",
            "Train Epoch: 64 [30720/50000 (61%)]\tLoss: 462.317871\n",
            "Train Epoch: 64 [33280/50000 (66%)]\tLoss: 461.156250\n",
            "Train Epoch: 64 [35840/50000 (71%)]\tLoss: 422.959656\n",
            "Train Epoch: 64 [38400/50000 (77%)]\tLoss: 502.791351\n",
            "Train Epoch: 64 [40960/50000 (82%)]\tLoss: 443.821686\n",
            "Train Epoch: 64 [43520/50000 (87%)]\tLoss: 461.013672\n",
            "Train Epoch: 64 [46080/50000 (92%)]\tLoss: 443.948639\n",
            "Train Epoch: 64 [48640/50000 (97%)]\tLoss: 452.032288\n",
            "Train Epoch: 65 [0/50000 (0%)]\tLoss: 438.357513\n",
            "Train Epoch: 65 [2560/50000 (5%)]\tLoss: 465.741699\n",
            "Train Epoch: 65 [5120/50000 (10%)]\tLoss: 441.556793\n",
            "Train Epoch: 65 [7680/50000 (15%)]\tLoss: 435.144684\n",
            "Train Epoch: 65 [10240/50000 (20%)]\tLoss: 446.628510\n",
            "Train Epoch: 65 [12800/50000 (26%)]\tLoss: 473.613037\n",
            "Train Epoch: 65 [15360/50000 (31%)]\tLoss: 444.717133\n",
            "Train Epoch: 65 [17920/50000 (36%)]\tLoss: 425.120789\n",
            "Train Epoch: 65 [20480/50000 (41%)]\tLoss: 477.522156\n",
            "Train Epoch: 65 [23040/50000 (46%)]\tLoss: 470.805359\n",
            "Train Epoch: 65 [25600/50000 (51%)]\tLoss: 411.604156\n",
            "Train Epoch: 65 [28160/50000 (56%)]\tLoss: 407.149933\n",
            "Train Epoch: 65 [30720/50000 (61%)]\tLoss: 474.323792\n",
            "Train Epoch: 65 [33280/50000 (66%)]\tLoss: 452.047546\n",
            "Train Epoch: 65 [35840/50000 (71%)]\tLoss: 428.250061\n",
            "Train Epoch: 65 [38400/50000 (77%)]\tLoss: 415.785492\n",
            "Train Epoch: 65 [40960/50000 (82%)]\tLoss: 454.226959\n",
            "Train Epoch: 65 [43520/50000 (87%)]\tLoss: 468.077057\n",
            "Train Epoch: 65 [46080/50000 (92%)]\tLoss: 462.132263\n",
            "Train Epoch: 65 [48640/50000 (97%)]\tLoss: 443.421570\n",
            "Epoch: 65\n",
            "Training loss: 465.65617868851643\n",
            "Train Epoch: 66 [0/50000 (0%)]\tLoss: 461.393372\n",
            "Train Epoch: 66 [2560/50000 (5%)]\tLoss: 467.833496\n",
            "Train Epoch: 66 [5120/50000 (10%)]\tLoss: 447.838074\n",
            "Train Epoch: 66 [7680/50000 (15%)]\tLoss: 461.128906\n",
            "Train Epoch: 66 [10240/50000 (20%)]\tLoss: 464.872650\n",
            "Train Epoch: 66 [12800/50000 (26%)]\tLoss: 456.332184\n",
            "Train Epoch: 66 [15360/50000 (31%)]\tLoss: 475.916901\n",
            "Train Epoch: 66 [17920/50000 (36%)]\tLoss: 425.809235\n",
            "Train Epoch: 66 [20480/50000 (41%)]\tLoss: 442.017639\n",
            "Train Epoch: 66 [23040/50000 (46%)]\tLoss: 425.569244\n",
            "Train Epoch: 66 [25600/50000 (51%)]\tLoss: 501.505066\n",
            "Train Epoch: 66 [28160/50000 (56%)]\tLoss: 462.370667\n",
            "Train Epoch: 66 [30720/50000 (61%)]\tLoss: 474.663818\n",
            "Train Epoch: 66 [33280/50000 (66%)]\tLoss: 506.235596\n",
            "Train Epoch: 66 [35840/50000 (71%)]\tLoss: 415.211609\n",
            "Train Epoch: 66 [38400/50000 (77%)]\tLoss: 471.990784\n",
            "Train Epoch: 66 [40960/50000 (82%)]\tLoss: 441.901917\n",
            "Train Epoch: 66 [43520/50000 (87%)]\tLoss: 497.285828\n",
            "Train Epoch: 66 [46080/50000 (92%)]\tLoss: 431.657898\n",
            "Train Epoch: 66 [48640/50000 (97%)]\tLoss: 428.773438\n",
            "Train Epoch: 67 [0/50000 (0%)]\tLoss: 474.010071\n",
            "Train Epoch: 67 [2560/50000 (5%)]\tLoss: 441.534576\n",
            "Train Epoch: 67 [5120/50000 (10%)]\tLoss: 427.588470\n",
            "Train Epoch: 67 [7680/50000 (15%)]\tLoss: 444.160950\n",
            "Train Epoch: 67 [10240/50000 (20%)]\tLoss: 417.179504\n",
            "Train Epoch: 67 [12800/50000 (26%)]\tLoss: 492.948151\n",
            "Train Epoch: 67 [15360/50000 (31%)]\tLoss: 437.669434\n",
            "Train Epoch: 67 [17920/50000 (36%)]\tLoss: 464.351715\n",
            "Train Epoch: 67 [20480/50000 (41%)]\tLoss: 405.446655\n",
            "Train Epoch: 67 [23040/50000 (46%)]\tLoss: 427.445618\n",
            "Train Epoch: 67 [25600/50000 (51%)]\tLoss: 420.803162\n",
            "Train Epoch: 67 [28160/50000 (56%)]\tLoss: 425.040283\n",
            "Train Epoch: 67 [30720/50000 (61%)]\tLoss: 435.728027\n",
            "Train Epoch: 67 [33280/50000 (66%)]\tLoss: 464.695312\n",
            "Train Epoch: 67 [35840/50000 (71%)]\tLoss: 471.103180\n",
            "Train Epoch: 67 [38400/50000 (77%)]\tLoss: 448.674469\n",
            "Train Epoch: 67 [40960/50000 (82%)]\tLoss: 463.534729\n",
            "Train Epoch: 67 [43520/50000 (87%)]\tLoss: 427.124573\n",
            "Train Epoch: 67 [46080/50000 (92%)]\tLoss: 370.588013\n",
            "Train Epoch: 67 [48640/50000 (97%)]\tLoss: 399.658844\n",
            "Train Epoch: 68 [0/50000 (0%)]\tLoss: 442.429291\n",
            "Train Epoch: 68 [2560/50000 (5%)]\tLoss: 432.213043\n",
            "Train Epoch: 68 [5120/50000 (10%)]\tLoss: 463.295410\n",
            "Train Epoch: 68 [7680/50000 (15%)]\tLoss: 437.195740\n",
            "Train Epoch: 68 [10240/50000 (20%)]\tLoss: 463.214264\n",
            "Train Epoch: 68 [12800/50000 (26%)]\tLoss: 436.070618\n",
            "Train Epoch: 68 [15360/50000 (31%)]\tLoss: 452.815521\n",
            "Train Epoch: 68 [17920/50000 (36%)]\tLoss: 465.386719\n",
            "Train Epoch: 68 [20480/50000 (41%)]\tLoss: 405.622986\n",
            "Train Epoch: 68 [23040/50000 (46%)]\tLoss: 908.845032\n",
            "Train Epoch: 68 [25600/50000 (51%)]\tLoss: 430.337830\n",
            "Train Epoch: 68 [28160/50000 (56%)]\tLoss: 435.150055\n",
            "Train Epoch: 68 [30720/50000 (61%)]\tLoss: 453.386292\n",
            "Train Epoch: 68 [33280/50000 (66%)]\tLoss: 433.300201\n",
            "Train Epoch: 68 [35840/50000 (71%)]\tLoss: 443.122986\n",
            "Train Epoch: 68 [38400/50000 (77%)]\tLoss: 459.342865\n",
            "Train Epoch: 68 [40960/50000 (82%)]\tLoss: 426.754700\n",
            "Train Epoch: 68 [43520/50000 (87%)]\tLoss: 448.856049\n",
            "Train Epoch: 68 [46080/50000 (92%)]\tLoss: 454.791107\n",
            "Train Epoch: 68 [48640/50000 (97%)]\tLoss: 424.356934\n",
            "Train Epoch: 69 [0/50000 (0%)]\tLoss: 392.369690\n",
            "Train Epoch: 69 [2560/50000 (5%)]\tLoss: 461.277008\n",
            "Train Epoch: 69 [5120/50000 (10%)]\tLoss: 416.002136\n",
            "Train Epoch: 69 [7680/50000 (15%)]\tLoss: 425.060791\n",
            "Train Epoch: 69 [10240/50000 (20%)]\tLoss: 431.891754\n",
            "Train Epoch: 69 [12800/50000 (26%)]\tLoss: 425.719788\n",
            "Train Epoch: 69 [15360/50000 (31%)]\tLoss: 489.275238\n",
            "Train Epoch: 69 [17920/50000 (36%)]\tLoss: 420.875092\n",
            "Train Epoch: 69 [20480/50000 (41%)]\tLoss: 444.904846\n",
            "Train Epoch: 69 [23040/50000 (46%)]\tLoss: 462.998688\n",
            "Train Epoch: 69 [25600/50000 (51%)]\tLoss: 434.209412\n",
            "Train Epoch: 69 [28160/50000 (56%)]\tLoss: 447.865631\n",
            "Train Epoch: 69 [30720/50000 (61%)]\tLoss: 536.107849\n",
            "Train Epoch: 69 [33280/50000 (66%)]\tLoss: 548.445679\n",
            "Train Epoch: 69 [35840/50000 (71%)]\tLoss: 682.797241\n",
            "Train Epoch: 69 [38400/50000 (77%)]\tLoss: 525.941772\n",
            "Train Epoch: 69 [40960/50000 (82%)]\tLoss: 456.269531\n",
            "Train Epoch: 69 [43520/50000 (87%)]\tLoss: 440.152588\n",
            "Train Epoch: 69 [46080/50000 (92%)]\tLoss: 456.963074\n",
            "Train Epoch: 69 [48640/50000 (97%)]\tLoss: 443.884705\n",
            "Train Epoch: 70 [0/50000 (0%)]\tLoss: 465.738708\n",
            "Train Epoch: 70 [2560/50000 (5%)]\tLoss: 425.132874\n",
            "Train Epoch: 70 [5120/50000 (10%)]\tLoss: 451.377991\n",
            "Train Epoch: 70 [7680/50000 (15%)]\tLoss: 481.463287\n",
            "Train Epoch: 70 [10240/50000 (20%)]\tLoss: 417.570892\n",
            "Train Epoch: 70 [12800/50000 (26%)]\tLoss: 427.976562\n",
            "Train Epoch: 70 [15360/50000 (31%)]\tLoss: 443.550629\n",
            "Train Epoch: 70 [17920/50000 (36%)]\tLoss: 432.572357\n",
            "Train Epoch: 70 [20480/50000 (41%)]\tLoss: 429.835052\n",
            "Train Epoch: 70 [23040/50000 (46%)]\tLoss: 448.251068\n",
            "Train Epoch: 70 [25600/50000 (51%)]\tLoss: 403.828308\n",
            "Train Epoch: 70 [28160/50000 (56%)]\tLoss: 444.929932\n",
            "Train Epoch: 70 [30720/50000 (61%)]\tLoss: 427.304199\n",
            "Train Epoch: 70 [33280/50000 (66%)]\tLoss: 468.635193\n",
            "Train Epoch: 70 [35840/50000 (71%)]\tLoss: 402.269531\n",
            "Train Epoch: 70 [38400/50000 (77%)]\tLoss: 438.504883\n",
            "Train Epoch: 70 [40960/50000 (82%)]\tLoss: 448.721619\n",
            "Train Epoch: 70 [43520/50000 (87%)]\tLoss: 415.711670\n",
            "Train Epoch: 70 [46080/50000 (92%)]\tLoss: 413.373444\n",
            "Train Epoch: 70 [48640/50000 (97%)]\tLoss: 459.593933\n",
            "Epoch: 70\n",
            "Training loss: 458.0686933556382\n",
            "Saving final model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_Zqkk9BP7I"
      },
      "source": [
        "# Genre Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Ry0Z-X4Gob"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import librosa\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbr--1XW5LJw",
        "outputId": "2b2c1450-8f58-424c-d174-1dd63a6c633c"
      },
      "source": [
        "# Read how many genres exist in our data set\n",
        "import os\n",
        "IN_PATH = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/genres'\n",
        "OUT_PATH = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/spectrograms'\n",
        "for folder in os.walk(IN_PATH):\n",
        "  folders = folder[1]\n",
        "  break\n",
        "print(folders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rock', 'reggae', 'pop', 'metal', 'jazz', 'hiphop', 'disco', 'country', 'classical', 'blues']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "qwfEYCQS1H1k",
        "outputId": "004ce268-66d7-4bee-bab9-07d06608183d"
      },
      "source": [
        "# Generate Spectrograms\n",
        "import matplotlib.pyplot as plt\n",
        "cmap = plt.get_cmap('inferno')\n",
        "plt.figure(figsize=(8,8))\n",
        "for folder in folders:\n",
        "  os.makedirs(os.path.join(OUT_PATH, folder), exist_ok=True)\n",
        "  print(folder)\n",
        "  for file in os.listdir(os.path.join(IN_PATH, folder)):\n",
        "    y, sr = librosa.load(os.path.join(IN_PATH, folder, file), mono=True, duration=5)\n",
        "    rmse = librosa.feature.rmse(y=y)\n",
        "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "    features = [torch.mean(chroma_stft), torch.mean(rmse), torch.mean(spec_cent), torch.mean(spec_bw), torch.mean(rolloff), torch.mean(zcr)]   \n",
        "    for e in mfcc:\n",
        "        features.append(torch.mean(e))\n",
        "    break\n",
        "    plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB')\n",
        "    plt.axis('off');\n",
        "    #plt.show()\n",
        "    plt.savefig(os.path.join(OUT_PATH, folder,f'{file[:-3].replace(\".\", \"\")}.png'))\n",
        "    plt.clf()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e0f3be0fb068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inferno'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'folders' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ztzfqXSDt9"
      },
      "source": [
        "# Spectrogram Data Loading\n",
        "from skimage import io, transform\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_folder, folders, transform=None):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "        # self.data_folder = data_folder\n",
        "        self.folders = folders\n",
        "        self.data = []\n",
        "        self.training = True\n",
        "        for folder in os.listdir(data_folder):\n",
        "          for file in os.listdir(os.path.join(data_folder, folder)):\n",
        "            path = os.path.join(data_folder, folder, file)\n",
        "            self.data.append(path)\n",
        "\n",
        "    def set_training(is_training):\n",
        "        self.training = is_training\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of songs\n",
        "         return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        img = io.imread(data)[:,:,:3]\n",
        "        if self.transform:\n",
        "          img = self.transform(img)\n",
        "        label = self.folders.index(os.path.basename(os.path.dirname(data)))\n",
        "        return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEmmxqPY_-Oa"
      },
      "source": [
        "# Raw Data Loading\n",
        "from scipy.io.wavfile import read, write\n",
        "import pandas as pd\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_folder, folders, transform=None):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "        # self.data_folder = data_folder\n",
        "        self.folders = folders\n",
        "        self.data = []\n",
        "        for folder in self.folders:\n",
        "          for file in os.listdir(os.path.join(data_folder, folder)):\n",
        "            path = os.path.join(data_folder, folder, file)\n",
        "            self.data.append(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of songs\n",
        "         return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        data = self.data[idx]\n",
        "        rate, audio = read(data)\n",
        "        label = self.folders.index(os.path.basename(os.path.dirname(data)))\n",
        "        return audio, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYRGlqqrbq2V"
      },
      "source": [
        "# Raw Data resample Loading\n",
        "from scipy.io.wavfile import read, write\n",
        "import pandas as pd\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_folder, folders, transform=None):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "        # self.data_folder = data_folder\n",
        "        self.folders = folders\n",
        "        self.data = []\n",
        "        for folder in self.folders:\n",
        "          for file in os.listdir(os.path.join(data_folder, folder)):\n",
        "            path = os.path.join(data_folder, folder, file)\n",
        "            self.data.append(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of songs\n",
        "         return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        num_output = 882000#8000\n",
        "        \n",
        "        data = self.data[idx]\n",
        "        rate, audio = read(data)\n",
        "        total_samples = len(audio)\n",
        "        seconds = total_samples // rate\n",
        "        samples_per_sec = num_output // seconds\n",
        "        extras =  num_output - samples_per_sec * seconds\n",
        "        left_over = audio[seconds * samples_per_sec:]\n",
        "        audio = audio[: seconds * samples_per_sec]\n",
        "        audio = audio.reshape((seconds, -1))\n",
        "        #audio = np.array_split(audio, seconds)\n",
        "        #audio = np.array(audio)\n",
        "        sampled = []\n",
        "        for i in range(len(audio)):\n",
        "          second = audio[i]\n",
        "          sampled = sampled + np.random.choice(second, samples_per_sec, replace=False).tolist()\n",
        "        sampled = sampled + np.random.choice(left_over, extras, replace=False).tolist()\n",
        "        # Need to sample 8000 amplitudes from audio\n",
        "        # Each second contains \"rate\" samples. Want to sample from\n",
        "        # all seconds uniformly. \n",
        "        final = np.array(sampled)\n",
        "        if self.transform:\n",
        "          final = self.transform(final)\n",
        "        label = self.folders.index(os.path.basename(os.path.dirname(data)))\n",
        "        return final, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr_T1OtKRfHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c325b03-2389-48c7-b11c-379c316b1415"
      },
      "source": [
        "# Feature Data Loading\n",
        "!pip install torchaudio\n",
        "from scipy.io.wavfile import read, write\n",
        "import torchaudio\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_folder, folders, device, transform=None):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "        # self.data_folder = data_folder\n",
        "        self.folders = folders\n",
        "        self.data = []\n",
        "        self.device = device\n",
        "        for folder in self.folders:\n",
        "          for file in os.listdir(os.path.join(data_folder, folder)):\n",
        "            path = os.path.join(data_folder, folder, file)\n",
        "            self.data.append(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of songs\n",
        "         return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        data = self.data[idx]\n",
        "        #audio, sr = torchaudio.load(data)\n",
        "        #audio = audio[:,sr * 4: sr * 4 + 50]\n",
        "        #mfcc = torchaudio.transforms.MFCC()(audio, padding=0)\n",
        "        #mfcc = mfcc.squeeze().mean(dim=1)\n",
        "        input, rate = torchaudio.load(data)\n",
        "        input = input.squeeze()\n",
        "        mfcc = torchaudio.transforms.MFCC()(input)\n",
        "        mfcc = mfcc.squeeze().mean(dim=1).to(self.device)\n",
        "        spec = torchaudio.transforms.Spectrogram()(input).mean(dim=1).to(self.device)\n",
        "        mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=rate)(input).mean(dim=1).to(self.device)\n",
        "        input_features = torch.cat((mfcc, spec, mel_spec), dim=0)\n",
        "        \n",
        "        label = self.folders.index(os.path.basename(os.path.dirname(data)))\n",
        "        return input_features, label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAgY-hokBiPQ"
      },
      "source": [
        "# Genre Classification Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYRmSMQL50fS"
      },
      "source": [
        "# CNN that classifies genre from a spectrogram\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MusicIdentifyNet(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(MusicIdentifyNet, self).__init__()\n",
        "        # input 576 x 576 x 3\n",
        "        # spectrogram resized 128 x 128 x 3\n",
        "        # raw resized 32 x 32 x 1\n",
        "        self.output_size = output_size\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #self.fc = nn.Linear(32768, self.output_size)\n",
        "        self.fc = nn.Linear(2048, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        #print(x.shape)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def get_feature(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label):\n",
        "        loss_val = F.cross_entropy(prediction, label)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOTrbRR8YvGm"
      },
      "source": [
        "# Genre classification using a fully connected network. Inputs are audio features.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MusicIdentifyNet(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(MusicIdentifyNet, self).__init__()\n",
        "        # input 8000\n",
        "        # Features 40\n",
        "        # 882000 audio samples\n",
        "        self.output_size = output_size\n",
        "        #self.fc = nn.Linear(661794, 10)\n",
        "        self.fc1 = nn.Linear(369, 500)\n",
        "        self.fc2 = nn.Linear(500, 250)\n",
        "        self.fc3 = nn.Linear(250, 100)\n",
        "        self.fc4 = nn.Linear(100, self.output_size)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        \n",
        "        #x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def get_feature(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label):\n",
        "        loss_val = F.cross_entropy(prediction, label)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMFk0n1tWwDX"
      },
      "source": [
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        #data = data.float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            correct += torch.sum(torch.argmax(output, dim=1) == label)\n",
        "            \n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnFB7VH0SJ3Z",
        "outputId": "0da8dea4-9ee6-4abd-bc30-eae2f8a4563e"
      },
      "source": [
        "# Training, Testing\n",
        "def main():\n",
        "    BATCH_SIZE = 50#128#256\n",
        "    FEATURE_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 256\n",
        "    EPOCHS = 30\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/'\n",
        "\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/genres/'\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    data_train = MusicDataset('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/genres', folders, device)\n",
        "    \n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True)#, **kwargs)\n",
        "    \n",
        "    model = MusicIdentifyNet(10).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "    try:\n",
        "        for epoch in range(EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            #test_loss, test_accuracy = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            #test_losses.append((epoch, test_loss))\n",
        "            #test_accuracies.append((epoch, test_accuracy))\n",
        "            \n",
        "            torch.save(model, DATA_PATH + 'large_fully_conn.pt')\n",
        "            if epoch % 5 == 0:\n",
        "              print('Epoch:', epoch)\n",
        "              print('Training loss:', train_loss)\n",
        "            \n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        torch.save(model, DATA_PATH + 'large_fully_conn.pt')\n",
        "        return model, device\n",
        "final_model, device = main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "torch.Size([369])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchaudio/functional.py:318: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 2.523378\n",
            "Train Epoch: 0 [500/1000 (50%)]\tLoss: 1.876901\n",
            "Epoch: 0\n",
            "Training loss: 2.0402449786663057\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 1.476927\n",
            "Train Epoch: 1 [500/1000 (50%)]\tLoss: 1.351130\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 1.280463\n",
            "Train Epoch: 2 [500/1000 (50%)]\tLoss: 1.417045\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 1.040135\n",
            "Train Epoch: 3 [500/1000 (50%)]\tLoss: 1.038127\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 1.063559\n",
            "Train Epoch: 4 [500/1000 (50%)]\tLoss: 1.167477\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 1.022339\n",
            "Train Epoch: 5 [500/1000 (50%)]\tLoss: 1.341738\n",
            "Epoch: 5\n",
            "Training loss: 0.9724592953920365\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 0.654276\n",
            "Train Epoch: 6 [500/1000 (50%)]\tLoss: 0.734699\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 0.970754\n",
            "Train Epoch: 7 [500/1000 (50%)]\tLoss: 0.843862\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 0.676677\n",
            "Train Epoch: 8 [500/1000 (50%)]\tLoss: 0.850532\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 0.804677\n",
            "Train Epoch: 9 [500/1000 (50%)]\tLoss: 0.810039\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 0.728953\n",
            "Train Epoch: 10 [500/1000 (50%)]\tLoss: 0.438089\n",
            "Epoch: 10\n",
            "Training loss: 0.6741444870829583\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 0.434699\n",
            "Train Epoch: 11 [500/1000 (50%)]\tLoss: 0.446312\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 0.676031\n",
            "Train Epoch: 12 [500/1000 (50%)]\tLoss: 0.564368\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 0.458839\n",
            "Train Epoch: 13 [500/1000 (50%)]\tLoss: 0.386250\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 0.400752\n",
            "Train Epoch: 14 [500/1000 (50%)]\tLoss: 0.468274\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 0.398188\n",
            "Train Epoch: 15 [500/1000 (50%)]\tLoss: 0.385887\n",
            "Epoch: 15\n",
            "Training loss: 0.4562099680304527\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 0.529629\n",
            "Train Epoch: 16 [500/1000 (50%)]\tLoss: 0.460862\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 0.459397\n",
            "Train Epoch: 17 [500/1000 (50%)]\tLoss: 0.499116\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 0.214070\n",
            "Train Epoch: 18 [500/1000 (50%)]\tLoss: 0.343356\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 0.231610\n",
            "Train Epoch: 19 [500/1000 (50%)]\tLoss: 0.221907\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 0.303558\n",
            "Train Epoch: 20 [500/1000 (50%)]\tLoss: 0.383208\n",
            "Epoch: 20\n",
            "Training loss: 0.30024983659386634\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 0.184367\n",
            "Train Epoch: 21 [500/1000 (50%)]\tLoss: 0.513676\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 0.544782\n",
            "Train Epoch: 22 [500/1000 (50%)]\tLoss: 0.489672\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 0.268003\n",
            "Train Epoch: 23 [500/1000 (50%)]\tLoss: 0.361804\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 0.147302\n",
            "Train Epoch: 24 [500/1000 (50%)]\tLoss: 0.360672\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 0.193871\n",
            "Train Epoch: 25 [500/1000 (50%)]\tLoss: 0.224769\n",
            "Epoch: 25\n",
            "Training loss: 0.272781203687191\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 0.287635\n",
            "Train Epoch: 26 [500/1000 (50%)]\tLoss: 0.161695\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 0.245203\n",
            "Train Epoch: 27 [500/1000 (50%)]\tLoss: 0.242552\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 0.170799\n",
            "Train Epoch: 28 [500/1000 (50%)]\tLoss: 0.308984\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 0.303591\n",
            "Train Epoch: 29 [500/1000 (50%)]\tLoss: 0.342039\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 0.189937\n",
            "Train Epoch: 30 [500/1000 (50%)]\tLoss: 0.127629\n",
            "Epoch: 30\n",
            "Training loss: 0.2096752915531397\n",
            "Saving final model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "t3eR94YX4zTq",
        "outputId": "c79410bc-7bc9-428b-eec2-9ba753b0f69c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "text = ''\n",
        "train_loss = []\n",
        "for line in text.splitlines():\n",
        "  if 'Loss' in line or 'loss' in line:\n",
        "    train_loss.append(float(line.split()[-1]))\n",
        "plt.plot(range(0,96,5), train_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training loss')\n",
        "plt.title('Model free direct style transfer training loss over epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcZZ3v8c+3t8rSDQnpAgNJCEtQQUUwAipXGVRA9F5wxlG4KozDiI643WFmRK/3uvKaccZtvC4zOCDgqMi4IjAig4g6yhIEAwGRGJYkBNIhe0KW7v7dP56n05VOVXV1p6sr3fV9v17Vfc5ztucsdX71nHOe5ygiMDMzGwstjc6AmZlNHg4qZmY2ZhxUzMxszDiomJnZmHFQMTOzMeOgYmZmY8ZBZZQkzZcUktpqGPfPJP2yyvDXS1ouabOk48Y2p8Pm7RRJK0r6l0g6ZTzzMNbyfjmy0fkYqUYeB5VIerOkn4z1uKPIx6OSXlWPeTezod//sdAUQSUfkDskdQ9JvyefgOY3Jme7fBp4d0R0RsQ9jcxIRBwTET8b6/nuzUlhvE4owwX/cTCmx4GkKyV9cm/mERHfiIjTxnpcm7yaIqhkjwDnDvRIej4wrXHZ2c2hwJJyA2opCY2XfSkvjSKptY6zr3gcDGc0+8b7s3Em9baPiEn/AR4FPgzcVZL2aeB/AwHMz2n7A1cDPcBjeZqWPKw1T7MGWAZclKdtK5n2cmAVsBL4JNCah/0Z8Msy+SoAm/N8tgB/KMnvB4DFwHagDTgJ+BWwHvgtcErJfCouu8wypwJXAuuAB4C/AVYM2Vavyt0fBb4D/BuwEfiL4ZYFvB14ENiU53888HWgH3gmr+/flslXN3B9Xr+1wC9IP3r2mBa4AXjPkOkXA6/P3QEcWbKNPw08DjwF/DMwtczynwtsA/ryctbn9CuBrwA35n30KuC1wD15mywHPloyn/l5+efnZa4B/nfJ8BOARXnap4DPVjkODga+SzoeHwHeWzKfPfbNkPW5ENgJ7Mjz/lGVY+sS4A8l++z1JfP5M0qO3ZzHdwIP5331JUCjGLcV+EzePo8A76bk+1ThO/yqkn36eeCJ/Pk8UKh2HOVhHyAds5uAh4BXVlhW2fNAXu564Hkl4xZJx+aBuf91wL15vF8BLxiyDrtt+zLLfg5wc877Q8AbS4ZdSTp+b87rcBtwaMnwlwJ3ARvy/5eWDDsA+FreXuuAH+T0U4AVwMXAatL3+m0l052Zj4lNedv99bDn20af8MfjM3BA5p303HxAryD9MiwNKlcDPwS6SCeH3wMX5GHvBH4HzM076FZ2DyrfB/4FmA4cCNwJvKPcl61M/nadBEvye29e1lTgEODpvINbgFfn/uJwyy6zrL8nfdEOyPO/n+pBZSdwdl7u1GHW80/zgfdiQMCRAwd96Xwr5OvvSF+Y9vz5bwyegHabFngjcEdJ/7F5e3QM3Z7A54Dr8vp2AT8C/q5CHvbYT6Qv8gbgZXkbTCF9EZ+f+19ACg5n5/Hn5+V/NW+vY0knkOfm4b8G3pq7O4GTyh0Hed53A/8X6AAOJ/2YOb3SvimzPlcCnyzzXdh1bJXst4PzfN5ECmyzy22TnMfrgRnAPNKJ94xRjPtO0slqDjAT+E9qDyofB24nHX9F0sn7E9WOI+DZpB8AB5fspyMqLKvaeeAK4NKScS8Cfpy7jyOdmE8knWPOz/kuVNr2Q5Y7PefxbaRgfxwp6B5dsj83AS8nBbh/GtjepON7HfDWPO25uX9WHn4D8O28rduBV+T0U4DevE3bSeeYrcDMPHwV8N9y90zg+GHPt/U+oe8LHwaDyofzQXcGKdq35QN5fj4IdgzswDzdO4Cf5e6fAu8sGXbawJcAOIh04phaMvxc4NZKJ6sh+SsXVP68pP8DwNeHTHNTPmirLrvMspaRv9i5/0KqB5Wflwwbbj1vAt5XbR9U2QYfJ32RjxxuWtKJfR2wIPd/Gvjy0O1JOplsoeTkAbwEeKRCHvbYT6Qv8tXDHF+fBz6Xu+fn5c8pGX4ncE7u/jnwMaC72nFAOjE9PmT4B4Gvlds3FfJ1JeWDyp8PM929wFnltknO48kl/dcCl4xi3J9S8sOH9P2sNaj8ATizZNjpwKPVjqN8PKzOy2mvsu7DnQdeRS5J5v7/As7L3V8hB7eS4Q8xeAKvuu1JAf0XQ9L+BfhIyf68pmRYJ6lkPZcUTO4cMu2v8z6ZTSrtzyyzzFNIJa22krTV5B87pNL2O4D9qh0zpZ9muqcC6VLK/yRt6KuHDOsmRerHStIeI5USIP2SWz5k2IBD87SrJK2XtJ50MBy4F3ktXdahwJ8OzDvP/2TSwTLSZVdbj1ryUW1Zc0lf+NH4R2Ap8BNJyyRdUmnEiNhG+tX1FkktpMD29TKjFkn3ze4uye+Pc/pIlG4DJJ0o6VZJPZI2kH51dw+Z5smS7q2kEwDABcBRwO8k3SXpdRWWeShw8JB9/iFSYC+br71Yn/Mk3VuynOeVWZ9SldZtJOMOPQ5Hsi4Hs+f39ODcXfY4ioilwPtJwXi1pGskHcyehjsP3ApMy8fAfOCFpNI7pH128ZB9Nrckb8Ot56HAiUOmfzPwrHLTR8Rm0mWyg8tsk9J8zwXWRsS6Cst9OiJ6S/pL99OfkEovj0m6TdJLquQfSL+ym0ZEPCbpEdJGumDI4DWkywmHkorlkIrsK3P3KtLOoWTYgOWkX/DdQ3bOXmV3yPy/HhFvHzqSpNkjXPbAegzcEJ5XZdxy+ai2rOXAETXMZ8+BEZtI13UvlvQ84KeS7oqIWypMexUpkPwS2BoRvy4zzhrSr7BjImJlmeG15nFo+jeBLwKviYhtkj5P9ZPw4IwiHgbOzcHwj4HvSJoVEVuGjLqcVKJaMIr8Djd8V7qkQ0mX6l4J/Doi+iTdSyrl1dMq0qWvAXMrjVjGE+z+UMO8nFb1OIqIbwLflLQf6cfQp0i/8EtVPQ/k7XMt6YfMU8D1eZmQ9tmlEXFplbxX22fLgdsi4tVVxtm1nSR1ki57DdxbOnTIuPNIP6KWAwdImhER66vMe8/MRtwFnCWpnXTf61qG2VfNVlKBFExOHfoljog+0ga7VFJX/rL9FelGKHnYeyXNkTSTdHNzYNpVwE+Az0jaT1KLpCMkvWKM8vxvwH+XdLqkVklT8vPlc0ax7GuBD0qaKWkO8J5aM1HDsv4V+GtJL1JyZN6OkL6Ah1eat6TX5fFFuofRRyqyl502B5F+0s3ecqUUIqKfdML8nKQD83IOkXR6hWw8BcyR1FFtO5Cuta/NAeUEUum3JpLeIqmY8zbwBe8vM+qdwCZJH5A0Ne/350l6ca3LYphtnk0nneh6cv7eRiqp1Nu1wPvy/phBusRbq28BH5ZUVKom8H/J39NKx5GkZ0s6VVKB9EDGM5TZ7jWcByD9qHgTqRTxzZL0rwLvzKUYSZou6bWSumpcr+uBoyS9VVJ7/rxY0nNLxjlT0sn5GP0EcHtELCc9SHKUpP8pqU3Sm4CjSUFvFfAfwJfz975d0suHy4ykDqW6R/tHxE7SAyHljtXdNF1QiYg/RMSiCoPfQ7oGv4z0C/ibpBtzkA6Ym0hPXv0G+N6Qac8j3VB9gHS9/zuky1NjkeflwFmkyx89pF8ef8Pg/hvJsj9GKhY/QgoQZU/IVVRcVkT8O3ApabttAn5A+iUF6V7Wh3Ox/q/LzHcB6WbtZtK14C9HxK3DTHs16Yb5v1HZB0iXQ26XtDEv49kVxv0p6dfvk5LWVJnnu4CPS9pEOqFdW2Xcoc4AlkjaTLrRek5EPDN0pHxyex3p8sojpF/Q/0p6MqlWlwNH5+32g3IjRMQDpMD8a1IQej7pPkG9fZV0/C0mPUl3I+mGcV8N036S9ATdYuA+0vdxoD5OpeOoQHpIZQ3pktyBpHtU5VQ7DxARd+ThB5NO1gPpi0hPP36R9N1YSrrUXpNc4jkNOIdU8niSVJoqlIz2TeAjpMteLwLekqd9mnS8XEx6aOVvgddFxMBx/FZSCex3pHsm768xW28FHs3fnXeSAmlVA0/XmE04ks4DLoyIkxudF9s7kl4D/HNEDL2EY5mkK0kP1Xy40XmppulKKjY5SJpGKjFc1ui82MjlS3pn5ks1h5B+fX9/uOls3+egYhNOvifSQ7pc881hRrd9k0iXYteRLn89SLqUaBOcL3+ZmdmYcUnFzMzGTFPVUwHo7u6O+fPnNzobZmYTyt13370mIoatONx0QWX+/PksWlTpiWIzMytH0nCtbwC+/GVmZmOobkEl1/q+U9Jvld4m+LGcfpikOyQtlfTtgdrLkgq5f2kePr9kXh/M6Q+V1oaWdEZOW6oqbUWZmdn4qGdJZTupOZRjSbWCz5B0EqmG6Oci4kjS44QDbXBdAKzL6Z/L4yHpaFIN02NItZG/nJusaCW9n+E1pOYIzs3jmplZg9QtqESyOfcOvNsggFNJTXtAahTw7Nx9Vu4nD39lbr/nLFJzz9sj4hFS0wcn5M/SiFgWETuAa/K4ZmbWIHW9p5JLFPeS2pq5mdQs+vqSFm5XMNik9CHkZp3z8A3ArNL0IdNUSjczswapa1CJiL6IeCGpiesTSK/KHHeSLpS0SNKinp6eRmTBzKwpjMvTX7kN/1tJb92bIWngUeY5DL6vZCW5nf48fH9Sa5u70odMUym93PIvi4iFEbGwWBzp+5nMzKxW9Xz6q5jfk4CkqaT3qj9ICi5vyKOdT3r1J6T3iJ+fu98A/DRSGzLXAefkp8MOIzVtfSdwF7AgP03WQbqZf1291ueqXz3Kj377RL1mb2Y2KdSz8uNs4Kr8lFYLcG1EXC/pAeAaSZ8kNSR3eR7/cuDrkpaS3hVwDkBELFF609oDpPctXJTfNYGkd5PecdIKXBERS6iTa+5aziEzpvLfjy33BlIzM4M6BpWIWAwcVyZ9Gen+ytD0bcCfVpjXpaSXPw1Nv5H0cp+6K3YV6Nm8fTwWZWY2YblGfY2KnQXWbHJQMTOrxkGlRsWuAj2btuNXBZiZVeagUqNiV4Edff1seGZno7NiZrbPclCpUbGrAECPL4GZmVXkoFKjYqeDipnZcBxUarSrpOInwMzMKnJQqZEvf5mZDc9BpUb7TWmjo63FQcXMrAoHlRpJothZcFAxM6vCQWUEXKvezKw6B5URGKgAaWZm5TmojICDiplZdQ4qI3BgV4G1W3ews6+/0VkxM9snOaiMQLGrQASs3bKj0VkxM9snOaiMgGvVm5lV56AyAq4AaWZWnYPKCDiomJlV56AyAt2dbv/LzKwaB5URmNLeyn5T2lxSMTOrwEFlhFxXxcysMgeVEXJQMTOrzEFlhIpdU3xPxcysAgeVEXJLxWZmlTmojFCxq8Dm7b1s3dHb6KyYme1zHFRGaKCuyppNbqrFzGwoB5URGggqqzdta3BOzMz2PQ4qI+T2v8zMKqtbUJE0V9Ktkh6QtETS+3L6RyWtlHRv/pxZMs0HJS2V9JCk00vSz8hpSyVdUpJ+mKQ7cvq3JXXUa30G7GqqxU+AmZntoZ4llV7g4og4GjgJuEjS0XnY5yLihflzI0Aedg5wDHAG8GVJrZJagS8BrwGOBs4tmc+n8ryOBNYBF9RxfQA4YHoHLXJJxcysnLoFlYhYFRG/yd2bgAeBQ6pMchZwTURsj4hHgKXACfmzNCKWRcQO4BrgLEkCTgW+k6e/Cji7PmszqLVFzPJjxWZmZY3LPRVJ84HjgDty0rslLZZ0haSZOe0QYHnJZCtyWqX0WcD6iOgdkl5u+RdKWiRpUU9Pz16vj+uqmJmVV/egIqkT+C7w/ojYCHwFOAJ4IbAK+Ey98xARl0XEwohYWCwW93p+xa6C76mYmZVR16AiqZ0UUL4REd8DiIinIqIvIvqBr5IubwGsBOaWTD4np1VKfxqYIaltSHrduf0vM7Py6vn0l4DLgQcj4rMl6bNLRns9cH/uvg44R1JB0mHAAuBO4C5gQX7Sq4N0M/+6iAjgVuANefrzgR/Wa31KFbsKrNm8nf7+GI/FmZlNGG3DjzJqLwPeCtwn6d6c9iHS01svBAJ4FHgHQEQskXQt8ADpybGLIqIPQNK7gZuAVuCKiFiS5/cB4BpJnwTuIQWxuit2FtjZF2x4Ziczp9f9KWYzswmjbkElIn4JqMygG6tMcylwaZn0G8tNFxHLGLx8Nm4O3G+wroqDipnZINeoHwXXqjczK89BZRR21ap3UDEz242Dyig4qJiZleegMgqdhTamtLe4roqZ2RAOKqMgyXVVzMzKcFAZJTfVYma2JweVUXJJxcxsTw4qo+T2v8zM9uSgMkrFzims3bKDnX39jc6Kmdk+w0FllAYeK356844G58TMbN/hoDJKrqtiZrYnB5VRGnxX/bYG58TMbN/hoDJKA0Fl9UaXVMzMBjiojFJ3Z2qd2Je/zMwGOaiMUqGtlf2ntvuxYjOzEg4qe8EVIM3MduegshfcVIuZ2e4cVPaCa9Wbme3OQWUv+PKXmdnuHFT2QrGrwNYdfWzZ3tvorJiZ7RMcVPaC31VvZrY7B5W9MFir3kHFzAwcVPaK2/8yM9udg8peONBBxcxsNw4qe2HmtA5aW+SgYmaWOajshZYW0d3Z4aBiZpY5qOwlV4A0MxtUt6Aiaa6kWyU9IGmJpPfl9AMk3Szp4fx/Zk6XpC9IWippsaTjS+Z1fh7/YUnnl6S/SNJ9eZovSFK91qcSN9ViZjaoniWVXuDiiDgaOAm4SNLRwCXALRGxALgl9wO8BliQPxcCX4EUhICPACcCJwAfGQhEeZy3l0x3Rh3XpyzXqjczG1S3oBIRqyLiN7l7E/AgcAhwFnBVHu0q4OzcfRZwdSS3AzMkzQZOB26OiLURsQ64GTgjD9svIm6PiACuLpnXuCl2FVizeTv9/THeizYz2+eMyz0VSfOB44A7gIMiYlUe9CRwUO4+BFheMtmKnFYtfUWZ9HLLv1DSIkmLenp69mpdhip2FujtD9Y/s3NM52tmNhHVPahI6gS+C7w/IjaWDssljLr/xI+IyyJiYUQsLBaLYzrvYtcUwHVVzMyghqAi6WWSpufut0j6rKRDa5m5pHZSQPlGRHwvJz+VL12R/6/O6SuBuSWTz8lp1dLnlEkfV65Vb2Y2qJaSyleArZKOBS4G/kC6f1FVfhLrcuDBiPhsyaDrgIEnuM4HfliSfl5+CuwkYEO+THYTcJqkmfkG/WnATXnYRkkn5WWdVzKvcTPY/te28V60mdk+p62GcXojIiSdBXwxIi6XdEEN070MeCtwn6R7c9qHgL8Hrs3zeAx4Yx52I3AmsBTYCrwNICLWSvoEcFce7+MRsTZ3vwu4EpgK/Ef+jCuXVMzMBtUSVDZJ+iDwFuDlklqA9uEmiohfApXqjbyyzPgBXFRhXlcAV5RJXwQ8b7i81NP0jlamtreyeqODiplZLZe/3gRsBy6IiCdJ9y7+sa65mkAkuVa9mVlWU0kF+KeI6JN0FPAc4Fv1zdbE4gqQZmZJLSWVnwMFSYcAPyHdJ7mynpmaaNxUi5lZUktQUURsBf4Y+HJE/CkNvo+xr/HlLzOzpKagIuklwJuBG0YwXdModhVYv3Un23v7Gp0VM7OGqiU4vB/4IPD9iFgi6XDg1vpma2IZeKz46c07GpwTM7PGGvZGfUTcBtwmqVNSZ0QsA95b/6xNHMXOwboqB8+Y2uDcmJk1Ti3NtDxf0j3AEuABSXdLOqb+WZs4XAHSzCyp5fLXvwB/FRGHRsQ8UlMtX61vtiaWwaZaHFTMrLnVElSmR8SueygR8TNget1yNAF1d7qkYmYGtVV+XCbp/wBfz/1vAZbVL0sTT0dbCzOntTuomFnTq6Wk8udAEfhe/hRzmpVwrXozs9qe/lqHn/YalitAmplVCSqSfkSVtzJGxP+oS44mqGJngd88vr7R2TAza6hqJZVPj1suJoGBy18RQXpnmJlZ86kYVHKlR6tRsavAMzv72LKjj85CLc8/mJlNPm7Da4y4AqSZmYPKmCl2TgEcVMysuTmojBGXVMzManikuMJTYBuARcC/RMS2emRsohkMKt4cZta8aimpLAM2k9r7+iqwkfSK4aNwG2C7zJjaTluLXFfFzJpaLY8pvTQiXlzS/yNJd0XEiyUtqVfGJpqWFtHt1wqbWZOrpaTSKWneQE/u7sy9fitViWJXgdUOKmbWxGopqVwM/FLSHwABhwHvkjQduKqemZtoil0Fntroeypm1rxqafvrRkkLgOfkpIdKbs5/vm45m4CKnQXuX7mh0dkwM2uYWqt+vwiYn8c/VhIRcXXdcjVBFbsKPL1lB339QWuLm2oxs+ZTy+uEv05qB+xk4MX5s7CG6a6QtFrS/SVpH5W0UtK9+XNmybAPSloq6SFJp5ekn5HTlkq6pCT9MEl35PRvS+qoea3rpNhVoK8/WLfVt5rMrDnVUlJZCBwdERVbLK7gSuCLwNASzeciYrfGKiUdDZwDHAMcDPynpKPy4C8BrwZWAHdJui4iHgA+led1jaR/Bi4AvjLCPI6p0gqQA2+DNDNrJrU8/XU/8KyRzjgifg6srXH0s4BrImJ7RDwCLAVOyJ+lEbEsInYA1wBnKTUDfCrwnTz9VcDZI83jWHOtejNrdrWUVLqBByTdCew6W+7F+1TeLek8Uo38i/NLwA4Bbi8ZZ0VOA1g+JP1EYBawPiJ6y4y/B0kXAhcCzJs3r9Joe63od9WbWZOrJah8dAyX9xXgE6RmXz4BfIZxeDVxRFwGXAawcOHCkV7Gq9mukopr1ZtZk6rlkeIxe69KRDw10C3pq8D1uXclMLdk1Dk5jQrpTwMzJLXl0krp+A0zvdDGtI5Wl1TMrGlVvKci6Zf5/yZJG0s+myRtHM3CJM0u6X096X4NwHXAOZIKkg4DFgB3AncBC/KTXh2km/nX5YcGbgXekKc/H/jhaPI01gbeAGlm1oyqvfnx5Py/azQzlvQt4BSgW9IK4CPAKZJeSLr89SjwjryMJZKuBR4AeoGLIqIvz+fdwE1AK3BFRAy0N/YB4BpJnwTuAS4fTT7HWtHtf5lZE6up8qOkVuCg0vEj4vFq00TEuWWSK574I+JS4NIy6TcCN5ZJX0Z6OmyfcuB+BX7/1OZGZ8PMrCFqeZ/Ke0iljKeA/pwcwAvqmK8Jq9hZ4L+WPt3obJiZNUQtJZX3Ac+OCJ8pa1DsKrDhmZ1s7+2j0Nba6OyYmY2rWio/Lie96dFqMPBY8ZrNbqrFzJpPLSWVZcDPJN3A7pUfP1u3XE1gpbXqD5kxtcG5MTMbX7UElcfzpyN/rIpi5xTAterNrDnVUvnxY+ORkcnC7X+ZWTOrGFQkfT4i3i/pR6SnvXazF21/TWqzOlNhzkHFzJpRtZLK1/P/T1cZx4Zob23hgOkd9Gz2a4XNrPlUq1F/d/4/Zm1/NQvXqjezZlVL5ccFwN8BRwNTBtIj4vA65mtCc/tfZtasaqmn8jVSk/W9wB+R3uT4b/XM1ERX7Cqw2kHFzJpQLUFlakTcAigiHouIjwKvrW+2JraBksrI38BsZjax1VJPZbukFuDh3GLwSqCzvtma2IqdBbb39rNpey/7TWlvdHbMzMZNLSWV9wHTgPcCLwLeQnp/iVXguipm1qyqBpXc5P2bImJzRKyIiLdFxJ9ExO3Vpmt2Dipm1qyqvfmxLb8o6+RxzM+k4KBiZs2q2j2VO4HjgXskXQf8O7BlYGBEfK/OeZuwip0OKmbWnGq5UT8FeBo4ldRci/J/B5UK9p/aTnur6NnsoGJmzaVaUDlQ0l8B9zMYTAb4WdkqWlpEt2vVm1kTqhZUWkmPDqvMMAeVYbhWvZk1o2pBZVVEfHzccjLJFDsLrNrgRiXNrLlUe6S4XAnFalTsKvieipk1nWpB5ZXjlotJ6MCuAk9v3k5fv68UmlnzqBhUImLteGZksil2FegPWLtlR6OzYmY2bmpppsVGwRUgzawZOajUya6g4vsqZtZEHFTqpNiZ3mfmkoqZNZO6BRVJV0haLen+krQDJN0s6eH8f2ZOl6QvSFoqabGk40umOT+P/7Ck80vSXyTpvjzNFyTtU0+rdXd1AA4qZtZc6llSuRI4Y0jaJcAtEbEAuCX3A7wGWJA/F5LeNImkA4CPACcCJwAfGQhEeZy3l0w3dFkNNa2jjc5Cm4OKmTWVugWViPg5MPQJsrOAq3L3VcDZJelXR3I7MEPSbOB04OaIWBsR64CbgTPysP0i4vZIr1e8umRe+wzXVTGzZjPe91QOiohVuftJ4KDcfQiwvGS8FTmtWvqKMullSbpQ0iJJi3p6evZuDUag2FmgZ5Nr1ZtZ82jYjfpcwhiXmoERcVlELIyIhcVicTwWCbj9LzNrPuMdVJ7Kl67I/1fn9JXA3JLx5uS0aulzyqTvUxxUzKzZjHdQuY7B99ufD/ywJP28/BTYScCGfJnsJuA0STPzDfrTgJvysI2STspPfZ1XMq99RrGrwMZtvWzb2dforJiZjYtaXtI1KpK+BZwCdEtaQXqK6++BayVdADwGvDGPfiNwJrAU2Aq8DVJTMZI+AdyVx/t4SfMx7yI9YTYV+I/82aeUvgFy7gHTGpwbM7P6q1tQiYhzKwzao6HKfH/logrzuQK4okz6IuB5e5PHeiutVe+gYmbNwDXq68jtf5lZs3FQqSMHFTNrNg4qdXTA9A4kBxUzax4OKnXU3trCAdM6XKvezJqGg0qdua6KmTUTB5U6c1Axs2bioFJnqf0vBxUzaw4OKnU20FJxqopjZja5OajUWbGrwI7efjZu6210VszM6s5Bpc5cV8XMmomDSp05qJhZM3FQqbMDS9r/MjOb7BxU6qzYOQVwScXMmoODSp3tN7WNjtYWBxUzawoOKnUmyRUgzaxpOKiMg+5cV8XMbLJzUBkHrlVvZs3CQWUc+PKXmTULB5VxUOwqsHbLdvr63VSLmU1uDirjoNhVoD/g6S0urZjZ5OagMg6KnakC5OqNDipmNrk5qIyDomvVm1mTcFAZBwe6/S8zaxIOKuOgu9NBxcyag4PKOJja0UpXoc1BxcwmPQeVcVLcr8Btv+/hlw+v8VsgzWzSakhQkfSopPsk3StpUU47QNLNkvMkaHgAAA1gSURBVB7O/2fmdEn6gqSlkhZLOr5kPufn8R+WdH4j1qVWHzjjOWzd0ctbLr+Ds7/8K/7zgaccXMxs0mlkSeWPIuKFEbEw918C3BIRC4Bbcj/Aa4AF+XMh8BVIQQj4CHAicALwkYFAtC86/Zhn8fO//SMuff3zWLtlO39x9SJe80+/4Ee/fcKVIs1s0tiXLn+dBVyVu68Czi5JvzqS24EZkmYDpwM3R8TaiFgH3AycMd6ZHolCWytvPvFQbr34FD77xmPZ2dfPe751D6/+7G38+6Ll7Ozrb3QWzcz2SqOCSgA/kXS3pAtz2kERsSp3PwkclLsPAZaXTLsip1VK34OkCyUtkrSop6dnrNZh1NpaW/jj4+dw8/96BV9+8/FMaW/lb76zmFP+8Wd8/fbH2Lazr9FZNDMblUYFlZMj4njSpa2LJL28dGCkmw1jdk0oIi6LiIURsbBYLI7VbPdaS4s48/mzueG9J/O1P3sxB+1X4P/84H5e/g+38tWfL2PL9t5GZ9HMbEQaElQiYmX+vxr4PumeyFP5shb5/+o8+kpgbsnkc3JapfQJRxJ/9JwD+e5fvpRvvv1EFhzUyaU3PsjJn/op/++Wh9nwzM5GZ9HMrCbjHlQkTZfUNdANnAbcD1wHDDzBdT7ww9x9HXBefgrsJGBDvkx2E3CapJn5Bv1pOW3CksRLj+jmG39xEt9710s5ft5MPnPz7zn573/KP/z4d6zetK3RWTQzq0rj/VirpMNJpROANuCbEXGppFnAtcA84DHgjRGxVpKAL5Juwm8F3hYRA48h/znwoTyvSyPia8Mtf+HChbFo0aIxXad6euCJjXzpZ0u58b5VCDjxsFm87tjZnHHMs5iVa+qbmdWbpLtLntatPF6z1ZWYaEFlwLKezfzg3ie4fvETLOvZQmuLeMnhs3jtC1KAmTm9o9FZNLNJzEGlgokaVAZEBL97chM3LF7F9Yuf4NGnt9LaIl52ZDeve8FsTj/6Wew/rb3R2TSzScZBpYKJHlRKRQRLntjIDfet4obFq3h87VbaW8XJR3bz2hcczKuPPoj9pzrAmNnec1CpYDIFlVIRwX0rN+QSzCpWrn+GjtYWXn5UN699wWxe9dyD6JriAGNmo+OgUsFkDSqlIoJ7l6/nhsWruOG+VazasI2OthZOPrKbVxxV5BVHFZnfPb3R2TSzCcRBpYJmCCql+vuDe5av4/rFq7j1d6t59OmtABw6a9quAHPS4bOYXmhrcE7NbF/moFJBswWVoR5ds4WfP9zDbQ/18Ks/PM0zO/voaG3hxYfNzEHmQI46qJP0JLeZWeKgUkGzB5VS23v7WPToOm77fQoyDz21CYBn7TeFlx/VzSuOOpCTj+z202Rm5qBSiYNKZas2PMPPf9/Dbb/v4RcPr2HTtl5aBMfNS6WYlx4xi+cdsj9T2lsbnVUzG2cOKhU4qNSmt6+fe5ev3xVkFq/cQAS0t4qjZ+/HcfNmcty8GRw3dyZzD5jqy2Vmk5yDSgUOKqPz9Obt3P3YOu5Zvp57Hl/Hb5dv4JncRP+s6R0pwMybyXFzZ/CCuTPo9I1/s0ml1qDib77VZFZngdOOeRanHfMsIJVkfv/UZu5Zvo57Hk+B5j8fTA1LS/Dsg7p2lWSOmzeDI4qdtLS4NGM22bmkYmNmw9ad3LsiBZiBQLNxW3onTNeUNp47ez+OKE7nsO7pHNbdyeHF6cydOY2Otn3pBaRmVo5LKjbu9p/WvqvuC6Q6Mo88vYV7Hl/Pbx5fx++f3MRNS55i7ZYdu6ZpbRFzZ07dLdAc3j2dw4rTedZ+U3yvxmyCcVCxumlpEUcUOzmi2MkbXjRnV/r6rTt4ZM2WXZ9la7bwSM8Wbl+2dtd9GoCp7a3M705B5vDidObMnEqxq0CxcwrFrgKzOjtob3Upx2xf4qBi427GtA6Om9fBcfNm7pbe3x88tWkbj/TkQJM/S57YwI+XPElf/56XamdOa0+BpqtAsbNAd2dhsL9rsP+AaR2+p2M2DhxUbJ/R0iJm7z+V2ftP5aVHdu82bEdvP6s3baNn03bWbN5Bz6bt6bN5MO03j69n9aZtbNvZv8e8W1vEjKntdE5po7PQRteUNjoL7fl/2670/aYMdLfvGm9gnOmFNjpaWxyczKpwULEJoaOthTkzpzFn5rSq40UEW3b05UCzfVfwWb1pG+u37mTz9l42b+tl0/ZeVq5/hs3bd6b+bb30likJVcpLoa2FKe2tu/5PaW+h0Fb5f6G9hUJrC+2tLbS1ttDeKtpaRHtbC+0tLbS1ivZd6S05XbvGbW9tobVFtEi0KL16ukXkfiGloDyQppJhlcYvN47Z3nJQsUlFUip5FNo4bAQtMUcE23v72bSttyTw7Ez9OW3Ljl627+xnW28f23f2sz3/39bbx7bcv21nPxue2blb//adfWzr7WdH754lqH3NYKAZDERi7IKO8h+RgqBI8931Pw8byMdA+kDe9jYLaY6D8xmY3a5l7JbRPYeXGvrk7B4/SWLP3v4IIiAI+vsH59M/kBaQZjvQHWm6Mj94SrfLkGzvkeeBrv+65NS6t4jhoGJG+gKmEkcrxa5CXZYREfT1Bzv7gp39/fT2BTv7+tnZV9od9Pan/7vS+/vZ2dtPX3/sOjENnHD6I52cBk5WA8NS/2B3f6QT08CJa9fJbcg4MWQ+A8tIaXt3Uh84mQ6ci0tPpmnY4PJjyEk2iDJn7REuv2Q/7N5ffTi7hseuoLRL9d49glHLQEDN0XMwYA+WMClJGy6glwa22C29pLtkSOs4XLp1UDEbJ5JoaxVtrTAVt59mk5OfxzQzszHjoGJmZmPGQcXMzMaMg4qZmY0ZBxUzMxszDipmZjZmHFTMzGzMOKiYmdmYabqXdEnqAR4b5eTdwJoxzM5E4/X3+nv9m9ehEVEcbqSmCyp7Q9KiWt58Nll5/b3+Xv/mXf9a+fKXmZmNGQcVMzMbMw4qI3NZozPQYF7/5ub1t2H5noqZmY0Zl1TMzGzMOKiYmdmYcVCpgaQzJD0kaamkSxqdn3qTNFfSrZIekLRE0vty+gGSbpb0cP4/s9F5rSdJrZLukXR97j9M0h35OPi2pI5G57GeJM2Q9B1Jv5P0oKSXNNMxIOl/5eP/fknfkjSl2Y6B0XBQGYakVuBLwGuAo4FzJR3d2FzVXS9wcUQcDZwEXJTX+RLglohYANyS+yez9wEPlvR/CvhcRBwJrAMuaEiuxs8/AT+OiOcAx5K2RVMcA5IOAd4LLIyI5wGtwDk03zEwYg4qwzsBWBoRyyJiB3ANcFaD81RXEbEqIn6TuzeRTiaHkNb7qjzaVcDZjclh/UmaA7wW+NfcL+BU4Dt5lMm+/vsDLwcuB4iIHRGxniY6BkivW58qqQ2YBqyiiY6B0XJQGd4hwPKS/hU5rSlImg8cB9wBHBQRq/KgJ4GDGpSt8fB54G+B/tw/C1gfEb25f7IfB4cBPcDX8iXAf5U0nSY5BiJiJfBp4HFSMNkA3E1zHQOj4qBiFUnqBL4LvD8iNpYOi/Qs+qR8Hl3S64DVEXF3o/PSQG3A8cBXIuI4YAtDLnVN8mNgJqlUdhhwMDAdOKOhmZogHFSGtxKYW9I/J6dNapLaSQHlGxHxvZz8lKTZefhsYHWj8ldnLwP+h6RHSZc7TyXdX5iRL4XA5D8OVgArIuKO3P8dUpBplmPgVcAjEdETETuB75GOi2Y6BkbFQWV4dwEL8lMfHaSbddc1OE91le8fXA48GBGfLRl0HXB+7j4f+OF45208RMQHI2JORMwn7e+fRsSbgVuBN+TRJu36A0TEk8BySc/OSa8EHqBJjgHSZa+TJE3L34eB9W+aY2C0XKO+BpLOJF1jbwWuiIhLG5ylupJ0MvAL4D4G7yl8iHRf5VpgHun1AW+MiLUNyeQ4kXQK8NcR8TpJh5NKLgcA9wBviYjtjcxfPUl6IelBhQ5gGfA20g/RpjgGJH0MeBPpach7gL8g3UNpmmNgNBxUzMxszPjyl5mZjRkHFTMzGzMOKmZmNmYcVMzMbMw4qJiZ2ZhxUDEbY5L6JN1b8hmzRhclzZd0/1jNz2ystQ0/ipmN0DMR8cJGZ8KsEVxSMRsnkh6V9A+S7pN0p6Qjc/p8ST+VtFjSLZLm5fSDJH1f0m/z56V5Vq2Svprf9fETSVMbtlJmQziomI29qUMuf72pZNiGiHg+8EVSKw0A/w+4KiJeAHwD+EJO/wJwW0QcS2p3a0lOXwB8KSKOAdYDf1Ln9TGrmWvUm40xSZsjorNM+qPAqRGxLDfY+WREzJK0BpgdETtz+qqI6JbUA8wpbQYkv4rg5vySLCR9AGiPiE/Wf83MhueSitn4igrdI1Ha1lQfvjdq+xAHFbPx9aaS/7/O3b8itYYM8GZSY56QXtf7l5Bea53fxmi2T/MvHLOxN1XSvSX9P46IgceKZ0paTCptnJvT3kN6w+LfkN62+Lac/j7gMkkXkEokf0l6C6HZPsv3VMzGSb6nsjAi1jQ6L2b14stfZmY2ZlxSMTOzMeOSipmZjRkHFTMzGzMOKmZmNmYcVMzMbMw4qJiZ2Zj5/3XlxoegiW3AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "0e08jD4RfT_w",
        "outputId": "a3705f1f-a395-47f5-b361-65b95b69da91"
      },
      "source": [
        "t = transforms.Compose([transforms.ToPILImage(), transforms.Resize(128), transforms.ToTensor()])\n",
        "data_train = MusicDataset('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/spectrograms', folders, t)\n",
        "imgplot = plt.imshow(torch.transpose(torch.transpose(data_train[0][0], 0,2), 0, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNCxjtbIMddE"
      },
      "source": [
        "# Interpolater\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRKn36tsJWR"
      },
      "source": [
        "# Raw Data Loading\n",
        "from scipy.io.wavfile import read, write\n",
        "import pandas as pd\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, rate):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.rate = rate\n",
        "        self.repeats = 50\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of songs\n",
        "         return self.repeats *(len(self.data) // self.rate)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        idx = self.rate * (idx // self.repeats)\n",
        "        data = np.random.choice(self.data[idx:idx + rate], size=500, replace=False)\n",
        "        label = self.data[idx:idx + rate]\n",
        "        return data.copy() / 32768, label.copy() / 32768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOw0IKt7Mv6p"
      },
      "source": [
        "# Raw audio fully connected network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MusicInterpolateNet(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(MusicInterpolateNet, self).__init__()\n",
        "        # input 500\n",
        "        self.fc1 = nn.Linear(500, 250)\n",
        "        self.fc2 = nn.Linear(250, 100)\n",
        "        self.fc3 = nn.Linear(100, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label):\n",
        "        loss_val = F.mse_loss(prediction, label)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfYco2azW0iW"
      },
      "source": [
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        data = data.float()\n",
        "        label = label.float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            correct += torch.sum(torch.argmax(output, dim=1) == label)\n",
        "            \n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kryGNGCwW6f4"
      },
      "source": [
        "# Training, Testing\n",
        "def main():\n",
        "    BATCH_SIZE = 50#128#256\n",
        "    FEATURE_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 256\n",
        "    EPOCHS = 50\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/'\n",
        "\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/'\n",
        "\n",
        "    t = transforms.Compose([transforms.ToTensor()])\n",
        "    rate, audio = read(path + 'stay.wav')\n",
        "    audio = audio[:,0]\n",
        "    data_train = MusicDataset(audio, rate)\n",
        "    print(data_train[0][0].shape)\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    #test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "    #                                          shuffle=False, **kwargs)\n",
        "\n",
        "    model = MusicInterpolateNet(rate).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    # start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "    try:\n",
        "        for epoch in range(EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            #test_loss, test_accuracy = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            #test_losses.append((epoch, test_loss))\n",
        "            #test_accuracies.append((epoch, test_accuracy))\n",
        "            torch.save(model, DATA_PATH + 'interpolate.pt')\n",
        "            if epoch % 5 == 0:\n",
        "              print('Epoch:', epoch)\n",
        "              print('Training loss:', train_loss)\n",
        "            \n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        torch.save(model, DATA_PATH + 'interpolate.pt')\n",
        "        return model, device\n",
        "final_model, device = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha6hsKbga-4Q"
      },
      "source": [
        "# Interpolate between audio using the interpolation network\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/'\n",
        "rate, audio = read(path + 'stay.wav')\n",
        "audio = audio[:,0]\n",
        "model = torch.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/interpolate.pt').to(device)\n",
        "pred = torch.tensor([]).to(device)\n",
        "for i in range(len(audio) // rate):\n",
        "  data = torch.from_numpy(np.random.choice(audio[i * rate:i * rate + rate], size=500, replace=False))\n",
        "  data = data.unsqueeze(0).to(device).float()\n",
        "  output = model(data)\n",
        "  pred = torch.cat((pred, output.flatten()), dim=0)\n",
        "\n",
        "pred = pred.cpu().detach().numpy() * 32768\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/output/'\n",
        "write(path + 'stay_interpolate.wav', rate, pred.astype('int16'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KmaCthfJtLO"
      },
      "source": [
        "# Extra stuff: Random MIDI generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM4sq4kIcYxG"
      },
      "source": [
        "from music21 import converter, instrument, note, chord\n",
        "import glob\n",
        "notes = []\n",
        "for file in glob.glob('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/*.mid'):\n",
        "    midi = converter.parse(file)\n",
        "    notes_to_parse = None\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "    if parts: # file has instrument parts\n",
        "        notes_to_parse = parts.parts[0].recurse()\n",
        "    else: # file has notes in a flat structure\n",
        "        notes_to_parse = midi.flat.notes\n",
        "    for element in notes_to_parse:\n",
        "        if isinstance(element, note.Note):\n",
        "            notes.append(str(element.pitch))\n",
        "        elif isinstance(element, chord.Chord):\n",
        "            notes.append('.'.join(str(n) for n in element.normalOrder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mVsgh-Ohqig",
        "outputId": "d77d7b5f-e160-44e5-eee3-d3b58e524ca2"
      },
      "source": [
        "!pip install mido\n",
        "import mido\n",
        "mid = mido.MidiFile('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/Pirates.mid', clip=True)\n",
        "mid.tracks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mido in /usr/local/lib/python3.6/dist-packages (1.2.9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<midi track \"He's A Pirate\" 9 messages>,\n",
              " <midi track 'Right Hand' 1285 messages>,\n",
              " <midi track 'Left Hand' 1557 messages>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paVqXfXXrT06"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "def msg2dict(msg):\n",
        "    result = dict()\n",
        "    if 'note_on' in msg:\n",
        "        on_ = True\n",
        "    elif 'note_off' in msg:\n",
        "        on_ = False\n",
        "    else:\n",
        "        on_ = None\n",
        "    result['time'] = int(msg[msg.rfind('time'):].split(' ')[0].split('=')[1].translate(\n",
        "        str.maketrans({a: None for a in string.punctuation})))\n",
        "\n",
        "    if on_ is not None:\n",
        "        for k in ['note', 'velocity']:\n",
        "            result[k] = int(msg[msg.rfind(k):].split(' ')[0].split('=')[1].translate(\n",
        "                str.maketrans({a: None for a in string.punctuation})))\n",
        "    return [result, on_]\n",
        "def switch_note(last_state, note, velocity, on_=True):\n",
        "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of this range will be ignored\n",
        "    result = [0] * 88 if last_state is None else last_state.copy()\n",
        "    if 21 <= note <= 108:\n",
        "        result[note-21] = velocity if on_ else 0\n",
        "    return result\n",
        "def get_new_state(new_msg, last_state):\n",
        "    new_msg, on_ = msg2dict(str(new_msg))\n",
        "    new_state = switch_note(last_state, note=new_msg['note'], velocity=new_msg['velocity'], on_=on_) if on_ is not None else last_state\n",
        "    return [new_state, new_msg['time']]\n",
        "def track2seq(track):\n",
        "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of the id range will be ignored\n",
        "    result = []\n",
        "    last_state, last_time = get_new_state(str(track[0]), [0]*88)\n",
        "    for i in range(1, len(track)):\n",
        "        new_state, new_time = get_new_state(track[i], last_state)\n",
        "        if new_time > 0:\n",
        "            result += [last_state]*new_time\n",
        "        last_state, last_time = new_state, new_time\n",
        "    return result\n",
        "def mid2arry(mid, min_msg_pct=0.1):\n",
        "    tracks_len = [len(tr) for tr in mid.tracks]\n",
        "    min_n_msg = max(tracks_len) * min_msg_pct\n",
        "    # convert each track to nested list\n",
        "    all_arys = []\n",
        "    for i in range(len(mid.tracks)):\n",
        "        if len(mid.tracks[i]) > min_n_msg:\n",
        "            ary_i = track2seq(mid.tracks[i])\n",
        "            all_arys.append(ary_i)\n",
        "    # make all nested list the same length\n",
        "    max_len = max([len(ary) for ary in all_arys])\n",
        "    for i in range(len(all_arys)):\n",
        "        if len(all_arys[i]) < max_len:\n",
        "            all_arys[i] += [[0] * 88] * (max_len - len(all_arys[i]))\n",
        "    all_arys = np.array(all_arys)\n",
        "    all_arys = all_arys.max(axis=0)\n",
        "    # trim: remove consecutive 0s in the beginning and at the end\n",
        "    sums = all_arys.sum(axis=1)\n",
        "    ends = np.where(sums > 0)[0]\n",
        "    return all_arys[min(ends): max(ends)]\n",
        "def arry2mid(ary, tempo=500000):\n",
        "    # get the difference\n",
        "    new_ary = np.concatenate([np.array([[0] * 88]), np.array(ary)], axis=0)\n",
        "    changes = new_ary[1:] - new_ary[:-1]\n",
        "    # create a midi file with an empty track\n",
        "    mid_new = mido.MidiFile()\n",
        "    track = mido.MidiTrack()\n",
        "    mid_new.tracks.append(track)\n",
        "    track.append(mido.MetaMessage('set_tempo', tempo=tempo, time=0))\n",
        "    # add difference in the empty track\n",
        "    last_time = 0\n",
        "    for ch in changes:\n",
        "        if set(ch) == {0}:  # no change\n",
        "            last_time += 1\n",
        "        else:\n",
        "            on_notes = np.where(ch > 0)[0]\n",
        "            on_notes_vol = ch[on_notes]\n",
        "            off_notes = np.where(ch < 0)[0]\n",
        "            first_ = True\n",
        "            for n, v in zip(on_notes, on_notes_vol):\n",
        "                new_time = last_time if first_ else 0\n",
        "                track.append(mido.Message('note_on', note=n + 21, velocity=v, time=new_time))\n",
        "                first_ = False\n",
        "            for n in off_notes:\n",
        "                new_time = last_time if first_ else 0\n",
        "                track.append(mido.Message('note_off', note=n + 21, velocity=0, time=new_time))\n",
        "                first_ = False\n",
        "            last_time = 0\n",
        "    return mid_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZjryD54r8vO"
      },
      "source": [
        "import torch\n",
        "class MusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio):\n",
        "        super(MusicDataset, self).__init__()\n",
        "        self.audio = audio\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of songs\n",
        "         return len(self.audio) - 5\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        data = self.audio[idx:idx + 5]\n",
        "        return data.copy(), self.audio[idx + 1].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M5yoOoztzEV"
      },
      "source": [
        "# Raw audio fully connected network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MidiNet(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(MidiNet, self).__init__()\n",
        "        # input 500\n",
        "        self.lstm = nn.LSTM(input_size=88, hidden_size=88, batch_first=True)\n",
        "        self.fc = nn.Linear(88 * 5, 88)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, hidden = self.lstm(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label):\n",
        "        loss_val = F.mse_loss(prediction, label)\n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeMKf2Zzuk8s"
      },
      "source": [
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        data = data.float()\n",
        "        label = label.float()\n",
        "        #print(data.shape)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            correct += torch.sum(torch.argmax(output, dim=1) == label)\n",
        "            \n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N5RqZx4utv4",
        "outputId": "cebb3574-d6c6-4f3d-e9b0-17dcf9ea2966"
      },
      "source": [
        "# Training, Testing\n",
        "def main():\n",
        "    BATCH_SIZE = 50#128#256\n",
        "    FEATURE_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 256\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = False\n",
        "    PRINT_INTERVAL = 10\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/'\n",
        "\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/data/midi/'\n",
        "\n",
        "    audio = mid2arry(mid)\n",
        "    data_train = MusicDataset(audio)\n",
        "    print(data_train[0][0].shape)\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    #test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "    #                                          shuffle=False, **kwargs)\n",
        "\n",
        "    model = MidiNet(88).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    try:\n",
        "        for epoch in range(EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            #test_loss, test_accuracy = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            #test_losses.append((epoch, test_loss))\n",
        "            #test_accuracies.append((epoch, test_accuracy))\n",
        "            torch.save(model, DATA_PATH + 'midi.pt')\n",
        "            if epoch % 5 == 0:\n",
        "              print('Epoch:', epoch)\n",
        "              print('Training loss:', train_loss)\n",
        "            \n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        torch.save(model, DATA_PATH + 'midi.pt')\n",
        "        return model, device\n",
        "final_model, device = main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 88)\n",
            "Using device cpu\n",
            "num workers: 2\n",
            "Train Epoch: 0 [0/49674 (0%)]\tLoss: 456.572601\n",
            "Train Epoch: 0 [500/49674 (1%)]\tLoss: 481.123871\n",
            "Train Epoch: 0 [1000/49674 (2%)]\tLoss: 390.540955\n",
            "Train Epoch: 0 [1500/49674 (3%)]\tLoss: 395.439514\n",
            "Train Epoch: 0 [2000/49674 (4%)]\tLoss: 330.397491\n",
            "Train Epoch: 0 [2500/49674 (5%)]\tLoss: 301.162598\n",
            "Train Epoch: 0 [3000/49674 (6%)]\tLoss: 291.853790\n",
            "Train Epoch: 0 [3500/49674 (7%)]\tLoss: 252.044922\n",
            "Train Epoch: 0 [4000/49674 (8%)]\tLoss: 249.183411\n",
            "Train Epoch: 0 [4500/49674 (9%)]\tLoss: 173.539581\n",
            "Train Epoch: 0 [5000/49674 (10%)]\tLoss: 187.119537\n",
            "Train Epoch: 0 [5500/49674 (11%)]\tLoss: 195.175888\n",
            "Train Epoch: 0 [6000/49674 (12%)]\tLoss: 159.003311\n",
            "Train Epoch: 0 [6500/49674 (13%)]\tLoss: 167.463821\n",
            "Train Epoch: 0 [7000/49674 (14%)]\tLoss: 147.693054\n",
            "Train Epoch: 0 [7500/49674 (15%)]\tLoss: 147.250763\n",
            "Train Epoch: 0 [8000/49674 (16%)]\tLoss: 121.766296\n",
            "Train Epoch: 0 [8500/49674 (17%)]\tLoss: 133.418243\n",
            "Train Epoch: 0 [9000/49674 (18%)]\tLoss: 113.596138\n",
            "Train Epoch: 0 [9500/49674 (19%)]\tLoss: 118.779434\n",
            "Train Epoch: 0 [10000/49674 (20%)]\tLoss: 127.771431\n",
            "Train Epoch: 0 [10500/49674 (21%)]\tLoss: 112.002045\n",
            "Train Epoch: 0 [11000/49674 (22%)]\tLoss: 109.172081\n",
            "Train Epoch: 0 [11500/49674 (23%)]\tLoss: 89.261772\n",
            "Train Epoch: 0 [12000/49674 (24%)]\tLoss: 103.990387\n",
            "Train Epoch: 0 [12500/49674 (25%)]\tLoss: 99.691612\n",
            "Train Epoch: 0 [13000/49674 (26%)]\tLoss: 93.845139\n",
            "Train Epoch: 0 [13500/49674 (27%)]\tLoss: 91.707428\n",
            "Train Epoch: 0 [14000/49674 (28%)]\tLoss: 85.922874\n",
            "Train Epoch: 0 [14500/49674 (29%)]\tLoss: 74.343956\n",
            "Train Epoch: 0 [15000/49674 (30%)]\tLoss: 70.978073\n",
            "Train Epoch: 0 [15500/49674 (31%)]\tLoss: 68.382988\n",
            "Train Epoch: 0 [16000/49674 (32%)]\tLoss: 76.182259\n",
            "Train Epoch: 0 [16500/49674 (33%)]\tLoss: 70.891434\n",
            "Train Epoch: 0 [17000/49674 (34%)]\tLoss: 73.073906\n",
            "Train Epoch: 0 [17500/49674 (35%)]\tLoss: 57.519535\n",
            "Train Epoch: 0 [18000/49674 (36%)]\tLoss: 59.841045\n",
            "Train Epoch: 0 [18500/49674 (37%)]\tLoss: 63.309944\n",
            "Train Epoch: 0 [19000/49674 (38%)]\tLoss: 63.149128\n",
            "Train Epoch: 0 [19500/49674 (39%)]\tLoss: 57.726517\n",
            "Train Epoch: 0 [20000/49674 (40%)]\tLoss: 55.431049\n",
            "Train Epoch: 0 [20500/49674 (41%)]\tLoss: 56.745060\n",
            "Train Epoch: 0 [21000/49674 (42%)]\tLoss: 49.659630\n",
            "Train Epoch: 0 [21500/49674 (43%)]\tLoss: 49.010799\n",
            "Train Epoch: 0 [22000/49674 (44%)]\tLoss: 48.898411\n",
            "Train Epoch: 0 [22500/49674 (45%)]\tLoss: 40.065655\n",
            "Train Epoch: 0 [23000/49674 (46%)]\tLoss: 33.698772\n",
            "Train Epoch: 0 [23500/49674 (47%)]\tLoss: 45.385826\n",
            "Train Epoch: 0 [24000/49674 (48%)]\tLoss: 40.154194\n",
            "Train Epoch: 0 [24500/49674 (49%)]\tLoss: 36.950703\n",
            "Train Epoch: 0 [25000/49674 (50%)]\tLoss: 32.310139\n",
            "Train Epoch: 0 [25500/49674 (51%)]\tLoss: 40.606628\n",
            "Train Epoch: 0 [26000/49674 (52%)]\tLoss: 31.775572\n",
            "Train Epoch: 0 [26500/49674 (53%)]\tLoss: 28.127968\n",
            "Train Epoch: 0 [27000/49674 (54%)]\tLoss: 31.262930\n",
            "Train Epoch: 0 [27500/49674 (55%)]\tLoss: 32.598095\n",
            "Train Epoch: 0 [28000/49674 (56%)]\tLoss: 29.474386\n",
            "Train Epoch: 0 [28500/49674 (57%)]\tLoss: 31.865807\n",
            "Train Epoch: 0 [29000/49674 (58%)]\tLoss: 34.149792\n",
            "Train Epoch: 0 [29500/49674 (59%)]\tLoss: 29.664537\n",
            "Train Epoch: 0 [30000/49674 (60%)]\tLoss: 26.480431\n",
            "Train Epoch: 0 [30500/49674 (61%)]\tLoss: 30.577780\n",
            "Train Epoch: 0 [31000/49674 (62%)]\tLoss: 31.519392\n",
            "Train Epoch: 0 [31500/49674 (63%)]\tLoss: 30.775049\n",
            "Train Epoch: 0 [32000/49674 (64%)]\tLoss: 31.412386\n",
            "Train Epoch: 0 [32500/49674 (65%)]\tLoss: 26.323744\n",
            "Train Epoch: 0 [33000/49674 (66%)]\tLoss: 24.045588\n",
            "Train Epoch: 0 [33500/49674 (67%)]\tLoss: 25.127159\n",
            "Train Epoch: 0 [34000/49674 (68%)]\tLoss: 26.905300\n",
            "Train Epoch: 0 [34500/49674 (69%)]\tLoss: 24.231201\n",
            "Train Epoch: 0 [35000/49674 (70%)]\tLoss: 20.487118\n",
            "Train Epoch: 0 [35500/49674 (71%)]\tLoss: 21.025404\n",
            "Train Epoch: 0 [36000/49674 (72%)]\tLoss: 25.670176\n",
            "Train Epoch: 0 [36500/49674 (73%)]\tLoss: 22.020098\n",
            "Train Epoch: 0 [37000/49674 (74%)]\tLoss: 19.896624\n",
            "Train Epoch: 0 [37500/49674 (75%)]\tLoss: 20.098963\n",
            "Train Epoch: 0 [38000/49674 (76%)]\tLoss: 18.709541\n",
            "Train Epoch: 0 [38500/49674 (77%)]\tLoss: 18.441206\n",
            "Train Epoch: 0 [39000/49674 (78%)]\tLoss: 24.299417\n",
            "Train Epoch: 0 [39500/49674 (79%)]\tLoss: 15.932133\n",
            "Train Epoch: 0 [40000/49674 (80%)]\tLoss: 19.938272\n",
            "Train Epoch: 0 [40500/49674 (81%)]\tLoss: 16.971136\n",
            "Train Epoch: 0 [41000/49674 (82%)]\tLoss: 24.816034\n",
            "Train Epoch: 0 [41500/49674 (84%)]\tLoss: 13.470590\n",
            "Train Epoch: 0 [42000/49674 (85%)]\tLoss: 20.420746\n",
            "Train Epoch: 0 [42500/49674 (86%)]\tLoss: 22.247847\n",
            "Train Epoch: 0 [43000/49674 (87%)]\tLoss: 18.301828\n",
            "Train Epoch: 0 [43500/49674 (88%)]\tLoss: 21.656027\n",
            "Train Epoch: 0 [44000/49674 (89%)]\tLoss: 16.322376\n",
            "Train Epoch: 0 [44500/49674 (90%)]\tLoss: 16.636993\n",
            "Train Epoch: 0 [45000/49674 (91%)]\tLoss: 11.941948\n",
            "Train Epoch: 0 [45500/49674 (92%)]\tLoss: 8.421144\n",
            "Train Epoch: 0 [46000/49674 (93%)]\tLoss: 17.988449\n",
            "Train Epoch: 0 [46500/49674 (94%)]\tLoss: 13.576355\n",
            "Train Epoch: 0 [47000/49674 (95%)]\tLoss: 12.906281\n",
            "Train Epoch: 0 [47500/49674 (96%)]\tLoss: 11.122180\n",
            "Train Epoch: 0 [48000/49674 (97%)]\tLoss: 17.327291\n",
            "Train Epoch: 0 [48500/49674 (98%)]\tLoss: 17.711834\n",
            "Train Epoch: 0 [49000/49674 (99%)]\tLoss: 12.225323\n",
            "Train Epoch: 0 [49500/49674 (100%)]\tLoss: 11.711488\n",
            "Epoch: 0\n",
            "Training loss: 78.88117429668036\n",
            "Train Epoch: 1 [0/49674 (0%)]\tLoss: 12.639508\n",
            "Train Epoch: 1 [500/49674 (1%)]\tLoss: 11.079206\n",
            "Train Epoch: 1 [1000/49674 (2%)]\tLoss: 7.993398\n",
            "Train Epoch: 1 [1500/49674 (3%)]\tLoss: 11.130784\n",
            "Train Epoch: 1 [2000/49674 (4%)]\tLoss: 14.868396\n",
            "Train Epoch: 1 [2500/49674 (5%)]\tLoss: 14.768649\n",
            "Train Epoch: 1 [3000/49674 (6%)]\tLoss: 11.407784\n",
            "Train Epoch: 1 [3500/49674 (7%)]\tLoss: 13.404280\n",
            "Train Epoch: 1 [4000/49674 (8%)]\tLoss: 11.243965\n",
            "Train Epoch: 1 [4500/49674 (9%)]\tLoss: 7.519445\n",
            "Train Epoch: 1 [5000/49674 (10%)]\tLoss: 14.148672\n",
            "Train Epoch: 1 [5500/49674 (11%)]\tLoss: 10.882736\n",
            "Train Epoch: 1 [6000/49674 (12%)]\tLoss: 8.079250\n",
            "Train Epoch: 1 [6500/49674 (13%)]\tLoss: 8.137225\n",
            "Train Epoch: 1 [7000/49674 (14%)]\tLoss: 12.266379\n",
            "Train Epoch: 1 [7500/49674 (15%)]\tLoss: 15.913020\n",
            "Train Epoch: 1 [8000/49674 (16%)]\tLoss: 10.614446\n",
            "Train Epoch: 1 [8500/49674 (17%)]\tLoss: 14.049213\n",
            "Train Epoch: 1 [9000/49674 (18%)]\tLoss: 13.155258\n",
            "Train Epoch: 1 [9500/49674 (19%)]\tLoss: 10.176927\n",
            "Train Epoch: 1 [10000/49674 (20%)]\tLoss: 7.464244\n",
            "Train Epoch: 1 [10500/49674 (21%)]\tLoss: 5.582662\n",
            "Train Epoch: 1 [11000/49674 (22%)]\tLoss: 10.461938\n",
            "Train Epoch: 1 [11500/49674 (23%)]\tLoss: 11.082252\n",
            "Train Epoch: 1 [12000/49674 (24%)]\tLoss: 7.663571\n",
            "Train Epoch: 1 [12500/49674 (25%)]\tLoss: 10.133138\n",
            "Train Epoch: 1 [13000/49674 (26%)]\tLoss: 10.548372\n",
            "Train Epoch: 1 [13500/49674 (27%)]\tLoss: 7.097038\n",
            "Train Epoch: 1 [14000/49674 (28%)]\tLoss: 6.546021\n",
            "Train Epoch: 1 [14500/49674 (29%)]\tLoss: 6.047131\n",
            "Train Epoch: 1 [15000/49674 (30%)]\tLoss: 9.383789\n",
            "Train Epoch: 1 [15500/49674 (31%)]\tLoss: 11.154087\n",
            "Train Epoch: 1 [16000/49674 (32%)]\tLoss: 8.211915\n",
            "Train Epoch: 1 [16500/49674 (33%)]\tLoss: 11.024791\n",
            "Train Epoch: 1 [17000/49674 (34%)]\tLoss: 8.819031\n",
            "Train Epoch: 1 [17500/49674 (35%)]\tLoss: 9.941134\n",
            "Train Epoch: 1 [18000/49674 (36%)]\tLoss: 8.375402\n",
            "Train Epoch: 1 [18500/49674 (37%)]\tLoss: 7.596021\n",
            "Train Epoch: 1 [19000/49674 (38%)]\tLoss: 13.283315\n",
            "Train Epoch: 1 [19500/49674 (39%)]\tLoss: 7.491433\n",
            "Train Epoch: 1 [20000/49674 (40%)]\tLoss: 6.678555\n",
            "Train Epoch: 1 [20500/49674 (41%)]\tLoss: 12.130915\n",
            "Train Epoch: 1 [21000/49674 (42%)]\tLoss: 6.007390\n",
            "Train Epoch: 1 [21500/49674 (43%)]\tLoss: 9.836496\n",
            "Train Epoch: 1 [22000/49674 (44%)]\tLoss: 5.951490\n",
            "Train Epoch: 1 [22500/49674 (45%)]\tLoss: 5.819468\n",
            "Train Epoch: 1 [23000/49674 (46%)]\tLoss: 6.159913\n",
            "Train Epoch: 1 [23500/49674 (47%)]\tLoss: 13.842382\n",
            "Train Epoch: 1 [24000/49674 (48%)]\tLoss: 8.683923\n",
            "Train Epoch: 1 [24500/49674 (49%)]\tLoss: 6.669365\n",
            "Train Epoch: 1 [25000/49674 (50%)]\tLoss: 9.840940\n",
            "Train Epoch: 1 [25500/49674 (51%)]\tLoss: 6.912455\n",
            "Train Epoch: 1 [26000/49674 (52%)]\tLoss: 19.812502\n",
            "Train Epoch: 1 [26500/49674 (53%)]\tLoss: 6.780182\n",
            "Train Epoch: 1 [27000/49674 (54%)]\tLoss: 6.620819\n",
            "Train Epoch: 1 [27500/49674 (55%)]\tLoss: 7.168278\n",
            "Train Epoch: 1 [28000/49674 (56%)]\tLoss: 8.345652\n",
            "Train Epoch: 1 [28500/49674 (57%)]\tLoss: 6.185342\n",
            "Train Epoch: 1 [29000/49674 (58%)]\tLoss: 7.335490\n",
            "Train Epoch: 1 [29500/49674 (59%)]\tLoss: 8.595101\n",
            "Train Epoch: 1 [30000/49674 (60%)]\tLoss: 6.942333\n",
            "Train Epoch: 1 [30500/49674 (61%)]\tLoss: 8.128879\n",
            "Train Epoch: 1 [31000/49674 (62%)]\tLoss: 4.943471\n",
            "Train Epoch: 1 [31500/49674 (63%)]\tLoss: 8.177669\n",
            "Train Epoch: 1 [32000/49674 (64%)]\tLoss: 5.075271\n",
            "Train Epoch: 1 [32500/49674 (65%)]\tLoss: 8.199691\n",
            "Train Epoch: 1 [33000/49674 (66%)]\tLoss: 5.757622\n",
            "Train Epoch: 1 [33500/49674 (67%)]\tLoss: 5.675577\n",
            "Train Epoch: 1 [34000/49674 (68%)]\tLoss: 7.275645\n",
            "Train Epoch: 1 [34500/49674 (69%)]\tLoss: 12.947083\n",
            "Train Epoch: 1 [35000/49674 (70%)]\tLoss: 9.616321\n",
            "Train Epoch: 1 [35500/49674 (71%)]\tLoss: 9.892749\n",
            "Train Epoch: 1 [36000/49674 (72%)]\tLoss: 5.593352\n",
            "Train Epoch: 1 [36500/49674 (73%)]\tLoss: 7.044851\n",
            "Train Epoch: 1 [37000/49674 (74%)]\tLoss: 3.233800\n",
            "Train Epoch: 1 [37500/49674 (75%)]\tLoss: 10.590700\n",
            "Train Epoch: 1 [38000/49674 (76%)]\tLoss: 3.570131\n",
            "Train Epoch: 1 [38500/49674 (77%)]\tLoss: 5.743301\n",
            "Train Epoch: 1 [39000/49674 (78%)]\tLoss: 5.753100\n",
            "Train Epoch: 1 [39500/49674 (79%)]\tLoss: 4.330534\n",
            "Train Epoch: 1 [40000/49674 (80%)]\tLoss: 3.675713\n",
            "Train Epoch: 1 [40500/49674 (81%)]\tLoss: 3.219012\n",
            "Train Epoch: 1 [41000/49674 (82%)]\tLoss: 5.858530\n",
            "Train Epoch: 1 [41500/49674 (84%)]\tLoss: 7.821545\n",
            "Train Epoch: 1 [42000/49674 (85%)]\tLoss: 4.834332\n",
            "Train Epoch: 1 [42500/49674 (86%)]\tLoss: 16.876326\n",
            "Train Epoch: 1 [43000/49674 (87%)]\tLoss: 6.618513\n",
            "Train Epoch: 1 [43500/49674 (88%)]\tLoss: 10.376815\n",
            "Train Epoch: 1 [44000/49674 (89%)]\tLoss: 5.009426\n",
            "Train Epoch: 1 [44500/49674 (90%)]\tLoss: 3.495268\n",
            "Train Epoch: 1 [45000/49674 (91%)]\tLoss: 6.036161\n",
            "Train Epoch: 1 [45500/49674 (92%)]\tLoss: 4.602670\n",
            "Train Epoch: 1 [46000/49674 (93%)]\tLoss: 5.776938\n",
            "Train Epoch: 1 [46500/49674 (94%)]\tLoss: 4.397210\n",
            "Train Epoch: 1 [47000/49674 (95%)]\tLoss: 4.066768\n",
            "Train Epoch: 1 [47500/49674 (96%)]\tLoss: 5.650509\n",
            "Train Epoch: 1 [48000/49674 (97%)]\tLoss: 6.126269\n",
            "Train Epoch: 1 [48500/49674 (98%)]\tLoss: 3.484070\n",
            "Train Epoch: 1 [49000/49674 (99%)]\tLoss: 5.091033\n",
            "Train Epoch: 1 [49500/49674 (100%)]\tLoss: 10.363473\n",
            "Train Epoch: 2 [0/49674 (0%)]\tLoss: 5.448049\n",
            "Train Epoch: 2 [500/49674 (1%)]\tLoss: 5.992674\n",
            "Train Epoch: 2 [1000/49674 (2%)]\tLoss: 8.900002\n",
            "Train Epoch: 2 [1500/49674 (3%)]\tLoss: 5.814775\n",
            "Train Epoch: 2 [2000/49674 (4%)]\tLoss: 3.556202\n",
            "Train Epoch: 2 [2500/49674 (5%)]\tLoss: 3.718031\n",
            "Train Epoch: 2 [3000/49674 (6%)]\tLoss: 4.866150\n",
            "Train Epoch: 2 [3500/49674 (7%)]\tLoss: 6.455503\n",
            "Train Epoch: 2 [4000/49674 (8%)]\tLoss: 6.259817\n",
            "Train Epoch: 2 [4500/49674 (9%)]\tLoss: 7.591194\n",
            "Train Epoch: 2 [5000/49674 (10%)]\tLoss: 5.434742\n",
            "Train Epoch: 2 [5500/49674 (11%)]\tLoss: 3.288486\n",
            "Train Epoch: 2 [6000/49674 (12%)]\tLoss: 8.243476\n",
            "Train Epoch: 2 [6500/49674 (13%)]\tLoss: 5.642158\n",
            "Train Epoch: 2 [7000/49674 (14%)]\tLoss: 4.735312\n",
            "Train Epoch: 2 [7500/49674 (15%)]\tLoss: 6.300777\n",
            "Train Epoch: 2 [8000/49674 (16%)]\tLoss: 8.455255\n",
            "Train Epoch: 2 [8500/49674 (17%)]\tLoss: 4.326602\n",
            "Train Epoch: 2 [9000/49674 (18%)]\tLoss: 4.632581\n",
            "Train Epoch: 2 [9500/49674 (19%)]\tLoss: 5.608463\n",
            "Train Epoch: 2 [10000/49674 (20%)]\tLoss: 4.687875\n",
            "Train Epoch: 2 [10500/49674 (21%)]\tLoss: 6.919662\n",
            "Train Epoch: 2 [11000/49674 (22%)]\tLoss: 3.617661\n",
            "Train Epoch: 2 [11500/49674 (23%)]\tLoss: 5.641205\n",
            "Train Epoch: 2 [12000/49674 (24%)]\tLoss: 4.739786\n",
            "Train Epoch: 2 [12500/49674 (25%)]\tLoss: 4.903747\n",
            "Train Epoch: 2 [13000/49674 (26%)]\tLoss: 5.591281\n",
            "Train Epoch: 2 [13500/49674 (27%)]\tLoss: 5.235601\n",
            "Train Epoch: 2 [14000/49674 (28%)]\tLoss: 3.401028\n",
            "Train Epoch: 2 [14500/49674 (29%)]\tLoss: 4.582493\n",
            "Train Epoch: 2 [15000/49674 (30%)]\tLoss: 3.874418\n",
            "Train Epoch: 2 [15500/49674 (31%)]\tLoss: 4.871029\n",
            "Train Epoch: 2 [16000/49674 (32%)]\tLoss: 3.313506\n",
            "Train Epoch: 2 [16500/49674 (33%)]\tLoss: 6.173980\n",
            "Train Epoch: 2 [17000/49674 (34%)]\tLoss: 7.274246\n",
            "Train Epoch: 2 [17500/49674 (35%)]\tLoss: 4.842148\n",
            "Train Epoch: 2 [18000/49674 (36%)]\tLoss: 4.774798\n",
            "Train Epoch: 2 [18500/49674 (37%)]\tLoss: 6.752598\n",
            "Train Epoch: 2 [19000/49674 (38%)]\tLoss: 2.603967\n",
            "Train Epoch: 2 [19500/49674 (39%)]\tLoss: 7.257980\n",
            "Train Epoch: 2 [20000/49674 (40%)]\tLoss: 3.997611\n",
            "Train Epoch: 2 [20500/49674 (41%)]\tLoss: 2.545997\n",
            "Train Epoch: 2 [21000/49674 (42%)]\tLoss: 5.396703\n",
            "Train Epoch: 2 [21500/49674 (43%)]\tLoss: 2.327463\n",
            "Train Epoch: 2 [22000/49674 (44%)]\tLoss: 4.896620\n",
            "Train Epoch: 2 [22500/49674 (45%)]\tLoss: 3.896158\n",
            "Train Epoch: 2 [23000/49674 (46%)]\tLoss: 2.714108\n",
            "Train Epoch: 2 [23500/49674 (47%)]\tLoss: 5.616872\n",
            "Train Epoch: 2 [24000/49674 (48%)]\tLoss: 6.966163\n",
            "Train Epoch: 2 [24500/49674 (49%)]\tLoss: 7.886306\n",
            "Train Epoch: 2 [25000/49674 (50%)]\tLoss: 5.013494\n",
            "Train Epoch: 2 [25500/49674 (51%)]\tLoss: 8.458964\n",
            "Train Epoch: 2 [26000/49674 (52%)]\tLoss: 7.932056\n",
            "Train Epoch: 2 [26500/49674 (53%)]\tLoss: 3.507793\n",
            "Train Epoch: 2 [27000/49674 (54%)]\tLoss: 3.939126\n",
            "Train Epoch: 2 [27500/49674 (55%)]\tLoss: 5.746181\n",
            "Train Epoch: 2 [28000/49674 (56%)]\tLoss: 6.318152\n",
            "Train Epoch: 2 [28500/49674 (57%)]\tLoss: 13.650165\n",
            "Train Epoch: 2 [29000/49674 (58%)]\tLoss: 6.163736\n",
            "Train Epoch: 2 [29500/49674 (59%)]\tLoss: 3.413496\n",
            "Train Epoch: 2 [30000/49674 (60%)]\tLoss: 2.623047\n",
            "Train Epoch: 2 [30500/49674 (61%)]\tLoss: 2.561203\n",
            "Train Epoch: 2 [31000/49674 (62%)]\tLoss: 2.992038\n",
            "Train Epoch: 2 [31500/49674 (63%)]\tLoss: 3.900221\n",
            "Train Epoch: 2 [32000/49674 (64%)]\tLoss: 2.869093\n",
            "Train Epoch: 2 [32500/49674 (65%)]\tLoss: 4.242732\n",
            "Train Epoch: 2 [33000/49674 (66%)]\tLoss: 5.931873\n",
            "Train Epoch: 2 [33500/49674 (67%)]\tLoss: 4.094988\n",
            "Train Epoch: 2 [34000/49674 (68%)]\tLoss: 4.489898\n",
            "Train Epoch: 2 [34500/49674 (69%)]\tLoss: 3.980429\n",
            "Train Epoch: 2 [35000/49674 (70%)]\tLoss: 3.049132\n",
            "Train Epoch: 2 [35500/49674 (71%)]\tLoss: 2.441180\n",
            "Train Epoch: 2 [36000/49674 (72%)]\tLoss: 4.851360\n",
            "Train Epoch: 2 [36500/49674 (73%)]\tLoss: 3.982722\n",
            "Train Epoch: 2 [37000/49674 (74%)]\tLoss: 1.908551\n",
            "Train Epoch: 2 [37500/49674 (75%)]\tLoss: 3.351117\n",
            "Train Epoch: 2 [38000/49674 (76%)]\tLoss: 4.795187\n",
            "Train Epoch: 2 [38500/49674 (77%)]\tLoss: 5.195850\n",
            "Train Epoch: 2 [39000/49674 (78%)]\tLoss: 3.850550\n",
            "Train Epoch: 2 [39500/49674 (79%)]\tLoss: 2.911727\n",
            "Train Epoch: 2 [40000/49674 (80%)]\tLoss: 2.733967\n",
            "Train Epoch: 2 [40500/49674 (81%)]\tLoss: 4.586667\n",
            "Train Epoch: 2 [41000/49674 (82%)]\tLoss: 2.024652\n",
            "Train Epoch: 2 [41500/49674 (84%)]\tLoss: 4.428872\n",
            "Train Epoch: 2 [42000/49674 (85%)]\tLoss: 8.867102\n",
            "Train Epoch: 2 [42500/49674 (86%)]\tLoss: 4.394822\n",
            "Train Epoch: 2 [43000/49674 (87%)]\tLoss: 2.956315\n",
            "Train Epoch: 2 [43500/49674 (88%)]\tLoss: 1.316146\n",
            "Train Epoch: 2 [44000/49674 (89%)]\tLoss: 3.242944\n",
            "Train Epoch: 2 [44500/49674 (90%)]\tLoss: 2.134801\n",
            "Train Epoch: 2 [45000/49674 (91%)]\tLoss: 4.338546\n",
            "Train Epoch: 2 [45500/49674 (92%)]\tLoss: 6.443210\n",
            "Train Epoch: 2 [46000/49674 (93%)]\tLoss: 2.785018\n",
            "Train Epoch: 2 [46500/49674 (94%)]\tLoss: 3.308036\n",
            "Train Epoch: 2 [47000/49674 (95%)]\tLoss: 4.499999\n",
            "Train Epoch: 2 [47500/49674 (96%)]\tLoss: 5.615506\n",
            "Train Epoch: 2 [48000/49674 (97%)]\tLoss: 6.586449\n",
            "Train Epoch: 2 [48500/49674 (98%)]\tLoss: 1.673579\n",
            "Train Epoch: 2 [49000/49674 (99%)]\tLoss: 2.134604\n",
            "Train Epoch: 2 [49500/49674 (100%)]\tLoss: 2.393371\n",
            "Train Epoch: 3 [0/49674 (0%)]\tLoss: 6.588753\n",
            "Train Epoch: 3 [500/49674 (1%)]\tLoss: 1.999934\n",
            "Train Epoch: 3 [1000/49674 (2%)]\tLoss: 3.332842\n",
            "Train Epoch: 3 [1500/49674 (3%)]\tLoss: 5.267951\n",
            "Train Epoch: 3 [2000/49674 (4%)]\tLoss: 3.194034\n",
            "Train Epoch: 3 [2500/49674 (5%)]\tLoss: 3.282129\n",
            "Train Epoch: 3 [3000/49674 (6%)]\tLoss: 3.963971\n",
            "Train Epoch: 3 [3500/49674 (7%)]\tLoss: 6.848200\n",
            "Train Epoch: 3 [4000/49674 (8%)]\tLoss: 3.264955\n",
            "Train Epoch: 3 [4500/49674 (9%)]\tLoss: 2.721233\n",
            "Train Epoch: 3 [5000/49674 (10%)]\tLoss: 1.740849\n",
            "Train Epoch: 3 [5500/49674 (11%)]\tLoss: 3.851104\n",
            "Train Epoch: 3 [6000/49674 (12%)]\tLoss: 3.393227\n",
            "Train Epoch: 3 [6500/49674 (13%)]\tLoss: 3.238111\n",
            "Train Epoch: 3 [7000/49674 (14%)]\tLoss: 3.842396\n",
            "Train Epoch: 3 [7500/49674 (15%)]\tLoss: 5.531205\n",
            "Train Epoch: 3 [8000/49674 (16%)]\tLoss: 2.298291\n",
            "Train Epoch: 3 [8500/49674 (17%)]\tLoss: 3.280206\n",
            "Train Epoch: 3 [9000/49674 (18%)]\tLoss: 4.590027\n",
            "Train Epoch: 3 [9500/49674 (19%)]\tLoss: 1.518456\n",
            "Train Epoch: 3 [10000/49674 (20%)]\tLoss: 1.846846\n",
            "Train Epoch: 3 [10500/49674 (21%)]\tLoss: 2.637647\n",
            "Train Epoch: 3 [11000/49674 (22%)]\tLoss: 5.189948\n",
            "Train Epoch: 3 [11500/49674 (23%)]\tLoss: 3.595497\n",
            "Train Epoch: 3 [12000/49674 (24%)]\tLoss: 2.870459\n",
            "Train Epoch: 3 [12500/49674 (25%)]\tLoss: 2.098032\n",
            "Train Epoch: 3 [13000/49674 (26%)]\tLoss: 2.946053\n",
            "Train Epoch: 3 [13500/49674 (27%)]\tLoss: 2.379965\n",
            "Train Epoch: 3 [14000/49674 (28%)]\tLoss: 3.618049\n",
            "Train Epoch: 3 [14500/49674 (29%)]\tLoss: 4.692993\n",
            "Train Epoch: 3 [15000/49674 (30%)]\tLoss: 3.360061\n",
            "Train Epoch: 3 [15500/49674 (31%)]\tLoss: 1.631901\n",
            "Train Epoch: 3 [16000/49674 (32%)]\tLoss: 2.592817\n",
            "Train Epoch: 3 [16500/49674 (33%)]\tLoss: 2.717492\n",
            "Train Epoch: 3 [17000/49674 (34%)]\tLoss: 3.319105\n",
            "Train Epoch: 3 [17500/49674 (35%)]\tLoss: 3.639035\n",
            "Train Epoch: 3 [18000/49674 (36%)]\tLoss: 4.273484\n",
            "Train Epoch: 3 [18500/49674 (37%)]\tLoss: 6.544589\n",
            "Train Epoch: 3 [19000/49674 (38%)]\tLoss: 2.179701\n",
            "Train Epoch: 3 [19500/49674 (39%)]\tLoss: 3.873455\n",
            "Train Epoch: 3 [20000/49674 (40%)]\tLoss: 2.729002\n",
            "Train Epoch: 3 [20500/49674 (41%)]\tLoss: 4.193395\n",
            "Train Epoch: 3 [21000/49674 (42%)]\tLoss: 2.904746\n",
            "Train Epoch: 3 [21500/49674 (43%)]\tLoss: 4.659822\n",
            "Train Epoch: 3 [22000/49674 (44%)]\tLoss: 2.196063\n",
            "Train Epoch: 3 [22500/49674 (45%)]\tLoss: 2.784095\n",
            "Train Epoch: 3 [23000/49674 (46%)]\tLoss: 6.521214\n",
            "Train Epoch: 3 [23500/49674 (47%)]\tLoss: 2.826482\n",
            "Train Epoch: 3 [24000/49674 (48%)]\tLoss: 2.377992\n",
            "Train Epoch: 3 [24500/49674 (49%)]\tLoss: 2.596764\n",
            "Train Epoch: 3 [25000/49674 (50%)]\tLoss: 3.261345\n",
            "Train Epoch: 3 [25500/49674 (51%)]\tLoss: 1.653054\n",
            "Train Epoch: 3 [26000/49674 (52%)]\tLoss: 3.068640\n",
            "Train Epoch: 3 [26500/49674 (53%)]\tLoss: 2.621582\n",
            "Train Epoch: 3 [27000/49674 (54%)]\tLoss: 3.950090\n",
            "Train Epoch: 3 [27500/49674 (55%)]\tLoss: 4.196668\n",
            "Train Epoch: 3 [28000/49674 (56%)]\tLoss: 1.603427\n",
            "Train Epoch: 3 [28500/49674 (57%)]\tLoss: 4.465265\n",
            "Train Epoch: 3 [29000/49674 (58%)]\tLoss: 2.345128\n",
            "Train Epoch: 3 [29500/49674 (59%)]\tLoss: 2.693703\n",
            "Train Epoch: 3 [30000/49674 (60%)]\tLoss: 2.955271\n",
            "Train Epoch: 3 [30500/49674 (61%)]\tLoss: 4.660937\n",
            "Train Epoch: 3 [31000/49674 (62%)]\tLoss: 3.218611\n",
            "Train Epoch: 3 [31500/49674 (63%)]\tLoss: 3.233628\n",
            "Train Epoch: 3 [32000/49674 (64%)]\tLoss: 7.216248\n",
            "Train Epoch: 3 [32500/49674 (65%)]\tLoss: 2.045339\n",
            "Train Epoch: 3 [33000/49674 (66%)]\tLoss: 3.813967\n",
            "Train Epoch: 3 [33500/49674 (67%)]\tLoss: 3.810398\n",
            "Train Epoch: 3 [34000/49674 (68%)]\tLoss: 2.194708\n",
            "Train Epoch: 3 [34500/49674 (69%)]\tLoss: 3.015523\n",
            "Train Epoch: 3 [35000/49674 (70%)]\tLoss: 7.527434\n",
            "Train Epoch: 3 [35500/49674 (71%)]\tLoss: 1.998383\n",
            "Train Epoch: 3 [36000/49674 (72%)]\tLoss: 2.020090\n",
            "Train Epoch: 3 [36500/49674 (73%)]\tLoss: 5.187429\n",
            "Train Epoch: 3 [37000/49674 (74%)]\tLoss: 2.803141\n",
            "Train Epoch: 3 [37500/49674 (75%)]\tLoss: 4.495725\n",
            "Train Epoch: 3 [38000/49674 (76%)]\tLoss: 2.967590\n",
            "Train Epoch: 3 [38500/49674 (77%)]\tLoss: 2.259564\n",
            "Train Epoch: 3 [39000/49674 (78%)]\tLoss: 2.733260\n",
            "Train Epoch: 3 [39500/49674 (79%)]\tLoss: 2.046824\n",
            "Train Epoch: 3 [40000/49674 (80%)]\tLoss: 3.731385\n",
            "Train Epoch: 3 [40500/49674 (81%)]\tLoss: 2.048942\n",
            "Train Epoch: 3 [41000/49674 (82%)]\tLoss: 3.091146\n",
            "Train Epoch: 3 [41500/49674 (84%)]\tLoss: 2.581055\n",
            "Train Epoch: 3 [42000/49674 (85%)]\tLoss: 3.964925\n",
            "Train Epoch: 3 [42500/49674 (86%)]\tLoss: 3.667819\n",
            "Train Epoch: 3 [43000/49674 (87%)]\tLoss: 2.067416\n",
            "Train Epoch: 3 [43500/49674 (88%)]\tLoss: 2.300277\n",
            "Train Epoch: 3 [44000/49674 (89%)]\tLoss: 2.132303\n",
            "Train Epoch: 3 [44500/49674 (90%)]\tLoss: 2.951259\n",
            "Train Epoch: 3 [45000/49674 (91%)]\tLoss: 4.048525\n",
            "Train Epoch: 3 [45500/49674 (92%)]\tLoss: 2.466630\n",
            "Train Epoch: 3 [46000/49674 (93%)]\tLoss: 3.679303\n",
            "Train Epoch: 3 [46500/49674 (94%)]\tLoss: 4.820288\n",
            "Train Epoch: 3 [47000/49674 (95%)]\tLoss: 2.856453\n",
            "Train Epoch: 3 [47500/49674 (96%)]\tLoss: 3.246971\n",
            "Train Epoch: 3 [48000/49674 (97%)]\tLoss: 2.711884\n",
            "Train Epoch: 3 [48500/49674 (98%)]\tLoss: 1.777023\n",
            "Train Epoch: 3 [49000/49674 (99%)]\tLoss: 4.790098\n",
            "Train Epoch: 3 [49500/49674 (100%)]\tLoss: 3.588578\n",
            "Train Epoch: 4 [0/49674 (0%)]\tLoss: 1.870403\n",
            "Train Epoch: 4 [500/49674 (1%)]\tLoss: 2.060990\n",
            "Train Epoch: 4 [1000/49674 (2%)]\tLoss: 2.169872\n",
            "Train Epoch: 4 [1500/49674 (3%)]\tLoss: 1.956983\n",
            "Train Epoch: 4 [2000/49674 (4%)]\tLoss: 9.379832\n",
            "Train Epoch: 4 [2500/49674 (5%)]\tLoss: 2.799253\n",
            "Train Epoch: 4 [3000/49674 (6%)]\tLoss: 1.474778\n",
            "Train Epoch: 4 [3500/49674 (7%)]\tLoss: 2.286060\n",
            "Train Epoch: 4 [4000/49674 (8%)]\tLoss: 1.410483\n",
            "Train Epoch: 4 [4500/49674 (9%)]\tLoss: 1.100024\n",
            "Train Epoch: 4 [5000/49674 (10%)]\tLoss: 2.888579\n",
            "Train Epoch: 4 [5500/49674 (11%)]\tLoss: 5.050864\n",
            "Train Epoch: 4 [6000/49674 (12%)]\tLoss: 3.102824\n",
            "Train Epoch: 4 [6500/49674 (13%)]\tLoss: 1.520382\n",
            "Train Epoch: 4 [7000/49674 (14%)]\tLoss: 3.371884\n",
            "Train Epoch: 4 [7500/49674 (15%)]\tLoss: 1.748143\n",
            "Train Epoch: 4 [8000/49674 (16%)]\tLoss: 2.151938\n",
            "Train Epoch: 4 [8500/49674 (17%)]\tLoss: 4.551886\n",
            "Train Epoch: 4 [9000/49674 (18%)]\tLoss: 2.083948\n",
            "Train Epoch: 4 [9500/49674 (19%)]\tLoss: 5.713840\n",
            "Train Epoch: 4 [10000/49674 (20%)]\tLoss: 2.278919\n",
            "Train Epoch: 4 [10500/49674 (21%)]\tLoss: 5.343170\n",
            "Train Epoch: 4 [11000/49674 (22%)]\tLoss: 1.877470\n",
            "Train Epoch: 4 [11500/49674 (23%)]\tLoss: 2.504951\n",
            "Train Epoch: 4 [12000/49674 (24%)]\tLoss: 1.724467\n",
            "Train Epoch: 4 [12500/49674 (25%)]\tLoss: 1.856086\n",
            "Train Epoch: 4 [13000/49674 (26%)]\tLoss: 3.050194\n",
            "Train Epoch: 4 [13500/49674 (27%)]\tLoss: 1.171906\n",
            "Train Epoch: 4 [14000/49674 (28%)]\tLoss: 1.807499\n",
            "Train Epoch: 4 [14500/49674 (29%)]\tLoss: 2.773080\n",
            "Train Epoch: 4 [15000/49674 (30%)]\tLoss: 2.757186\n",
            "Train Epoch: 4 [15500/49674 (31%)]\tLoss: 1.412822\n",
            "Train Epoch: 4 [16000/49674 (32%)]\tLoss: 1.954721\n",
            "Train Epoch: 4 [16500/49674 (33%)]\tLoss: 5.116597\n",
            "Train Epoch: 4 [17000/49674 (34%)]\tLoss: 4.600605\n",
            "Train Epoch: 4 [17500/49674 (35%)]\tLoss: 2.759199\n",
            "Train Epoch: 4 [18000/49674 (36%)]\tLoss: 1.545595\n",
            "Train Epoch: 4 [18500/49674 (37%)]\tLoss: 4.879134\n",
            "Train Epoch: 4 [19000/49674 (38%)]\tLoss: 4.442244\n",
            "Train Epoch: 4 [19500/49674 (39%)]\tLoss: 1.673254\n",
            "Train Epoch: 4 [20000/49674 (40%)]\tLoss: 1.461988\n",
            "Train Epoch: 4 [20500/49674 (41%)]\tLoss: 1.621992\n",
            "Train Epoch: 4 [21000/49674 (42%)]\tLoss: 2.187353\n",
            "Train Epoch: 4 [21500/49674 (43%)]\tLoss: 3.740282\n",
            "Train Epoch: 4 [22000/49674 (44%)]\tLoss: 2.216078\n",
            "Train Epoch: 4 [22500/49674 (45%)]\tLoss: 1.506834\n",
            "Train Epoch: 4 [23000/49674 (46%)]\tLoss: 1.980840\n",
            "Train Epoch: 4 [23500/49674 (47%)]\tLoss: 2.814095\n",
            "Train Epoch: 4 [24000/49674 (48%)]\tLoss: 5.587682\n",
            "Train Epoch: 4 [24500/49674 (49%)]\tLoss: 4.191796\n",
            "Train Epoch: 4 [25000/49674 (50%)]\tLoss: 4.031493\n",
            "Train Epoch: 4 [25500/49674 (51%)]\tLoss: 3.518299\n",
            "Train Epoch: 4 [26000/49674 (52%)]\tLoss: 4.659568\n",
            "Train Epoch: 4 [26500/49674 (53%)]\tLoss: 3.082752\n",
            "Train Epoch: 4 [27000/49674 (54%)]\tLoss: 1.957248\n",
            "Train Epoch: 4 [27500/49674 (55%)]\tLoss: 3.017657\n",
            "Train Epoch: 4 [28000/49674 (56%)]\tLoss: 3.528390\n",
            "Train Epoch: 4 [28500/49674 (57%)]\tLoss: 4.856489\n",
            "Train Epoch: 4 [29000/49674 (58%)]\tLoss: 4.044332\n",
            "Train Epoch: 4 [29500/49674 (59%)]\tLoss: 2.704831\n",
            "Train Epoch: 4 [30000/49674 (60%)]\tLoss: 2.972940\n",
            "Train Epoch: 4 [30500/49674 (61%)]\tLoss: 4.527733\n",
            "Train Epoch: 4 [31000/49674 (62%)]\tLoss: 2.714551\n",
            "Train Epoch: 4 [31500/49674 (63%)]\tLoss: 1.722369\n",
            "Train Epoch: 4 [32000/49674 (64%)]\tLoss: 2.053863\n",
            "Train Epoch: 4 [32500/49674 (65%)]\tLoss: 2.326161\n",
            "Train Epoch: 4 [33000/49674 (66%)]\tLoss: 2.446347\n",
            "Train Epoch: 4 [33500/49674 (67%)]\tLoss: 2.882511\n",
            "Train Epoch: 4 [34000/49674 (68%)]\tLoss: 1.692093\n",
            "Train Epoch: 4 [34500/49674 (69%)]\tLoss: 2.932707\n",
            "Train Epoch: 4 [35000/49674 (70%)]\tLoss: 3.551465\n",
            "Train Epoch: 4 [35500/49674 (71%)]\tLoss: 3.161821\n",
            "Train Epoch: 4 [36000/49674 (72%)]\tLoss: 2.427732\n",
            "Train Epoch: 4 [36500/49674 (73%)]\tLoss: 2.406997\n",
            "Train Epoch: 4 [37000/49674 (74%)]\tLoss: 2.747016\n",
            "Train Epoch: 4 [37500/49674 (75%)]\tLoss: 5.096647\n",
            "Train Epoch: 4 [38000/49674 (76%)]\tLoss: 1.877081\n",
            "Train Epoch: 4 [38500/49674 (77%)]\tLoss: 3.398324\n",
            "Train Epoch: 4 [39000/49674 (78%)]\tLoss: 1.993758\n",
            "Train Epoch: 4 [39500/49674 (79%)]\tLoss: 2.147100\n",
            "Train Epoch: 4 [40000/49674 (80%)]\tLoss: 1.632588\n",
            "Train Epoch: 4 [40500/49674 (81%)]\tLoss: 3.686946\n",
            "Train Epoch: 4 [41000/49674 (82%)]\tLoss: 1.135756\n",
            "Train Epoch: 4 [41500/49674 (84%)]\tLoss: 2.850555\n",
            "Train Epoch: 4 [42000/49674 (85%)]\tLoss: 2.282759\n",
            "Train Epoch: 4 [42500/49674 (86%)]\tLoss: 4.629956\n",
            "Train Epoch: 4 [43000/49674 (87%)]\tLoss: 4.970262\n",
            "Train Epoch: 4 [43500/49674 (88%)]\tLoss: 1.613148\n",
            "Train Epoch: 4 [44000/49674 (89%)]\tLoss: 1.521254\n",
            "Train Epoch: 4 [44500/49674 (90%)]\tLoss: 1.709942\n",
            "Train Epoch: 4 [45000/49674 (91%)]\tLoss: 3.428912\n",
            "Train Epoch: 4 [45500/49674 (92%)]\tLoss: 2.197193\n",
            "Train Epoch: 4 [46000/49674 (93%)]\tLoss: 4.568519\n",
            "Train Epoch: 4 [46500/49674 (94%)]\tLoss: 3.468569\n",
            "Train Epoch: 4 [47000/49674 (95%)]\tLoss: 1.865628\n",
            "Train Epoch: 4 [47500/49674 (96%)]\tLoss: 3.662062\n",
            "Train Epoch: 4 [48000/49674 (97%)]\tLoss: 1.990036\n",
            "Train Epoch: 4 [48500/49674 (98%)]\tLoss: 3.073627\n",
            "Train Epoch: 4 [49000/49674 (99%)]\tLoss: 1.300826\n",
            "Train Epoch: 4 [49500/49674 (100%)]\tLoss: 1.656167\n",
            "Train Epoch: 5 [0/49674 (0%)]\tLoss: 2.554644\n",
            "Train Epoch: 5 [500/49674 (1%)]\tLoss: 3.156015\n",
            "Train Epoch: 5 [1000/49674 (2%)]\tLoss: 2.206120\n",
            "Train Epoch: 5 [1500/49674 (3%)]\tLoss: 4.674292\n",
            "Train Epoch: 5 [2000/49674 (4%)]\tLoss: 2.033337\n",
            "Train Epoch: 5 [2500/49674 (5%)]\tLoss: 2.376092\n",
            "Train Epoch: 5 [3000/49674 (6%)]\tLoss: 1.536784\n",
            "Train Epoch: 5 [3500/49674 (7%)]\tLoss: 4.679535\n",
            "Train Epoch: 5 [4000/49674 (8%)]\tLoss: 2.449303\n",
            "Train Epoch: 5 [4500/49674 (9%)]\tLoss: 1.460886\n",
            "Train Epoch: 5 [5000/49674 (10%)]\tLoss: 1.563330\n",
            "Train Epoch: 5 [5500/49674 (11%)]\tLoss: 2.986092\n",
            "Train Epoch: 5 [6000/49674 (12%)]\tLoss: 1.613633\n",
            "Train Epoch: 5 [6500/49674 (13%)]\tLoss: 1.846929\n",
            "Train Epoch: 5 [7000/49674 (14%)]\tLoss: 1.663342\n",
            "Train Epoch: 5 [7500/49674 (15%)]\tLoss: 3.765268\n",
            "Train Epoch: 5 [8000/49674 (16%)]\tLoss: 2.598747\n",
            "Train Epoch: 5 [8500/49674 (17%)]\tLoss: 1.965796\n",
            "Train Epoch: 5 [9000/49674 (18%)]\tLoss: 2.166070\n",
            "Train Epoch: 5 [9500/49674 (19%)]\tLoss: 0.807256\n",
            "Train Epoch: 5 [10000/49674 (20%)]\tLoss: 1.564088\n",
            "Train Epoch: 5 [10500/49674 (21%)]\tLoss: 1.960737\n",
            "Train Epoch: 5 [11000/49674 (22%)]\tLoss: 2.419434\n",
            "Train Epoch: 5 [11500/49674 (23%)]\tLoss: 1.225709\n",
            "Train Epoch: 5 [12000/49674 (24%)]\tLoss: 1.190686\n",
            "Train Epoch: 5 [12500/49674 (25%)]\tLoss: 2.876342\n",
            "Train Epoch: 5 [13000/49674 (26%)]\tLoss: 2.468434\n",
            "Train Epoch: 5 [13500/49674 (27%)]\tLoss: 1.265678\n",
            "Train Epoch: 5 [14000/49674 (28%)]\tLoss: 1.779788\n",
            "Train Epoch: 5 [14500/49674 (29%)]\tLoss: 3.319539\n",
            "Train Epoch: 5 [15000/49674 (30%)]\tLoss: 2.717815\n",
            "Train Epoch: 5 [15500/49674 (31%)]\tLoss: 3.113502\n",
            "Train Epoch: 5 [16000/49674 (32%)]\tLoss: 4.231991\n",
            "Train Epoch: 5 [16500/49674 (33%)]\tLoss: 1.839057\n",
            "Train Epoch: 5 [17000/49674 (34%)]\tLoss: 1.221036\n",
            "Train Epoch: 5 [17500/49674 (35%)]\tLoss: 2.599795\n",
            "Train Epoch: 5 [18000/49674 (36%)]\tLoss: 3.959410\n",
            "Train Epoch: 5 [18500/49674 (37%)]\tLoss: 1.566350\n",
            "Train Epoch: 5 [19000/49674 (38%)]\tLoss: 2.151743\n",
            "Train Epoch: 5 [19500/49674 (39%)]\tLoss: 0.986202\n",
            "Train Epoch: 5 [20000/49674 (40%)]\tLoss: 2.297881\n",
            "Train Epoch: 5 [20500/49674 (41%)]\tLoss: 1.408397\n",
            "Train Epoch: 5 [21000/49674 (42%)]\tLoss: 3.729488\n",
            "Train Epoch: 5 [21500/49674 (43%)]\tLoss: 2.120749\n",
            "Train Epoch: 5 [22000/49674 (44%)]\tLoss: 3.513482\n",
            "Train Epoch: 5 [22500/49674 (45%)]\tLoss: 3.611868\n",
            "Train Epoch: 5 [23000/49674 (46%)]\tLoss: 5.812349\n",
            "Train Epoch: 5 [23500/49674 (47%)]\tLoss: 4.609383\n",
            "Train Epoch: 5 [24000/49674 (48%)]\tLoss: 1.918240\n",
            "Train Epoch: 5 [24500/49674 (49%)]\tLoss: 1.729905\n",
            "Train Epoch: 5 [25000/49674 (50%)]\tLoss: 1.278518\n",
            "Train Epoch: 5 [25500/49674 (51%)]\tLoss: 1.358868\n",
            "Train Epoch: 5 [26000/49674 (52%)]\tLoss: 3.215068\n",
            "Train Epoch: 5 [26500/49674 (53%)]\tLoss: 1.238826\n",
            "Train Epoch: 5 [27000/49674 (54%)]\tLoss: 5.854557\n",
            "Train Epoch: 5 [27500/49674 (55%)]\tLoss: 1.568547\n",
            "Train Epoch: 5 [28000/49674 (56%)]\tLoss: 1.362933\n",
            "Train Epoch: 5 [28500/49674 (57%)]\tLoss: 1.398417\n",
            "Train Epoch: 5 [29000/49674 (58%)]\tLoss: 2.137739\n",
            "Train Epoch: 5 [29500/49674 (59%)]\tLoss: 2.260654\n",
            "Train Epoch: 5 [30000/49674 (60%)]\tLoss: 3.964370\n",
            "Train Epoch: 5 [30500/49674 (61%)]\tLoss: 2.277385\n",
            "Train Epoch: 5 [31000/49674 (62%)]\tLoss: 1.804989\n",
            "Train Epoch: 5 [31500/49674 (63%)]\tLoss: 3.722433\n",
            "Train Epoch: 5 [32000/49674 (64%)]\tLoss: 1.665680\n",
            "Train Epoch: 5 [32500/49674 (65%)]\tLoss: 2.711268\n",
            "Train Epoch: 5 [33000/49674 (66%)]\tLoss: 1.858765\n",
            "Train Epoch: 5 [33500/49674 (67%)]\tLoss: 2.494772\n",
            "Train Epoch: 5 [34000/49674 (68%)]\tLoss: 5.811383\n",
            "Train Epoch: 5 [34500/49674 (69%)]\tLoss: 2.987526\n",
            "Train Epoch: 5 [35000/49674 (70%)]\tLoss: 2.692496\n",
            "Train Epoch: 5 [35500/49674 (71%)]\tLoss: 3.075475\n",
            "Train Epoch: 5 [36000/49674 (72%)]\tLoss: 3.491692\n",
            "Train Epoch: 5 [36500/49674 (73%)]\tLoss: 1.406164\n",
            "Train Epoch: 5 [37000/49674 (74%)]\tLoss: 1.404440\n",
            "Train Epoch: 5 [37500/49674 (75%)]\tLoss: 3.167720\n",
            "Train Epoch: 5 [38000/49674 (76%)]\tLoss: 1.095928\n",
            "Train Epoch: 5 [38500/49674 (77%)]\tLoss: 2.175251\n",
            "Train Epoch: 5 [39000/49674 (78%)]\tLoss: 4.315083\n",
            "Train Epoch: 5 [39500/49674 (79%)]\tLoss: 3.680013\n",
            "Train Epoch: 5 [40000/49674 (80%)]\tLoss: 2.120343\n",
            "Train Epoch: 5 [40500/49674 (81%)]\tLoss: 0.993357\n",
            "Train Epoch: 5 [41000/49674 (82%)]\tLoss: 1.768820\n",
            "Train Epoch: 5 [41500/49674 (84%)]\tLoss: 1.215670\n",
            "Train Epoch: 5 [42000/49674 (85%)]\tLoss: 2.613211\n",
            "Train Epoch: 5 [42500/49674 (86%)]\tLoss: 1.919830\n",
            "Train Epoch: 5 [43000/49674 (87%)]\tLoss: 1.739373\n",
            "Train Epoch: 5 [43500/49674 (88%)]\tLoss: 1.363976\n",
            "Train Epoch: 5 [44000/49674 (89%)]\tLoss: 2.820302\n",
            "Train Epoch: 5 [44500/49674 (90%)]\tLoss: 1.411039\n",
            "Train Epoch: 5 [45000/49674 (91%)]\tLoss: 3.871212\n",
            "Train Epoch: 5 [45500/49674 (92%)]\tLoss: 2.993568\n",
            "Train Epoch: 5 [46000/49674 (93%)]\tLoss: 2.037702\n",
            "Train Epoch: 5 [46500/49674 (94%)]\tLoss: 2.476148\n",
            "Train Epoch: 5 [47000/49674 (95%)]\tLoss: 2.256272\n",
            "Train Epoch: 5 [47500/49674 (96%)]\tLoss: 1.778394\n",
            "Train Epoch: 5 [48000/49674 (97%)]\tLoss: 3.536831\n",
            "Train Epoch: 5 [48500/49674 (98%)]\tLoss: 1.762957\n",
            "Train Epoch: 5 [49000/49674 (99%)]\tLoss: 3.292072\n",
            "Train Epoch: 5 [49500/49674 (100%)]\tLoss: 2.373891\n",
            "Epoch: 5\n",
            "Training loss: 2.3883366325731488\n",
            "Train Epoch: 6 [0/49674 (0%)]\tLoss: 1.490105\n",
            "Train Epoch: 6 [500/49674 (1%)]\tLoss: 1.846048\n",
            "Train Epoch: 6 [1000/49674 (2%)]\tLoss: 2.771477\n",
            "Train Epoch: 6 [1500/49674 (3%)]\tLoss: 1.161921\n",
            "Train Epoch: 6 [2000/49674 (4%)]\tLoss: 2.210922\n",
            "Train Epoch: 6 [2500/49674 (5%)]\tLoss: 1.668005\n",
            "Train Epoch: 6 [3000/49674 (6%)]\tLoss: 0.844820\n",
            "Train Epoch: 6 [3500/49674 (7%)]\tLoss: 0.753888\n",
            "Train Epoch: 6 [4000/49674 (8%)]\tLoss: 1.743228\n",
            "Train Epoch: 6 [4500/49674 (9%)]\tLoss: 1.427557\n",
            "Train Epoch: 6 [5000/49674 (10%)]\tLoss: 1.435488\n",
            "Train Epoch: 6 [5500/49674 (11%)]\tLoss: 2.520510\n",
            "Train Epoch: 6 [6000/49674 (12%)]\tLoss: 2.242725\n",
            "Train Epoch: 6 [6500/49674 (13%)]\tLoss: 2.994705\n",
            "Train Epoch: 6 [7000/49674 (14%)]\tLoss: 0.917214\n",
            "Train Epoch: 6 [7500/49674 (15%)]\tLoss: 1.867357\n",
            "Train Epoch: 6 [8000/49674 (16%)]\tLoss: 1.804551\n",
            "Train Epoch: 6 [8500/49674 (17%)]\tLoss: 2.670680\n",
            "Train Epoch: 6 [9000/49674 (18%)]\tLoss: 1.041236\n",
            "Train Epoch: 6 [9500/49674 (19%)]\tLoss: 0.816097\n",
            "Train Epoch: 6 [10000/49674 (20%)]\tLoss: 2.967428\n",
            "Train Epoch: 6 [10500/49674 (21%)]\tLoss: 1.867729\n",
            "Train Epoch: 6 [11000/49674 (22%)]\tLoss: 1.611240\n",
            "Train Epoch: 6 [11500/49674 (23%)]\tLoss: 1.325362\n",
            "Train Epoch: 6 [12000/49674 (24%)]\tLoss: 1.907446\n",
            "Train Epoch: 6 [12500/49674 (25%)]\tLoss: 2.396521\n",
            "Train Epoch: 6 [13000/49674 (26%)]\tLoss: 1.103935\n",
            "Train Epoch: 6 [13500/49674 (27%)]\tLoss: 3.266803\n",
            "Train Epoch: 6 [14000/49674 (28%)]\tLoss: 2.178504\n",
            "Train Epoch: 6 [14500/49674 (29%)]\tLoss: 2.345260\n",
            "Train Epoch: 6 [15000/49674 (30%)]\tLoss: 0.780263\n",
            "Train Epoch: 6 [15500/49674 (31%)]\tLoss: 1.491776\n",
            "Train Epoch: 6 [16000/49674 (32%)]\tLoss: 1.687072\n",
            "Train Epoch: 6 [16500/49674 (33%)]\tLoss: 4.645577\n",
            "Train Epoch: 6 [17000/49674 (34%)]\tLoss: 4.634013\n",
            "Train Epoch: 6 [17500/49674 (35%)]\tLoss: 2.237538\n",
            "Train Epoch: 6 [18000/49674 (36%)]\tLoss: 0.931046\n",
            "Train Epoch: 6 [18500/49674 (37%)]\tLoss: 1.262348\n",
            "Train Epoch: 6 [19000/49674 (38%)]\tLoss: 1.408772\n",
            "Train Epoch: 6 [19500/49674 (39%)]\tLoss: 2.476706\n",
            "Train Epoch: 6 [20000/49674 (40%)]\tLoss: 2.703120\n",
            "Train Epoch: 6 [20500/49674 (41%)]\tLoss: 1.725853\n",
            "Train Epoch: 6 [21000/49674 (42%)]\tLoss: 3.099249\n",
            "Train Epoch: 6 [21500/49674 (43%)]\tLoss: 3.120129\n",
            "Train Epoch: 6 [22000/49674 (44%)]\tLoss: 3.178492\n",
            "Train Epoch: 6 [22500/49674 (45%)]\tLoss: 3.266167\n",
            "Train Epoch: 6 [23000/49674 (46%)]\tLoss: 1.637513\n",
            "Train Epoch: 6 [23500/49674 (47%)]\tLoss: 1.053602\n",
            "Train Epoch: 6 [24000/49674 (48%)]\tLoss: 3.468712\n",
            "Train Epoch: 6 [24500/49674 (49%)]\tLoss: 1.450465\n",
            "Train Epoch: 6 [25000/49674 (50%)]\tLoss: 4.567103\n",
            "Train Epoch: 6 [25500/49674 (51%)]\tLoss: 3.946262\n",
            "Train Epoch: 6 [26000/49674 (52%)]\tLoss: 2.151147\n",
            "Train Epoch: 6 [26500/49674 (53%)]\tLoss: 3.531982\n",
            "Train Epoch: 6 [27000/49674 (54%)]\tLoss: 1.031387\n",
            "Train Epoch: 6 [27500/49674 (55%)]\tLoss: 3.400028\n",
            "Train Epoch: 6 [28000/49674 (56%)]\tLoss: 0.905054\n",
            "Train Epoch: 6 [28500/49674 (57%)]\tLoss: 2.863600\n",
            "Train Epoch: 6 [29000/49674 (58%)]\tLoss: 1.119630\n",
            "Train Epoch: 6 [29500/49674 (59%)]\tLoss: 3.113543\n",
            "Train Epoch: 6 [30000/49674 (60%)]\tLoss: 2.196601\n",
            "Train Epoch: 6 [30500/49674 (61%)]\tLoss: 1.223280\n",
            "Train Epoch: 6 [31000/49674 (62%)]\tLoss: 2.453905\n",
            "Train Epoch: 6 [31500/49674 (63%)]\tLoss: 1.249216\n",
            "Train Epoch: 6 [32000/49674 (64%)]\tLoss: 3.212938\n",
            "Train Epoch: 6 [32500/49674 (65%)]\tLoss: 2.137431\n",
            "Train Epoch: 6 [33000/49674 (66%)]\tLoss: 2.121431\n",
            "Train Epoch: 6 [33500/49674 (67%)]\tLoss: 1.180853\n",
            "Train Epoch: 6 [34000/49674 (68%)]\tLoss: 2.117113\n",
            "Train Epoch: 6 [34500/49674 (69%)]\tLoss: 2.093924\n",
            "Train Epoch: 6 [35000/49674 (70%)]\tLoss: 3.862550\n",
            "Train Epoch: 6 [35500/49674 (71%)]\tLoss: 3.769696\n",
            "Train Epoch: 6 [36000/49674 (72%)]\tLoss: 1.985275\n",
            "Train Epoch: 6 [36500/49674 (73%)]\tLoss: 1.709560\n",
            "Train Epoch: 6 [37000/49674 (74%)]\tLoss: 1.080779\n",
            "Train Epoch: 6 [37500/49674 (75%)]\tLoss: 2.982651\n",
            "Train Epoch: 6 [38000/49674 (76%)]\tLoss: 3.071303\n",
            "Train Epoch: 6 [38500/49674 (77%)]\tLoss: 2.265167\n",
            "Train Epoch: 6 [39000/49674 (78%)]\tLoss: 0.720658\n",
            "Train Epoch: 6 [39500/49674 (79%)]\tLoss: 0.778366\n",
            "Train Epoch: 6 [40000/49674 (80%)]\tLoss: 2.388200\n",
            "Train Epoch: 6 [40500/49674 (81%)]\tLoss: 0.796685\n",
            "Train Epoch: 6 [41000/49674 (82%)]\tLoss: 2.476331\n",
            "Train Epoch: 6 [41500/49674 (84%)]\tLoss: 3.242826\n",
            "Train Epoch: 6 [42000/49674 (85%)]\tLoss: 1.865582\n",
            "Train Epoch: 6 [42500/49674 (86%)]\tLoss: 0.961684\n",
            "Train Epoch: 6 [43000/49674 (87%)]\tLoss: 3.507702\n",
            "Train Epoch: 6 [43500/49674 (88%)]\tLoss: 2.960123\n",
            "Train Epoch: 6 [44000/49674 (89%)]\tLoss: 2.961048\n",
            "Train Epoch: 6 [44500/49674 (90%)]\tLoss: 1.406574\n",
            "Train Epoch: 6 [45000/49674 (91%)]\tLoss: 2.034736\n",
            "Train Epoch: 6 [45500/49674 (92%)]\tLoss: 1.301227\n",
            "Train Epoch: 6 [46000/49674 (93%)]\tLoss: 1.706515\n",
            "Train Epoch: 6 [46500/49674 (94%)]\tLoss: 3.116996\n",
            "Train Epoch: 6 [47000/49674 (95%)]\tLoss: 1.300935\n",
            "Train Epoch: 6 [47500/49674 (96%)]\tLoss: 2.652734\n",
            "Train Epoch: 6 [48000/49674 (97%)]\tLoss: 1.535228\n",
            "Train Epoch: 6 [48500/49674 (98%)]\tLoss: 1.786341\n",
            "Train Epoch: 6 [49000/49674 (99%)]\tLoss: 1.986872\n",
            "Train Epoch: 6 [49500/49674 (100%)]\tLoss: 0.706932\n",
            "Train Epoch: 7 [0/49674 (0%)]\tLoss: 2.752557\n",
            "Train Epoch: 7 [500/49674 (1%)]\tLoss: 1.451470\n",
            "Train Epoch: 7 [1000/49674 (2%)]\tLoss: 2.538416\n",
            "Train Epoch: 7 [1500/49674 (3%)]\tLoss: 2.567601\n",
            "Train Epoch: 7 [2000/49674 (4%)]\tLoss: 3.321470\n",
            "Train Epoch: 7 [2500/49674 (5%)]\tLoss: 1.665469\n",
            "Train Epoch: 7 [3000/49674 (6%)]\tLoss: 2.109604\n",
            "Train Epoch: 7 [3500/49674 (7%)]\tLoss: 1.992421\n",
            "Train Epoch: 7 [4000/49674 (8%)]\tLoss: 3.064872\n",
            "Train Epoch: 7 [4500/49674 (9%)]\tLoss: 1.972902\n",
            "Train Epoch: 7 [5000/49674 (10%)]\tLoss: 2.252646\n",
            "Train Epoch: 7 [5500/49674 (11%)]\tLoss: 1.760532\n",
            "Train Epoch: 7 [6000/49674 (12%)]\tLoss: 0.558137\n",
            "Train Epoch: 7 [6500/49674 (13%)]\tLoss: 3.140662\n",
            "Train Epoch: 7 [7000/49674 (14%)]\tLoss: 3.471347\n",
            "Train Epoch: 7 [7500/49674 (15%)]\tLoss: 1.515167\n",
            "Train Epoch: 7 [8000/49674 (16%)]\tLoss: 2.735504\n",
            "Train Epoch: 7 [8500/49674 (17%)]\tLoss: 2.756243\n",
            "Train Epoch: 7 [9000/49674 (18%)]\tLoss: 2.621352\n",
            "Train Epoch: 7 [9500/49674 (19%)]\tLoss: 1.453015\n",
            "Train Epoch: 7 [10000/49674 (20%)]\tLoss: 0.973164\n",
            "Train Epoch: 7 [10500/49674 (21%)]\tLoss: 1.532100\n",
            "Train Epoch: 7 [11000/49674 (22%)]\tLoss: 2.125472\n",
            "Train Epoch: 7 [11500/49674 (23%)]\tLoss: 0.970433\n",
            "Train Epoch: 7 [12000/49674 (24%)]\tLoss: 1.132985\n",
            "Train Epoch: 7 [12500/49674 (25%)]\tLoss: 1.133966\n",
            "Train Epoch: 7 [13000/49674 (26%)]\tLoss: 1.243751\n",
            "Train Epoch: 7 [13500/49674 (27%)]\tLoss: 1.807645\n",
            "Train Epoch: 7 [14000/49674 (28%)]\tLoss: 2.689586\n",
            "Train Epoch: 7 [14500/49674 (29%)]\tLoss: 1.459788\n",
            "Train Epoch: 7 [15000/49674 (30%)]\tLoss: 2.637093\n",
            "Train Epoch: 7 [15500/49674 (31%)]\tLoss: 1.597912\n",
            "Train Epoch: 7 [16000/49674 (32%)]\tLoss: 1.416583\n",
            "Train Epoch: 7 [16500/49674 (33%)]\tLoss: 3.080953\n",
            "Train Epoch: 7 [17000/49674 (34%)]\tLoss: 1.977791\n",
            "Train Epoch: 7 [17500/49674 (35%)]\tLoss: 1.030811\n",
            "Train Epoch: 7 [18000/49674 (36%)]\tLoss: 2.222346\n",
            "Train Epoch: 7 [18500/49674 (37%)]\tLoss: 3.424582\n",
            "Train Epoch: 7 [19000/49674 (38%)]\tLoss: 1.600661\n",
            "Train Epoch: 7 [19500/49674 (39%)]\tLoss: 3.166985\n",
            "Train Epoch: 7 [20000/49674 (40%)]\tLoss: 1.481459\n",
            "Train Epoch: 7 [20500/49674 (41%)]\tLoss: 1.238785\n",
            "Train Epoch: 7 [21000/49674 (42%)]\tLoss: 2.943043\n",
            "Train Epoch: 7 [21500/49674 (43%)]\tLoss: 1.231855\n",
            "Train Epoch: 7 [22000/49674 (44%)]\tLoss: 1.177076\n",
            "Train Epoch: 7 [22500/49674 (45%)]\tLoss: 0.913904\n",
            "Train Epoch: 7 [23000/49674 (46%)]\tLoss: 1.265608\n",
            "Train Epoch: 7 [23500/49674 (47%)]\tLoss: 1.458168\n",
            "Train Epoch: 7 [24000/49674 (48%)]\tLoss: 1.725151\n",
            "Train Epoch: 7 [24500/49674 (49%)]\tLoss: 2.768255\n",
            "Train Epoch: 7 [25000/49674 (50%)]\tLoss: 1.710590\n",
            "Train Epoch: 7 [25500/49674 (51%)]\tLoss: 2.518702\n",
            "Train Epoch: 7 [26000/49674 (52%)]\tLoss: 1.667991\n",
            "Train Epoch: 7 [26500/49674 (53%)]\tLoss: 0.976178\n",
            "Train Epoch: 7 [27000/49674 (54%)]\tLoss: 1.972176\n",
            "Train Epoch: 7 [27500/49674 (55%)]\tLoss: 1.023864\n",
            "Train Epoch: 7 [28000/49674 (56%)]\tLoss: 1.532846\n",
            "Train Epoch: 7 [28500/49674 (57%)]\tLoss: 1.529168\n",
            "Train Epoch: 7 [29000/49674 (58%)]\tLoss: 3.387927\n",
            "Train Epoch: 7 [29500/49674 (59%)]\tLoss: 0.654550\n",
            "Train Epoch: 7 [30000/49674 (60%)]\tLoss: 2.317962\n",
            "Train Epoch: 7 [30500/49674 (61%)]\tLoss: 1.138951\n",
            "Train Epoch: 7 [31000/49674 (62%)]\tLoss: 3.214844\n",
            "Train Epoch: 7 [31500/49674 (63%)]\tLoss: 0.917866\n",
            "Train Epoch: 7 [32000/49674 (64%)]\tLoss: 0.906719\n",
            "Train Epoch: 7 [32500/49674 (65%)]\tLoss: 1.758256\n",
            "Train Epoch: 7 [33000/49674 (66%)]\tLoss: 1.792279\n",
            "Train Epoch: 7 [33500/49674 (67%)]\tLoss: 1.876289\n",
            "Train Epoch: 7 [34000/49674 (68%)]\tLoss: 1.460805\n",
            "Train Epoch: 7 [34500/49674 (69%)]\tLoss: 1.155126\n",
            "Train Epoch: 7 [35000/49674 (70%)]\tLoss: 1.036667\n",
            "Train Epoch: 7 [35500/49674 (71%)]\tLoss: 1.142701\n",
            "Train Epoch: 7 [36000/49674 (72%)]\tLoss: 1.777843\n",
            "Train Epoch: 7 [36500/49674 (73%)]\tLoss: 1.049736\n",
            "Train Epoch: 7 [37000/49674 (74%)]\tLoss: 3.265174\n",
            "Train Epoch: 7 [37500/49674 (75%)]\tLoss: 1.549528\n",
            "Train Epoch: 7 [38000/49674 (76%)]\tLoss: 2.247896\n",
            "Train Epoch: 7 [38500/49674 (77%)]\tLoss: 1.761322\n",
            "Train Epoch: 7 [39000/49674 (78%)]\tLoss: 3.867898\n",
            "Train Epoch: 7 [39500/49674 (79%)]\tLoss: 3.780613\n",
            "Train Epoch: 7 [40000/49674 (80%)]\tLoss: 1.735893\n",
            "Train Epoch: 7 [40500/49674 (81%)]\tLoss: 3.222381\n",
            "Train Epoch: 7 [41000/49674 (82%)]\tLoss: 1.026361\n",
            "Train Epoch: 7 [41500/49674 (84%)]\tLoss: 2.786276\n",
            "Train Epoch: 7 [42000/49674 (85%)]\tLoss: 1.538371\n",
            "Train Epoch: 7 [42500/49674 (86%)]\tLoss: 1.637956\n",
            "Train Epoch: 7 [43000/49674 (87%)]\tLoss: 1.344763\n",
            "Train Epoch: 7 [43500/49674 (88%)]\tLoss: 0.863291\n",
            "Train Epoch: 7 [44000/49674 (89%)]\tLoss: 1.764636\n",
            "Train Epoch: 7 [44500/49674 (90%)]\tLoss: 2.357806\n",
            "Train Epoch: 7 [45000/49674 (91%)]\tLoss: 1.168390\n",
            "Train Epoch: 7 [45500/49674 (92%)]\tLoss: 0.977479\n",
            "Train Epoch: 7 [46000/49674 (93%)]\tLoss: 2.520531\n",
            "Train Epoch: 7 [46500/49674 (94%)]\tLoss: 2.768382\n",
            "Train Epoch: 7 [47000/49674 (95%)]\tLoss: 1.505317\n",
            "Train Epoch: 7 [47500/49674 (96%)]\tLoss: 2.550699\n",
            "Train Epoch: 7 [48000/49674 (97%)]\tLoss: 2.079211\n",
            "Train Epoch: 7 [48500/49674 (98%)]\tLoss: 1.048912\n",
            "Train Epoch: 7 [49000/49674 (99%)]\tLoss: 1.752906\n",
            "Train Epoch: 7 [49500/49674 (100%)]\tLoss: 0.914562\n",
            "Train Epoch: 8 [0/49674 (0%)]\tLoss: 1.955153\n",
            "Train Epoch: 8 [500/49674 (1%)]\tLoss: 0.747165\n",
            "Train Epoch: 8 [1000/49674 (2%)]\tLoss: 3.152450\n",
            "Train Epoch: 8 [1500/49674 (3%)]\tLoss: 1.632880\n",
            "Train Epoch: 8 [2000/49674 (4%)]\tLoss: 1.644063\n",
            "Train Epoch: 8 [2500/49674 (5%)]\tLoss: 1.250633\n",
            "Train Epoch: 8 [3000/49674 (6%)]\tLoss: 1.191214\n",
            "Train Epoch: 8 [3500/49674 (7%)]\tLoss: 2.511198\n",
            "Train Epoch: 8 [4000/49674 (8%)]\tLoss: 1.607471\n",
            "Train Epoch: 8 [4500/49674 (9%)]\tLoss: 0.763411\n",
            "Train Epoch: 8 [5000/49674 (10%)]\tLoss: 0.359794\n",
            "Train Epoch: 8 [5500/49674 (11%)]\tLoss: 2.242075\n",
            "Train Epoch: 8 [6000/49674 (12%)]\tLoss: 1.118363\n",
            "Train Epoch: 8 [6500/49674 (13%)]\tLoss: 3.529790\n",
            "Train Epoch: 8 [7000/49674 (14%)]\tLoss: 1.627522\n",
            "Train Epoch: 8 [7500/49674 (15%)]\tLoss: 0.622085\n",
            "Train Epoch: 8 [8000/49674 (16%)]\tLoss: 0.813827\n",
            "Train Epoch: 8 [8500/49674 (17%)]\tLoss: 1.131956\n",
            "Train Epoch: 8 [9000/49674 (18%)]\tLoss: 0.551496\n",
            "Train Epoch: 8 [9500/49674 (19%)]\tLoss: 1.346688\n",
            "Train Epoch: 8 [10000/49674 (20%)]\tLoss: 1.854936\n",
            "Train Epoch: 8 [10500/49674 (21%)]\tLoss: 2.470522\n",
            "Train Epoch: 8 [11000/49674 (22%)]\tLoss: 0.786684\n",
            "Train Epoch: 8 [11500/49674 (23%)]\tLoss: 1.723149\n",
            "Train Epoch: 8 [12000/49674 (24%)]\tLoss: 2.736813\n",
            "Train Epoch: 8 [12500/49674 (25%)]\tLoss: 0.938824\n",
            "Train Epoch: 8 [13000/49674 (26%)]\tLoss: 2.511539\n",
            "Train Epoch: 8 [13500/49674 (27%)]\tLoss: 1.480256\n",
            "Train Epoch: 8 [14000/49674 (28%)]\tLoss: 1.868152\n",
            "Train Epoch: 8 [14500/49674 (29%)]\tLoss: 2.424981\n",
            "Train Epoch: 8 [15000/49674 (30%)]\tLoss: 3.892908\n",
            "Train Epoch: 8 [15500/49674 (31%)]\tLoss: 1.185581\n",
            "Train Epoch: 8 [16000/49674 (32%)]\tLoss: 0.918639\n",
            "Train Epoch: 8 [16500/49674 (33%)]\tLoss: 2.135263\n",
            "Train Epoch: 8 [17000/49674 (34%)]\tLoss: 1.282127\n",
            "Train Epoch: 8 [17500/49674 (35%)]\tLoss: 2.748519\n",
            "Train Epoch: 8 [18000/49674 (36%)]\tLoss: 1.351728\n",
            "Train Epoch: 8 [18500/49674 (37%)]\tLoss: 3.623305\n",
            "Train Epoch: 8 [19000/49674 (38%)]\tLoss: 2.754153\n",
            "Train Epoch: 8 [19500/49674 (39%)]\tLoss: 1.707084\n",
            "Train Epoch: 8 [20000/49674 (40%)]\tLoss: 1.499663\n",
            "Train Epoch: 8 [20500/49674 (41%)]\tLoss: 0.931597\n",
            "Train Epoch: 8 [21000/49674 (42%)]\tLoss: 0.914755\n",
            "Train Epoch: 8 [21500/49674 (43%)]\tLoss: 1.539956\n",
            "Train Epoch: 8 [22000/49674 (44%)]\tLoss: 2.438066\n",
            "Train Epoch: 8 [22500/49674 (45%)]\tLoss: 1.786343\n",
            "Train Epoch: 8 [23000/49674 (46%)]\tLoss: 2.382905\n",
            "Train Epoch: 8 [23500/49674 (47%)]\tLoss: 1.563056\n",
            "Train Epoch: 8 [24000/49674 (48%)]\tLoss: 1.139991\n",
            "Train Epoch: 8 [24500/49674 (49%)]\tLoss: 1.117863\n",
            "Train Epoch: 8 [25000/49674 (50%)]\tLoss: 1.218061\n",
            "Train Epoch: 8 [25500/49674 (51%)]\tLoss: 1.325053\n",
            "Train Epoch: 8 [26000/49674 (52%)]\tLoss: 2.968872\n",
            "Train Epoch: 8 [26500/49674 (53%)]\tLoss: 0.588318\n",
            "Train Epoch: 8 [27000/49674 (54%)]\tLoss: 2.099396\n",
            "Train Epoch: 8 [27500/49674 (55%)]\tLoss: 2.359669\n",
            "Train Epoch: 8 [28000/49674 (56%)]\tLoss: 2.231184\n",
            "Train Epoch: 8 [28500/49674 (57%)]\tLoss: 0.871885\n",
            "Train Epoch: 8 [29000/49674 (58%)]\tLoss: 2.626021\n",
            "Train Epoch: 8 [29500/49674 (59%)]\tLoss: 1.628407\n",
            "Train Epoch: 8 [30000/49674 (60%)]\tLoss: 1.926475\n",
            "Train Epoch: 8 [30500/49674 (61%)]\tLoss: 1.005592\n",
            "Train Epoch: 8 [31000/49674 (62%)]\tLoss: 0.860239\n",
            "Train Epoch: 8 [31500/49674 (63%)]\tLoss: 1.240943\n",
            "Train Epoch: 8 [32000/49674 (64%)]\tLoss: 3.017496\n",
            "Train Epoch: 8 [32500/49674 (65%)]\tLoss: 1.537390\n",
            "Train Epoch: 8 [33000/49674 (66%)]\tLoss: 1.551357\n",
            "Train Epoch: 8 [33500/49674 (67%)]\tLoss: 3.617003\n",
            "Train Epoch: 8 [34000/49674 (68%)]\tLoss: 0.961621\n",
            "Train Epoch: 8 [34500/49674 (69%)]\tLoss: 2.753754\n",
            "Train Epoch: 8 [35000/49674 (70%)]\tLoss: 3.940944\n",
            "Train Epoch: 8 [35500/49674 (71%)]\tLoss: 1.935729\n",
            "Train Epoch: 8 [36000/49674 (72%)]\tLoss: 2.303560\n",
            "Train Epoch: 8 [36500/49674 (73%)]\tLoss: 1.551126\n",
            "Train Epoch: 8 [37000/49674 (74%)]\tLoss: 1.884949\n",
            "Train Epoch: 8 [37500/49674 (75%)]\tLoss: 1.205462\n",
            "Train Epoch: 8 [38000/49674 (76%)]\tLoss: 1.874066\n",
            "Train Epoch: 8 [38500/49674 (77%)]\tLoss: 1.123619\n",
            "Train Epoch: 8 [39000/49674 (78%)]\tLoss: 1.499932\n",
            "Train Epoch: 8 [39500/49674 (79%)]\tLoss: 0.924048\n",
            "Train Epoch: 8 [40000/49674 (80%)]\tLoss: 2.617755\n",
            "Train Epoch: 8 [40500/49674 (81%)]\tLoss: 0.963204\n",
            "Train Epoch: 8 [41000/49674 (82%)]\tLoss: 3.456961\n",
            "Train Epoch: 8 [41500/49674 (84%)]\tLoss: 3.164093\n",
            "Train Epoch: 8 [42000/49674 (85%)]\tLoss: 1.729574\n",
            "Train Epoch: 8 [42500/49674 (86%)]\tLoss: 4.343089\n",
            "Train Epoch: 8 [43000/49674 (87%)]\tLoss: 1.583748\n",
            "Train Epoch: 8 [43500/49674 (88%)]\tLoss: 1.031237\n",
            "Train Epoch: 8 [44000/49674 (89%)]\tLoss: 2.801652\n",
            "Train Epoch: 8 [44500/49674 (90%)]\tLoss: 0.744591\n",
            "Train Epoch: 8 [45000/49674 (91%)]\tLoss: 2.069107\n",
            "Train Epoch: 8 [45500/49674 (92%)]\tLoss: 1.743723\n",
            "Train Epoch: 8 [46000/49674 (93%)]\tLoss: 1.880715\n",
            "Train Epoch: 8 [46500/49674 (94%)]\tLoss: 1.564158\n",
            "Train Epoch: 8 [47000/49674 (95%)]\tLoss: 1.848190\n",
            "Train Epoch: 8 [47500/49674 (96%)]\tLoss: 2.540314\n",
            "Train Epoch: 8 [48000/49674 (97%)]\tLoss: 1.428711\n",
            "Train Epoch: 8 [48500/49674 (98%)]\tLoss: 1.656269\n",
            "Train Epoch: 8 [49000/49674 (99%)]\tLoss: 1.515316\n",
            "Train Epoch: 8 [49500/49674 (100%)]\tLoss: 2.102480\n",
            "Train Epoch: 9 [0/49674 (0%)]\tLoss: 3.868235\n",
            "Train Epoch: 9 [500/49674 (1%)]\tLoss: 1.539219\n",
            "Train Epoch: 9 [1000/49674 (2%)]\tLoss: 1.336139\n",
            "Train Epoch: 9 [1500/49674 (3%)]\tLoss: 1.753990\n",
            "Train Epoch: 9 [2000/49674 (4%)]\tLoss: 0.638776\n",
            "Train Epoch: 9 [2500/49674 (5%)]\tLoss: 1.692431\n",
            "Train Epoch: 9 [3000/49674 (6%)]\tLoss: 0.655119\n",
            "Train Epoch: 9 [3500/49674 (7%)]\tLoss: 2.338075\n",
            "Train Epoch: 9 [4000/49674 (8%)]\tLoss: 2.892466\n",
            "Train Epoch: 9 [4500/49674 (9%)]\tLoss: 1.379615\n",
            "Train Epoch: 9 [5000/49674 (10%)]\tLoss: 0.991583\n",
            "Train Epoch: 9 [5500/49674 (11%)]\tLoss: 1.537032\n",
            "Train Epoch: 9 [6000/49674 (12%)]\tLoss: 0.966926\n",
            "Train Epoch: 9 [6500/49674 (13%)]\tLoss: 1.867220\n",
            "Train Epoch: 9 [7000/49674 (14%)]\tLoss: 2.978910\n",
            "Train Epoch: 9 [7500/49674 (15%)]\tLoss: 1.339355\n",
            "Train Epoch: 9 [8000/49674 (16%)]\tLoss: 1.489335\n",
            "Train Epoch: 9 [8500/49674 (17%)]\tLoss: 1.952815\n",
            "Train Epoch: 9 [9000/49674 (18%)]\tLoss: 1.233299\n",
            "Train Epoch: 9 [9500/49674 (19%)]\tLoss: 1.146047\n",
            "Train Epoch: 9 [10000/49674 (20%)]\tLoss: 2.606534\n",
            "Train Epoch: 9 [10500/49674 (21%)]\tLoss: 1.842529\n",
            "Train Epoch: 9 [11000/49674 (22%)]\tLoss: 3.684258\n",
            "Train Epoch: 9 [11500/49674 (23%)]\tLoss: 1.850625\n",
            "Train Epoch: 9 [12000/49674 (24%)]\tLoss: 3.130745\n",
            "Train Epoch: 9 [12500/49674 (25%)]\tLoss: 1.994084\n",
            "Train Epoch: 9 [13000/49674 (26%)]\tLoss: 1.622036\n",
            "Train Epoch: 9 [13500/49674 (27%)]\tLoss: 1.336966\n",
            "Train Epoch: 9 [14000/49674 (28%)]\tLoss: 2.022123\n",
            "Train Epoch: 9 [14500/49674 (29%)]\tLoss: 0.840295\n",
            "Train Epoch: 9 [15000/49674 (30%)]\tLoss: 1.393307\n",
            "Train Epoch: 9 [15500/49674 (31%)]\tLoss: 2.093831\n",
            "Train Epoch: 9 [16000/49674 (32%)]\tLoss: 2.226449\n",
            "Train Epoch: 9 [16500/49674 (33%)]\tLoss: 1.114128\n",
            "Train Epoch: 9 [17000/49674 (34%)]\tLoss: 1.443510\n",
            "Train Epoch: 9 [17500/49674 (35%)]\tLoss: 2.184957\n",
            "Train Epoch: 9 [18000/49674 (36%)]\tLoss: 2.289994\n",
            "Train Epoch: 9 [18500/49674 (37%)]\tLoss: 1.649709\n",
            "Train Epoch: 9 [19000/49674 (38%)]\tLoss: 2.445126\n",
            "Train Epoch: 9 [19500/49674 (39%)]\tLoss: 1.423862\n",
            "Train Epoch: 9 [20000/49674 (40%)]\tLoss: 2.281261\n",
            "Train Epoch: 9 [20500/49674 (41%)]\tLoss: 1.616935\n",
            "Train Epoch: 9 [21000/49674 (42%)]\tLoss: 1.694536\n",
            "Train Epoch: 9 [21500/49674 (43%)]\tLoss: 1.932325\n",
            "Train Epoch: 9 [22000/49674 (44%)]\tLoss: 1.877828\n",
            "Train Epoch: 9 [22500/49674 (45%)]\tLoss: 2.266574\n",
            "Train Epoch: 9 [23000/49674 (46%)]\tLoss: 0.616059\n",
            "Train Epoch: 9 [23500/49674 (47%)]\tLoss: 1.334983\n",
            "Train Epoch: 9 [24000/49674 (48%)]\tLoss: 0.866341\n",
            "Train Epoch: 9 [24500/49674 (49%)]\tLoss: 0.696211\n",
            "Train Epoch: 9 [25000/49674 (50%)]\tLoss: 1.324269\n",
            "Train Epoch: 9 [25500/49674 (51%)]\tLoss: 1.299658\n",
            "Train Epoch: 9 [26000/49674 (52%)]\tLoss: 3.763381\n",
            "Train Epoch: 9 [26500/49674 (53%)]\tLoss: 1.164726\n",
            "Train Epoch: 9 [27000/49674 (54%)]\tLoss: 0.881156\n",
            "Train Epoch: 9 [27500/49674 (55%)]\tLoss: 1.032379\n",
            "Train Epoch: 9 [28000/49674 (56%)]\tLoss: 2.485270\n",
            "Train Epoch: 9 [28500/49674 (57%)]\tLoss: 1.297580\n",
            "Train Epoch: 9 [29000/49674 (58%)]\tLoss: 2.806864\n",
            "Train Epoch: 9 [29500/49674 (59%)]\tLoss: 3.616981\n",
            "Train Epoch: 9 [30000/49674 (60%)]\tLoss: 1.812656\n",
            "Train Epoch: 9 [30500/49674 (61%)]\tLoss: 1.398220\n",
            "Train Epoch: 9 [31000/49674 (62%)]\tLoss: 2.464743\n",
            "Train Epoch: 9 [31500/49674 (63%)]\tLoss: 2.273306\n",
            "Train Epoch: 9 [32000/49674 (64%)]\tLoss: 3.686543\n",
            "Train Epoch: 9 [32500/49674 (65%)]\tLoss: 3.637399\n",
            "Train Epoch: 9 [33000/49674 (66%)]\tLoss: 1.492412\n",
            "Train Epoch: 9 [33500/49674 (67%)]\tLoss: 3.402454\n",
            "Train Epoch: 9 [34000/49674 (68%)]\tLoss: 2.492926\n",
            "Train Epoch: 9 [34500/49674 (69%)]\tLoss: 2.694513\n",
            "Train Epoch: 9 [35000/49674 (70%)]\tLoss: 1.587175\n",
            "Train Epoch: 9 [35500/49674 (71%)]\tLoss: 1.376542\n",
            "Train Epoch: 9 [36000/49674 (72%)]\tLoss: 1.486050\n",
            "Train Epoch: 9 [36500/49674 (73%)]\tLoss: 1.996938\n",
            "Train Epoch: 9 [37000/49674 (74%)]\tLoss: 2.957780\n",
            "Train Epoch: 9 [37500/49674 (75%)]\tLoss: 2.172443\n",
            "Train Epoch: 9 [38000/49674 (76%)]\tLoss: 1.632048\n",
            "Train Epoch: 9 [38500/49674 (77%)]\tLoss: 1.424983\n",
            "Train Epoch: 9 [39000/49674 (78%)]\tLoss: 0.810455\n",
            "Train Epoch: 9 [39500/49674 (79%)]\tLoss: 1.986667\n",
            "Train Epoch: 9 [40000/49674 (80%)]\tLoss: 1.319053\n",
            "Train Epoch: 9 [40500/49674 (81%)]\tLoss: 1.614156\n",
            "Train Epoch: 9 [41000/49674 (82%)]\tLoss: 1.357282\n",
            "Train Epoch: 9 [41500/49674 (84%)]\tLoss: 1.592080\n",
            "Train Epoch: 9 [42000/49674 (85%)]\tLoss: 1.952783\n",
            "Train Epoch: 9 [42500/49674 (86%)]\tLoss: 1.970546\n",
            "Train Epoch: 9 [43000/49674 (87%)]\tLoss: 2.851214\n",
            "Train Epoch: 9 [43500/49674 (88%)]\tLoss: 1.116617\n",
            "Train Epoch: 9 [44000/49674 (89%)]\tLoss: 2.074050\n",
            "Train Epoch: 9 [44500/49674 (90%)]\tLoss: 1.002876\n",
            "Train Epoch: 9 [45000/49674 (91%)]\tLoss: 0.978154\n",
            "Train Epoch: 9 [45500/49674 (92%)]\tLoss: 1.269285\n",
            "Train Epoch: 9 [46000/49674 (93%)]\tLoss: 1.561408\n",
            "Train Epoch: 9 [46500/49674 (94%)]\tLoss: 1.362887\n",
            "Train Epoch: 9 [47000/49674 (95%)]\tLoss: 0.886091\n",
            "Train Epoch: 9 [47500/49674 (96%)]\tLoss: 1.853488\n",
            "Train Epoch: 9 [48000/49674 (97%)]\tLoss: 1.766599\n",
            "Train Epoch: 9 [48500/49674 (98%)]\tLoss: 1.674420\n",
            "Train Epoch: 9 [49000/49674 (99%)]\tLoss: 1.943462\n",
            "Train Epoch: 9 [49500/49674 (100%)]\tLoss: 1.253462\n",
            "Train Epoch: 10 [0/49674 (0%)]\tLoss: 1.325689\n",
            "Train Epoch: 10 [500/49674 (1%)]\tLoss: 1.024271\n",
            "Train Epoch: 10 [1000/49674 (2%)]\tLoss: 2.397344\n",
            "Train Epoch: 10 [1500/49674 (3%)]\tLoss: 1.172979\n",
            "Train Epoch: 10 [2000/49674 (4%)]\tLoss: 1.118232\n",
            "Train Epoch: 10 [2500/49674 (5%)]\tLoss: 2.097939\n",
            "Train Epoch: 10 [3000/49674 (6%)]\tLoss: 1.329196\n",
            "Train Epoch: 10 [3500/49674 (7%)]\tLoss: 0.903792\n",
            "Train Epoch: 10 [4000/49674 (8%)]\tLoss: 0.868296\n",
            "Train Epoch: 10 [4500/49674 (9%)]\tLoss: 2.487838\n",
            "Train Epoch: 10 [5000/49674 (10%)]\tLoss: 1.757363\n",
            "Train Epoch: 10 [5500/49674 (11%)]\tLoss: 1.011850\n",
            "Train Epoch: 10 [6000/49674 (12%)]\tLoss: 2.359792\n",
            "Train Epoch: 10 [6500/49674 (13%)]\tLoss: 1.695043\n",
            "Train Epoch: 10 [7000/49674 (14%)]\tLoss: 1.259551\n",
            "Train Epoch: 10 [7500/49674 (15%)]\tLoss: 1.046781\n",
            "Train Epoch: 10 [8000/49674 (16%)]\tLoss: 1.876468\n",
            "Train Epoch: 10 [8500/49674 (17%)]\tLoss: 1.154750\n",
            "Train Epoch: 10 [9000/49674 (18%)]\tLoss: 2.058568\n",
            "Train Epoch: 10 [9500/49674 (19%)]\tLoss: 1.125373\n",
            "Train Epoch: 10 [10000/49674 (20%)]\tLoss: 0.796096\n",
            "Train Epoch: 10 [10500/49674 (21%)]\tLoss: 1.310429\n",
            "Train Epoch: 10 [11000/49674 (22%)]\tLoss: 1.319017\n",
            "Train Epoch: 10 [11500/49674 (23%)]\tLoss: 0.653131\n",
            "Train Epoch: 10 [12000/49674 (24%)]\tLoss: 2.400606\n",
            "Train Epoch: 10 [12500/49674 (25%)]\tLoss: 1.255162\n",
            "Train Epoch: 10 [13000/49674 (26%)]\tLoss: 0.852872\n",
            "Train Epoch: 10 [13500/49674 (27%)]\tLoss: 1.111434\n",
            "Train Epoch: 10 [14000/49674 (28%)]\tLoss: 1.710933\n",
            "Train Epoch: 10 [14500/49674 (29%)]\tLoss: 1.202249\n",
            "Train Epoch: 10 [15000/49674 (30%)]\tLoss: 2.157358\n",
            "Train Epoch: 10 [15500/49674 (31%)]\tLoss: 3.071754\n",
            "Train Epoch: 10 [16000/49674 (32%)]\tLoss: 0.750538\n",
            "Train Epoch: 10 [16500/49674 (33%)]\tLoss: 1.011920\n",
            "Train Epoch: 10 [17000/49674 (34%)]\tLoss: 1.039116\n",
            "Train Epoch: 10 [17500/49674 (35%)]\tLoss: 1.210985\n",
            "Train Epoch: 10 [18000/49674 (36%)]\tLoss: 0.894169\n",
            "Train Epoch: 10 [18500/49674 (37%)]\tLoss: 1.412677\n",
            "Train Epoch: 10 [19000/49674 (38%)]\tLoss: 2.803257\n",
            "Train Epoch: 10 [19500/49674 (39%)]\tLoss: 1.029158\n",
            "Train Epoch: 10 [20000/49674 (40%)]\tLoss: 1.029825\n",
            "Train Epoch: 10 [20500/49674 (41%)]\tLoss: 0.688949\n",
            "Train Epoch: 10 [21000/49674 (42%)]\tLoss: 0.911549\n",
            "Train Epoch: 10 [21500/49674 (43%)]\tLoss: 1.491865\n",
            "Train Epoch: 10 [22000/49674 (44%)]\tLoss: 1.714340\n",
            "Train Epoch: 10 [22500/49674 (45%)]\tLoss: 1.179930\n",
            "Train Epoch: 10 [23000/49674 (46%)]\tLoss: 2.986404\n",
            "Train Epoch: 10 [23500/49674 (47%)]\tLoss: 1.634678\n",
            "Train Epoch: 10 [24000/49674 (48%)]\tLoss: 1.514263\n",
            "Train Epoch: 10 [24500/49674 (49%)]\tLoss: 3.568664\n",
            "Train Epoch: 10 [25000/49674 (50%)]\tLoss: 1.508880\n",
            "Train Epoch: 10 [25500/49674 (51%)]\tLoss: 2.375660\n",
            "Train Epoch: 10 [26000/49674 (52%)]\tLoss: 1.216665\n",
            "Train Epoch: 10 [26500/49674 (53%)]\tLoss: 1.267478\n",
            "Train Epoch: 10 [27000/49674 (54%)]\tLoss: 1.608222\n",
            "Train Epoch: 10 [27500/49674 (55%)]\tLoss: 0.742325\n",
            "Train Epoch: 10 [28000/49674 (56%)]\tLoss: 0.989536\n",
            "Train Epoch: 10 [28500/49674 (57%)]\tLoss: 2.264027\n",
            "Train Epoch: 10 [29000/49674 (58%)]\tLoss: 0.701224\n",
            "Train Epoch: 10 [29500/49674 (59%)]\tLoss: 2.712322\n",
            "Train Epoch: 10 [30000/49674 (60%)]\tLoss: 1.526667\n",
            "Train Epoch: 10 [30500/49674 (61%)]\tLoss: 2.783179\n",
            "Train Epoch: 10 [31000/49674 (62%)]\tLoss: 0.898218\n",
            "Train Epoch: 10 [31500/49674 (63%)]\tLoss: 1.231894\n",
            "Train Epoch: 10 [32000/49674 (64%)]\tLoss: 1.338739\n",
            "Train Epoch: 10 [32500/49674 (65%)]\tLoss: 0.644622\n",
            "Train Epoch: 10 [33000/49674 (66%)]\tLoss: 1.520477\n",
            "Train Epoch: 10 [33500/49674 (67%)]\tLoss: 1.966321\n",
            "Train Epoch: 10 [34000/49674 (68%)]\tLoss: 2.788309\n",
            "Train Epoch: 10 [34500/49674 (69%)]\tLoss: 2.726566\n",
            "Train Epoch: 10 [35000/49674 (70%)]\tLoss: 0.823771\n",
            "Train Epoch: 10 [35500/49674 (71%)]\tLoss: 2.578487\n",
            "Train Epoch: 10 [36000/49674 (72%)]\tLoss: 0.490501\n",
            "Train Epoch: 10 [36500/49674 (73%)]\tLoss: 2.181785\n",
            "Train Epoch: 10 [37000/49674 (74%)]\tLoss: 2.292220\n",
            "Train Epoch: 10 [37500/49674 (75%)]\tLoss: 1.336333\n",
            "Train Epoch: 10 [38000/49674 (76%)]\tLoss: 1.737330\n",
            "Train Epoch: 10 [38500/49674 (77%)]\tLoss: 1.472091\n",
            "Train Epoch: 10 [39000/49674 (78%)]\tLoss: 1.588818\n",
            "Train Epoch: 10 [39500/49674 (79%)]\tLoss: 0.952616\n",
            "Train Epoch: 10 [40000/49674 (80%)]\tLoss: 1.907853\n",
            "Train Epoch: 10 [40500/49674 (81%)]\tLoss: 0.890899\n",
            "Train Epoch: 10 [41000/49674 (82%)]\tLoss: 1.612138\n",
            "Train Epoch: 10 [41500/49674 (84%)]\tLoss: 3.270330\n",
            "Train Epoch: 10 [42000/49674 (85%)]\tLoss: 1.289640\n",
            "Train Epoch: 10 [42500/49674 (86%)]\tLoss: 0.831203\n",
            "Train Epoch: 10 [43000/49674 (87%)]\tLoss: 1.316145\n",
            "Train Epoch: 10 [43500/49674 (88%)]\tLoss: 2.745352\n",
            "Train Epoch: 10 [44000/49674 (89%)]\tLoss: 0.780558\n",
            "Train Epoch: 10 [44500/49674 (90%)]\tLoss: 1.299538\n",
            "Train Epoch: 10 [45000/49674 (91%)]\tLoss: 2.044884\n",
            "Train Epoch: 10 [45500/49674 (92%)]\tLoss: 1.415295\n",
            "Train Epoch: 10 [46000/49674 (93%)]\tLoss: 2.072932\n",
            "Train Epoch: 10 [46500/49674 (94%)]\tLoss: 1.522921\n",
            "Train Epoch: 10 [47000/49674 (95%)]\tLoss: 3.398025\n",
            "Train Epoch: 10 [47500/49674 (96%)]\tLoss: 1.719579\n",
            "Train Epoch: 10 [48000/49674 (97%)]\tLoss: 1.394836\n",
            "Train Epoch: 10 [48500/49674 (98%)]\tLoss: 1.059833\n",
            "Train Epoch: 10 [49000/49674 (99%)]\tLoss: 1.963588\n",
            "Train Epoch: 10 [49500/49674 (100%)]\tLoss: 3.973901\n",
            "Epoch: 10\n",
            "Training loss: 1.6949142552237635\n",
            "Saving final model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "669IhocVxO3_",
        "outputId": "95e35fcf-5764-4d2f-c0b3-7db8c5d7bb79"
      },
      "source": [
        "USE_CUDA = False\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "final_model = torch.load('/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/midi.pt')\n",
        "start_note = audio[0:5]\n",
        "length = len(audio)\n",
        "input = torch.from_numpy(start_note).unsqueeze(0).to(device).float()\n",
        "pred = input.clone().squeeze()\n",
        "with torch.no_grad():\n",
        "  for i in range(length):\n",
        "    output = final_model(input)\n",
        "    pred = torch.cat((pred, output), dim=0)\n",
        "    input = pred[-5:].unsqueeze(0)\n",
        "    if i % 1000 == 0:\n",
        "      print(i)\n",
        "\n",
        "mid_new = arry2mid(pred.cpu().detach().numpy().astype('int64'), 545455)\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/output/'\n",
        "mid_new.save(path + 'mid_new.mid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}